{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Traverse LCP, then MFS\n",
    "\n",
    "* Replace earlier masked-array strategy with regular arrays, using 0 to represent a null.\n",
    "* Real data (offset of token within witness) is one-based.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from linsuffarr import SuffixArray\n",
    "from linsuffarr import UNIT_BYTE\n",
    "import pprint\n",
    "from dataclasses import dataclass\n",
    "from heapq import * # priority heap, https://docs.python.org/3/library/heapq.html\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sigla = ['w0', 'w1', 'w2', 'w3', 'w4', 'w5']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt', 'darwin1861.txt', 'darwin1866.txt', 'darwin1869.txt', 'darwin1872.txt']\n",
    "# sigla = ['w0', 'w1', 'w2', 'w3']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt', 'darwin1861.txt', 'darwin1866.txt']\n",
    "# sigla = ['w0', 'w1']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt']\n",
    "sigla = ['w0', 'w1', 'w2', 'w3', 'w4']\n",
    "filenames = ['abc/abcd.txt', 'abc/abcda.txt', 'abc/abcdb.txt', 'abc/abcdc.txt', 'abc/abcdd.txt']\n",
    "first_paragraph = 0\n",
    "last_paragraph = 10\n",
    "how_many_paragraphs = last_paragraph - first_paragraph\n",
    "raw_data_dict = {}\n",
    "for siglum, filename in zip(sigla, filenames):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line for line in lines if line != '\\n']\n",
    "        raw_data_dict[siglum] = \" \".join(lines[first_paragraph : last_paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_witnesses(witness_strings: List[str]): # one string per witness\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    witnesses = []\n",
    "    for witness_string in witness_strings:\n",
    "        witness_tokens = witness_string.split()\n",
    "        witnesses.append(witness_tokens)\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_token_array(witness_token_lists): # list of token lists per witness\n",
    "    '''Create token array (single list, with separator \" # \" between witnesses'''\n",
    "    token_array = [] # strings\n",
    "    token_membership_array = [] # witness identifiers, same offsets as in token_array\n",
    "    token_witness_offset_array = [] # one-based offset of token in witness\n",
    "    last_witness_offset = len(witness_token_lists) - 1\n",
    "    for index, witness_token_list in enumerate(witness_token_lists):\n",
    "        token_array.extend(witness_token_list)\n",
    "        for token_offset, token in enumerate(witness_token_list): # don't need enumerate, just len()\n",
    "            token_witness_offset_array.append(token_offset)\n",
    "        token_membership_array.extend([index for token in witness_token_list])\n",
    "        if index < last_witness_offset:\n",
    "            separator = \" #\" + str(index + 1) + \" \"\n",
    "            token_array.append(separator)\n",
    "            token_membership_array.append(separator)\n",
    "            token_witness_offset_array.append(-1)\n",
    "    return token_array, token_membership_array, token_witness_offset_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "witness_sigla = [key for key in raw_data_dict.keys()]\n",
    "witnesses = tokenize_witnesses([value for value in raw_data_dict.values()]) # strings\n",
    "# token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token_array, token_membership_array, token_witness_offset_array = create_token_array(witnesses)\n",
    "# print(f\"{token_array=}\")\n",
    "# print(f\"{token_membership_array=}\")\n",
    "# print(f\"{token_witness_offset_array=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "suffix_array = SuffixArray(token_array, unit=UNIT_BYTE)\n",
    "# print(suffix_array)\n",
    "# LCP=0 means that the block has nothing in common with the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array('i', [0, 0, 0, 0, 0])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcp_array = suffix_array._LCP_values\n",
    "lcp_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create Block dataclass\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not into token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    witnesses: set\n",
    "    witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    # how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_blocks_old (_lcp_array):\n",
    "    '''Create blocks from lcp array\n",
    "\n",
    "    Skip first lcp value, which is a fake; otherwise compare lcp value to length of block at top of stack.\n",
    "    Four possibilities:\n",
    "\n",
    "        stack is empty\n",
    "            * if lcp value == 0, proceed to next lcp value (continue)\n",
    "            * if lcp value > 0, create block and push onto stack, then proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) matches block length at top of stack\n",
    "            * proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) is longer than block length at top of stack\n",
    "            * create and push new block\n",
    "\n",
    "        lcp value is shorter than block length at top of stack\n",
    "            * (recursive) if block at top of stack is longer than current lcp value, pop and append to _blocks\n",
    "            * if block at top of stack is equal to lcp value, proceed to next lcp value (continue)\n",
    "            * if block at top of stack is shorter than current lcp value ...\n",
    "            *   create and push new block starting at start position of most recently closed block, then proceed to next lcp value (continue)\n",
    "\n",
    "    In other words:\n",
    "\n",
    "        We proceed to next lcp value if:\n",
    "            * stack is empty and lcp value == 0\n",
    "            * lcp value matches block length at top of stack (can we combine this with the preceding, since an empty stack effectively has a zero-length block on top?)\n",
    "\n",
    "        We push a new value on stack and then proceed to next lcp value if:\n",
    "            * stack is empty and lcp value > 0\n",
    "            * lcp value is longer than block length at top of stack (where is the start position?)\n",
    "\n",
    "        We pop from the stack to _blocks and then check the next stack value (stick with same lcp) if:\n",
    "            * lcp value is shorter than current block value\n",
    "\n",
    "cases (occurrences are always one more than number of repetitions):\n",
    "    5 5 2     --> 1 block of 5 occurs 3 times, 1 block of 2 occures 4 times\n",
    "    2 5 5 2   --> 1 block of 2 occurs 5 times, 1 block of 5 occures 3 times\n",
    "    5 5 0 2   --> 1 block of 5 occurs 3 times, 1 block of 2 occures 2 times\n",
    "    2 5 5 2 3 --> \n",
    "\n",
    "\n",
    "Nested while structures:\n",
    "\n",
    "(Create blocks in two places because they have different start positions)\n",
    "(Nested while loops because we traverse two things: lcp array and, sometimes, stack)\n",
    "\n",
    "while next-lcp-value: # traverse lcp array\n",
    "    if something\n",
    "    elif something else\n",
    "    elif perhaps yet another something else\n",
    "    else: # possible hidden block (or possibly not)\n",
    "        while something-on-the-stack: # traverse stack for some lcp value situations\n",
    "            pop larger values\n",
    "        if hidden-block:\n",
    "            create and push\n",
    "clean-up-stack-after-last-lcp-value # or tack a 0 onto the end of the lcp to avoid extra clean-up code\n",
    "'''\n",
    "    from collections import deque # deque has faster append and pop than list\n",
    "    _blocks = []\n",
    "    open_block_stack = deque()\n",
    "    for offset, lcp in enumerate(lcp_array):\n",
    "        # three situations: next one is same value, higher that last, or lower than last\n",
    "        # if same value: same pattern\n",
    "        # if higher or lower, new pattern (may overlap with previous, unless one or the other value is 0)\n",
    "        peek = open_block_stack[-1] if open_block_stack else None\n",
    "        peek_token_count = peek.token_count if peek else 0\n",
    "        if offset == 0: # skip the first one, which is a transition from a fake start value\n",
    "            continue # resume loop with next item in lcp array\n",
    "        elif lcp == peek_token_count:\n",
    "            pass # same pattern (happens with repetition), so do nothing\n",
    "        elif lcp > peek_token_count: # new prefix is longer than previous one, so start new pattern\n",
    "            # can fill in end_position and frequency only when we encounter a shorter value in the LCP array\n",
    "            # start_position is number of patterns that are the same \n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = offset - 1, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "        else: # new prefix is shorter than previous one, so:\n",
    "                # 1. close open blocks with higher values\n",
    "                # 2. do something else\n",
    "            while open_block_stack and open_block_stack[-1].token_count > lcp: # if an open block is longer than the current length, pop and close it\n",
    "                block_being_modified = open_block_stack.pop()\n",
    "                block_being_modified.end_position = offset - 1\n",
    "                block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "                _blocks.append(block_being_modified)\n",
    "            if lcp > 0 and (not open_block_stack or open_block_stack[-1].token_count < lcp):\n",
    "                open_block_stack.append(Block(token_count = lcp, start_position = _blocks[-1].start_position, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "\n",
    "    while open_block_stack: # pop anything left in open_block_stack\n",
    "        block_being_modified = open_block_stack.pop()\n",
    "        block_being_modified.end_position = len(lcp_array) - 1\n",
    "        block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "        _blocks.append(block_being_modified)\n",
    "\n",
    "    # add all_start_positions and then witness_count properties to blocks\n",
    "    for _index, _block in enumerate(_blocks):\n",
    "        # block_start_position through block_end_position gives offsets of all start positions in suffix_array\n",
    "        _block.all_start_positions = sorted([suffix_array.SA[x] for x in range(_block.start_position,_block.end_position + 1)])\n",
    "        # use all start positions to find witness count\n",
    "        _block.witnesses = set(token_membership_array[offset] for offset in _block.all_start_positions)\n",
    "        _block.witness_count = len(_block.witnesses)\n",
    "    return _blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Lcp_interval_candidate:\n",
    "    lcp_start_offset: int\n",
    "    lcp_interval_token_count: int\n",
    "    lcp_end_offset: int = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def expand_prefix(prefix_to_expand:Lcp_interval_candidate):\n",
    "    token_start_position = suffix_array.SA[prefix_to_expand.lcp_start_offset]\n",
    "    token_count = prefix_to_expand.lcp_interval_token_count\n",
    "    tokens = token_array[token_start_position: token_start_position + token_count]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_for_depth_and_repetition(_lcp_interval:Lcp_interval_candidate, _witness_count: int) -> bool:\n",
    "    \"\"\"Write a docstring someday\n",
    "\n",
    "    Number of prefixes >= total number of witnesses\n",
    "    Accumulate set of witness sigla for prefixes\n",
    "    if:\n",
    "        no witness occurs more than once, return True to keep this block\n",
    "    else:\n",
    "        return False\n",
    "    \"\"\"\n",
    "#     print(f\"Checking for depth and repetition for: {_lcp_interval=}\")\n",
    "#     print(f\"Occurs {_lcp_interval.lcp_end_offset - _lcp_interval.lcp_start_offset + 1} times in witness set\")\n",
    "#     print(expand_prefix(_lcp_interval))\n",
    "#     print()\n",
    "    block_instance_count = _lcp_interval.lcp_end_offset - _lcp_interval.lcp_start_offset + 1\n",
    "    if block_instance_count != _witness_count:\n",
    "        return False\n",
    "    else:\n",
    "        witnesses_found = []\n",
    "        for lcp_interval_item_offset in range(_lcp_interval.lcp_start_offset, _lcp_interval.lcp_end_offset + 1):\n",
    "            token_position = suffix_array.SA[lcp_interval_item_offset] # point from prefix to suffix array position\n",
    "            witness_siglum = token_membership_array[token_position] # point from token array position to witness identifier\n",
    "            if witness_siglum in witnesses_found:\n",
    "                return False\n",
    "            else:\n",
    "                witnesses_found.append(witness_siglum)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_blocks(_lcp_array: list):\n",
    "    \"\"\"Write a docstring someday\n",
    "\n",
    "    Look at changes in length of LCP array\n",
    "    Initial value is 0 or -1 because it's a comparison with previous, and first has no previous\n",
    "    Next value is number of tokens shared with previous\n",
    "    Exact length doesn't matter, but if it changes, new pattern:\n",
    "        If it stays the same, take note but do nothing yet; it means that the pattern repeats\n",
    "        No change for a while, then goes to 0:\n",
    "            Number of repetitions plus 1, e.g., 5 5 5 0 = 4 instances of 5\n",
    "            Once it changes to 0, we've seen complete pattern\n",
    "        Changer to smaller means hidden, deeper block\n",
    "        Changes to longer means ???\n",
    "    \"\"\"\n",
    "    accumulator = [] # lcp positions (not values) since most recent 0\n",
    "    frequent_sequences = [] # lcp intervals to be considered for mfs\n",
    "    #\n",
    "    # lcp value\n",
    "    # if == 0 it's a new interval, so:\n",
    "    #   1. if there is already an accumulation, commit (process) it\n",
    "    #      \"committing the buffer\" means checking for repetition and depth\n",
    "    #          if it passes check: store in mfs list\n",
    "    #          otherwise throw it away\n",
    "    #   2. clear buffer (accumulator) and begin accumulating new buffer with the new offset with 0 value\n",
    "    # otherwise it isn't zero, so there must be a buffer in place, so add to it (for now)\n",
    "    for offset, value in enumerate(_lcp_array):\n",
    "        if not accumulator and value == 0: # if accumulator is empty and new value is 0, do nothing\n",
    "            continue\n",
    "        elif not accumulator: # accumulator is empty and new value is non-zero, so begin new accumulator\n",
    "            accumulator.append(Lcp_interval_candidate(lcp_start_offset = offset - 1, lcp_interval_token_count = value))\n",
    "        elif value > accumulator[-1].lcp_interval_token_count: # new interval, so add to accumulator and continue\n",
    "            accumulator.append(Lcp_interval_candidate(lcp_start_offset = offset - 1, lcp_interval_token_count = value))\n",
    "        elif value == accumulator[-1].lcp_interval_token_count: # same block as before, so do nothing\n",
    "            continue\n",
    "        else: # new value is less than top of accumulator, so pop everything that is higher\n",
    "            # Positions in lcp array and suffix array coincide:\n",
    "            #   The lcp array value is the length of the sequence\n",
    "            #   The suffix array value is the start position of the sequence\n",
    "            # Assume accumulator values (offsets into lcp array) point to [3, 6] and new value is 4, so:\n",
    "            #   First: Pop pointer to 6 (length value in lcp array), store in frequent_sequences\n",
    "            #   Second: Push new pointer to same position in lcp array, but change value in lcp array to 4\n",
    "            while accumulator and accumulator[-1].lcp_interval_token_count > value:\n",
    "                # Create pointer to last closed block that is not filtered (like frequent_sequences)\n",
    "                newly_closed_block = accumulator.pop()\n",
    "                newly_closed_block.lcp_end_offset = offset - 1\n",
    "                if check_for_depth_and_repetition(newly_closed_block, len(witnesses)):\n",
    "                    frequent_sequences.append([newly_closed_block.lcp_start_offset, newly_closed_block.lcp_end_offset, newly_closed_block.lcp_interval_token_count])\n",
    "            # There are three options:\n",
    "            #   1. there is content in the accumulator and latest value is not 0\n",
    "            #   2. accumulator is empty and latest value is 0\n",
    "            #   3. accumulator is empty and latest value is not 0\n",
    "            # (the fourth logical combination, content in the accumulator and 0 value, cannot occur\n",
    "            #     because a 0 value will empty the accumulator)\n",
    "            if value > 0 and (not accumulator or accumulator[-1].lcp_interval_token_count != value):\n",
    "                accumulator.append(Lcp_interval_candidate(lcp_start_offset = newly_closed_block.lcp_start_offset, lcp_interval_token_count = value))\n",
    "    # End of lcp array; run through any residual accumulator values\n",
    "    while accumulator:\n",
    "        newly_closed_block = accumulator.pop()\n",
    "        newly_closed_block.lcp_end_offset = len(_lcp_array) - 1\n",
    "        if check_for_depth_and_repetition(newly_closed_block, len(witnesses)):\n",
    "            frequent_sequences.append([newly_closed_block.lcp_start_offset, len(_lcp_array)-1, newly_closed_block.lcp_interval_token_count])\n",
    "    return frequent_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# frequent_sequences is a list of lists\n",
    "# the embedded lists contain LCP indices\n",
    "#   LCP indices point into LCP array, but same index also points into suffix array\n",
    "#   value in LCP array points to prefix length (compared to previous one)\n",
    "#   value in suffix array points into token array\n",
    "frequent_sequences = create_blocks(lcp_array)\n",
    "# print(len(frequent_sequences))\n",
    "# pp.pprint(frequent_sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # A sequence like [40, 41, 42, 43] represents the same prefix. Each of these is a position in the LCP array that represents the length of the prefix.\n",
    "# for i in range(40, 44):\n",
    "#     print(f\"{lcp_array[i]=}\")\n",
    "#     print(f\"{token_array[suffix_array.SA[i]]=}\")\n",
    "# # Returns: 193, 30, 78, 0\n",
    "# # The length of a block is the lowest value higher than 0, so in this case 30.\n",
    "# # The suffix_array is in suffix_array.SA. Each of the four values is for a specific witness, so choose the first one arbitrarily, so 193.\n",
    "# # Examine 193rd value in suffix array:\n",
    "# print(f\"{suffix_array.SA[193]=}\")\n",
    "# # This returns 3378. The suffix array value is a pointer into the token array. So:\n",
    "# print(f\"{token_array[3378]=}\")\n",
    "# # The blocks are in alphabetical order.\n",
    "# # Look at part of token string\n",
    "# \" \".join(token_array[3368:3388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look at results\n",
    "# NB: This is debug output only\n",
    "# print(f\"{suffix_array._LCP_values=}\")\n",
    "# print(f\"{suffix_array.SA=}\")\n",
    "# pp.pprint(witnesses)\n",
    "#\n",
    "# print(\"Values are lcp_start_offset, lcp_end_offset, and lcp_interval_token_count\")\n",
    "# print()\n",
    "# largest_blocks = {} # key is token end position, value is (length, [witness-start-positions])\n",
    "# for frequent_sequence in frequent_sequences:\n",
    "# #     print(f\"Before filtering: examining frequent sequence {frequent_sequence}\")\n",
    "#     length = frequent_sequence[2]\n",
    "#     suffix_array_values = [suffix_array.SA[i] for i in range(frequent_sequence[0], frequent_sequence[1] + 1)]\n",
    "#     tokens = [token_array[i] for i in range(suffix_array_values[0], suffix_array_values[0] + length)]\n",
    "#     token_end_position = min(suffix_array_values) + length # token end position for first witness\n",
    "#     if token_end_position not in largest_blocks: # first block with this end position, so create new key\n",
    "#         largest_blocks[token_end_position] = (length, suffix_array_values)\n",
    "#     else: # if new block is longer, replace old one with same key\n",
    "#         if length > largest_blocks[token_end_position][0]:\n",
    "#             largest_blocks[token_end_position] = (length, suffix_array_values)\n",
    "# for b in frequent_sequences:\n",
    "#     lcp_start_value = b[0]\n",
    "#     token_start_position = suffix_array.SA[lcp_start_value]\n",
    "#     token_count = b[2]\n",
    "#     tokens = token_array[token_start_position: token_start_position + token_count]\n",
    "#     print(b, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(suffix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest_blocks={154: (25, [129, 364, 599, 837, 1072]), 1: (1, [0, 236, 471, 706, 944]), 204: (11, [193, 428, 663, 901, 1131]), 219: (10, [209, 444, 679, 917, 1146]), 128: (5, [123, 358, 593, 831, 1066]), 4: (2, [2, 238, 473, 711, 949]), 54: (15, [39, 274, 509, 747, 984]), 84: (29, [55, 290, 525, 763, 1001]), 179: (8, [171, 406, 641, 879, 1110]), 28: (22, [6, 242, 477, 715, 952]), 122: (25, [97, 332, 567, 805, 1040]), 235: (15, [220, 455, 690, 928, 1156]), 93: (7, [86, 321, 556, 794, 1030]), 166: (11, [155, 390, 625, 863, 1097]), 191: (11, [180, 415, 650, 888, 1119]), 207: (2, [205, 440, 675, 913, 1143]), 34: (5, [29, 265, 500, 738, 975]), 38: (2, [36, 271, 506, 744, 980]), 36: (1, [35, 270, 505, 743, 983]), 169: (2, [167, 402, 637, 875, 1108]), 193: (2, [191, 426, 661, 899, 1144]), 96: (2, [94, 329, 564, 802, 1038])}\n"
     ]
    }
   ],
   "source": [
    "# To remove embedded prefixes:\n",
    "#\n",
    "# 1. Create dictionary with end position in witness 0 (arbitrarily) as key\n",
    "# 2. Set value of key to longest sequence with that end position\n",
    "# 3. Dictionary values will contain only longest frequent sequences, removing embedded ones,\n",
    "#    as tuples if (length, [token start positions for all witnesses])\n",
    "\n",
    "@dataclass\n",
    "class LongestSequence:\n",
    "    length: int\n",
    "    witness_start_and_end: List[int]\n",
    "\n",
    "def find_longest_sequences(_frequent_sequences, _suffix_array):\n",
    "    _largest_blocks = {} # key is token end position, value is (length, [witness-start-positions])\n",
    "    for frequent_sequence in _frequent_sequences:\n",
    "        length = frequent_sequence[2]\n",
    "        suffix_array_values = [_suffix_array.SA[i] for i in range(frequent_sequence[0], frequent_sequence[1] + 1)]\n",
    "        token_end_position = min(suffix_array_values) + length # token end position for first witness\n",
    "        if token_end_position not in _largest_blocks: # first block with this end position, so create new key\n",
    "            _largest_blocks[token_end_position] = (length, sorted(suffix_array_values))\n",
    "        else: # if new block is longer, replace old one with same key\n",
    "            if length > _largest_blocks[token_end_position][0]:\n",
    "                _largest_blocks[token_end_position] = (length, sorted(suffix_array_values))\n",
    "    return _largest_blocks\n",
    "\n",
    "largest_blocks = find_longest_sequences(frequent_sequences, suffix_array)\n",
    "print(f\"{largest_blocks=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2022-06-28 pre-NEH plan for rebuilding beam search\n",
    "\n",
    "## `perform_beam_search_step()`\n",
    "\n",
    "State is current position of pointers for all witnesses for item on current beam = block identifier  \n",
    "(NB: Previously we stored state as tuple of token offsets for all witnesses, but since they have to correspond at block level, it's simpler to store a block identifier)  \n",
    "(NB: We use the token offsets for scoring, since scoring is based on token [not block] count)  \n",
    "We also need pointers from each block to next in witness order (separate list for each witness)  \n",
    "\n",
    "Example:\n",
    "\n",
    "state: block A (100, 200, 300)\n",
    "\n",
    "block B (150, 220, z1)  \n",
    "block C (120, 240, z2)\n",
    "\n",
    "Next block after A for witness 0 is C; next block after A for witness 1 is B\n",
    "\n",
    "To find next blocks, order block IDs separately for each witness, e.g.:  \n",
    "\n",
    "w0 pointers: {A: C, C: B}  \n",
    "w1 pointers: {A: B, B: C}  \n",
    "\n",
    "List of dictionaries, where db_pointers[0] is for witness 0, etc.  \n",
    "db_pointers = [ {A: C, C: B}, {A: B, B: C} ]  \n",
    "\n",
    "NB: This may be what we meant by our `db_pointers` variable in our earlier implementation  \n",
    "\n",
    "## `evaluate_option()`\n",
    "\n",
    "Starting from current state (block), find next block for a single witness and score  \n",
    "Score for option: sum of token position + block length for each witness   \n",
    "Score is tokens placed; smaller is better because it leaves higher potential  \n",
    "\n",
    "We also need:  \n",
    "Start position for each witness (for scoring purposes)  \n",
    "Accumulate all blocks en route on beam option to avoid having to reconstruct path at the end   \n",
    "\n",
    "## Exit conditions\n",
    "\n",
    "Exit condition for beam option is when there is no next block:  \n",
    "Might still be good final value, but cannot be traversed further, so store and include in final ranking\n",
    "\n",
    "Exit condition for entire beam search is when the beam is empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(order=True, frozen=True, eq=True) # heapqueue is priority queue, so requires comparison\n",
    "class BeamOption:\n",
    "    score: int\n",
    "    state: tuple # TODO change to tuple of tuples (history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_option(data, state, witness_id):\n",
    "    \"\"\"Evaluate individual option for next move\n",
    "\n",
    "    Called from perform_beam_search_step()\n",
    "\n",
    "    Parameters:\n",
    "        data: list of lists, does not mutate\n",
    "        state: offsets of all pointers in all witnesses\n",
    "        witness_id: witness for which we are advancing by 1 position\n",
    "            and then moving all others to corresponding token value\n",
    "            TODO: Some of these will be duplicates; simplify\n",
    "\n",
    "    Returns:\n",
    "        new_state: adjusted pointers for all witnesses\n",
    "        score: number of tokens placed or skipped\n",
    "            low score is better, since it means more remaining potential\n",
    "            since state is list of offsets, sum(state) is the score\n",
    "        NB: If option would move out of range, return (None, None)\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Inside evaluate_option(); {state=}\")\n",
    "    next_witness_value = data[witness_id][state[witness_id] + 1]\n",
    "    new_state = []\n",
    "    for witness_tokens in data: # each witness as list of tokensf\n",
    "        new_state.append(witness_tokens.index(next_witness_value))\n",
    "    return tuple(new_state), sum(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perform_beam_search_step(data, state):\n",
    "    \"\"\"Evaluate and return all options for next move\n",
    "\n",
    "    Called from perform_beam_search()\n",
    "\n",
    "    Flow:\n",
    "        Find all unique token values if we advance each pointer by 1 position\n",
    "            May all be equal, different, or mixed\n",
    "            NB: If option would move pointer out of range, remove that option\n",
    "                from consideration\n",
    "        Call evaluate_option() with each unique next token value\n",
    "            evaluate_option() returns score for each unique option\n",
    "        Keep β highest score positions\n",
    "        Recur (move to evaluate when all pointers are at END)\n",
    "        Choose best of β options when recursion ends\n",
    "\n",
    "    Parameters:\n",
    "        data: list of lists, does not mutate\n",
    "        state: offsets of all pointers in all witnesses\n",
    "\n",
    "    Variables:\n",
    "        options : next token value to use for position\n",
    "        β best states at end of step\n",
    "\n",
    "    Returns:\n",
    "        all results (will be prioritized and filtered later)\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Inside perform_beam_search_step(); {state=}\")\n",
    "    all_scored_options = [] # items are lists of witness_id, state, score\n",
    "    for witness_id, witness_offset in enumerate(state[0]):\n",
    "        new_state_step, score = evaluate_option(data, state[0], witness_id)\n",
    "        # prepend new state to history of all states (tuple of tuples)\n",
    "        all_scored_options.append(BeamOption(score, (new_state_step,) + state))\n",
    "    if debug:\n",
    "        print(f\"Return from perform_beam_search_step() with {all_scored_options=}\")\n",
    "    return all_scored_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def deduplicate_beam(_beam, beam_size):\n",
    "    \"\"\"Return new beam without duplicates\n",
    "\n",
    "    Remove duplicate beam candidates that have same score and same most recent state.\n",
    "    Duplicates may have different histories before most recent state, but cannot have\n",
    "    different histories going forward. This means that we eliminate candidates as good\n",
    "    as the ones we keep, that is, look for *a* solution but not *all* solutions.\n",
    "\n",
    "    Parameters:\n",
    "        beam : list of BeamOption instances\n",
    "\n",
    "    Returns:\n",
    "        reduced beam with β or fewer best results after deduplication\n",
    "\n",
    "    TODO: We can deduplicate a priority queue by walking the list and building a new\n",
    "        list that contains only values that are not equal to their immediate preceding\n",
    "        neighbor in the input. This may be faster than converting to a set.\n",
    "    \"\"\"\n",
    "    heapify(list(set(_beam)))\n",
    "    return _beam[:beam_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Beam search (see 2022-04-26_beam-search-description.txt)\n",
    "# See also: https://blog.finxter.com/python-beam-search-algorithm/\n",
    "# At each stage, keep β best results\n",
    "# As long as all witnesses continue to agree, no branching;\n",
    "#   Begin to branch at first difference\n",
    "# For each best result, options are advance one in each witness (some may be the same) or skip in all\n",
    "# Weight is number of tokens no longer available (skipped or selected); keep lowest values\n",
    "# All branches end at END node\n",
    "# Best solution has largest number of placed tokens\n",
    "# Complexity = β * tier-count\n",
    "#\n",
    "# The full data set is the largest blocks, which is a dictionary, where:\n",
    "#   key: end position of witness 0\n",
    "#   value: tuple of two values\n",
    "#     t[0]: length of block (same for all witnesses)\n",
    "#     t[1]: list of start positions of block for all witnesses (in witness order)\n",
    "#\n",
    "# TODO: Should the token start positions for the block form a numpy matrix?\n",
    "# largest_blocks={154: (25, [129, 364, 599, 837, 1072]), 1: (1, [0, 236, 471, 706, 944]), 204: (11, [193, 428, 663, 901, 1131]), 219: (10, [209, 444, 679, 917, 1146]), 128: (5, [123, 358, 593, 831, 1066]), 4: (2, [2, 238, 473, 711, 949]), 54: (15, [39, 274, 509, 747, 984]), 84: (29, [55, 290, 525, 763, 1001]), 179: (8, [171, 406, 641, 879, 1110]), 28: (22, [6, 242, 477, 715, 952]), 122: (25, [97, 332, 567, 805, 1040]), 235: (15, [220, 455, 690, 928, 1156]), 93: (7, [86, 321, 556, 794, 1030]), 166: (11, [155, 390, 625, 863, 1097]), 191: (11, [180, 415, 650, 888, 1119]), 207: (2, [205, 440, 675, 913, 1143]), 34: (5, [29, 265, 500, 738, 975]), 38: (2, [36, 271, 506, 744, 980]), 36: (1, [35, 270, 505, 743, 983]), 169: (2, [167, 402, 637, 875, 1108]), 193: (2, [191, 426, 661, 899, 1144]), 96: (2, [94, 329, 564, 802, 1038])}\n",
    "#\n",
    "# 2022-06-25 status report:\n",
    "#\n",
    "# Exit condition to conclude the beam search:\n",
    "#\n",
    "# For each option on the beam we need to be able to advance every witness, that is, no witness can be at the end.\n",
    "# We read an exit condition for the beam search when there is no option on the beam for which every witness is not yet at the end.\n",
    "# Our earlier code is faulty because it checked only whether witness 0 was at the end, and there may be other options on the beam\n",
    "#   that we should also check.\n",
    "# We expect that if one option has a witness that reaches the end and another option doesn't, the former will have a poor score.\n",
    "# If the best option has nowhere to do, we are done.\n",
    "\n",
    "def perform_beam_search(data, beam_size):\n",
    "    \"\"\"Perform beam search, return β best results\n",
    "\n",
    "    Uses the preceding perform_beam_search_step() and evaluate_option() functions\"\"\"\n",
    "# This assumes that witness 0 is in order; what we should do is find the majority order\n",
    "#     last_token_pos = len(data[0]) - 1\n",
    "    last_token_pos = data[max(data.keys())][1][0]\n",
    "    beam = [BeamOption(0, ((tuple([-1] * len(witnesses)),)))] # (0, (-1, -1, -1, -1, -1))\n",
    "    if debug:\n",
    "        debug_counter = 0\n",
    "    while beam: # exit when beam is empty\n",
    "    # In a loop on each of β states\n",
    "    #     Call perform_beam_search_step() for each of β options\n",
    "    #     Keep all results\n",
    "    #     After last state, reset beam to new best states and break out of inner loop\n",
    "        if debug:\n",
    "            print()\n",
    "            print(f\"In perform_beam_search() at iteration {debug_counter=}; {beam=}\")\n",
    "            debug_counter += 1\n",
    "        h = [] # priority heap to accumulate step results, which will be new beam\n",
    "        for option in beam: # stop when no more tokens to advance\n",
    "            if debug:\n",
    "                print(f\"{option=}\")\n",
    "            for result_item in (perform_beam_search_step(data, option.state)):\n",
    "                if debug:\n",
    "                    print(f\"{result_item=}\")\n",
    "                if result_item not in h: # don't add more than once\n",
    "                    # FIXME: this is embarrassingly inefficient\n",
    "                    heappush(h, result_item)\n",
    "        if debug:\n",
    "            print(f\"Entire queue: {h=}\")\n",
    "            print(f\"About to continue with {beam=}\")\n",
    "        beam = deduplicate_beam(h, beam_size)\n",
    "    return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# debug = True\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# print(db_positions)\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m beam_result \u001B[38;5;241m=\u001B[39m \u001B[43mperform_beam_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlargest_blocks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36mperform_beam_search\u001B[0;34m(data, beam_size)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug:\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moption\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m result_item \u001B[38;5;129;01min\u001B[39;00m (\u001B[43mperform_beam_search_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moption\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m):\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug:\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult_item\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36mperform_beam_search_step\u001B[0;34m(data, state)\u001B[0m\n\u001B[1;32m     30\u001B[0m all_scored_options \u001B[38;5;241m=\u001B[39m [] \u001B[38;5;66;03m# items are lists of witness_id, state, score\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m witness_id, witness_offset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(state[\u001B[38;5;241m0\u001B[39m]):\n\u001B[0;32m---> 32\u001B[0m     new_state_step, score \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_option\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwitness_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# prepend new state to history of all states (tuple of tuples)\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     all_scored_options\u001B[38;5;241m.\u001B[39mappend(BeamOption(score, (new_state_step,) \u001B[38;5;241m+\u001B[39m state))\n",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36mevaluate_option\u001B[0;34m(data, state, witness_id)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug:\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInside evaluate_option(); \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m next_witness_value \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mwitness_id\u001B[49m\u001B[43m]\u001B[49m[state[witness_id] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     23\u001B[0m new_state \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m witness_tokens \u001B[38;5;129;01min\u001B[39;00m data: \u001B[38;5;66;03m# each witness as list of tokensf\u001B[39;00m\n",
      "\u001B[0;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "# debug = True\n",
    "# print(db_positions)\n",
    "beam_result = perform_beam_search(largest_blocks, 3) # returns β best results, choice is domain-specific\n",
    "# print(beam_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization code currently works from dictionary of longest blocks\n",
    "# We need only the values: the first item is the length and the second is the start position in all witnesses\n",
    "# largest_blocks={7: (7, [16, 0]), 15: (7, [24, 8])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# To visualize as vertical alignment table\n",
    "\n",
    "1. Last step of beam search returns full history of all final options. \n",
    "1. Each final option has a `score` and a `state`. \n",
    "1. Keep the option with the highest score (or one arbitrary option with the highest score in case of ties).\n",
    "1. Use state history to construct alignment table.\n",
    "1. The alignment is an alternation of aligned and non-aligned segments.\n",
    "\n",
    "For each state retrieve the witness offsets as described below. We need that for only one witness for the aligned segments, but we need to look at all of them in order to get the non-aligned segments.\n",
    "\n",
    "## What the history tells us:\n",
    "\n",
    "1. The numbers in the history are offsets into the witness lists in `db`, which is a list of lists.\n",
    "1. Each of those values is an offset into the token array.that points to the end (!) of an aligned sequence.\n",
    "1. The beginning of that sequence is available from `longest_prefixes`, which is a list of tuples that consist of the end position for witness 0 and an `MfsComponentCandidate` object. For example, `( 1363,\n",
    "    MfsComponentCandidate(first_lcp_interval=1918, last_lcp_interval=1921, length=84, witness_start_and_end=[(1280, 1363), (3909, 3992), (6652, 6735), (9408, 9491)]))` tells us (first item) that it contains\n",
    "    information for the block in which witness 0 ends at 1363, and its `witness_start_and_end` property is a list of tuples that give the start and end positions for all witness.\n",
    "1. … do stuff …\n",
    "1. The unaligned segments are the token sequences between the aligned segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": "\n    <html>\n        <head>\n            <style type=\"text/css\">\n                table, tr, th, td {border: 1px solid black; border-collapse: collapse;}\n                th, td {padding: 3px;}\n                td:first-child {text-align: right;}\n                .aligned { background-color: beige;}\n                .nonaligned { background-color: lightgray;}\n            </style></head><body><table><tr><th>Row</th>\n    <th>w0</th>\n<th>w1</th>\n<th>w2</th>\n<th>w3</th>\n<th>w4</th></tr>\n<tr class=\"aligned\"><td>0</td><td colspan=\"5\">Darwin</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>1859</td><td>1860</td><td>1861</td><td>1866 Causes of Variability.</td><td>1869 Causes of Variability.</td></td>\n<tr class=\"aligned\"><td>1</td><td colspan=\"5\">WHEN we</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>look to</td><td>look to</td><td>look to</td><td>look to</td><td>compare</td></td>\n<tr class=\"aligned\"><td>2</td><td colspan=\"5\">the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>us,</td><td>us,</td><td>us,</td><td>us,</td><td>us</td></td>\n<tr class=\"aligned\"><td>3</td><td colspan=\"5\">is, that they generally differ</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>much</td><td></td><td></td><td></td><td>from each other</td></td>\n<tr class=\"aligned\"><td>4</td><td colspan=\"5\">more</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td></td><td></td><td></td><td></td><td></td></td>\n<tr class=\"aligned\"><td>5</td><td colspan=\"5\">from each</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>other,</td><td>other</td><td>other</td><td>other</td><td>other more</td></td>\n<tr class=\"aligned\"><td>6</td><td colspan=\"5\">than do the individuals of any one species or variety in a state of nature.</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>When</td><td>When</td><td>When</td><td>When</td><td>And if</td></td>\n<tr class=\"aligned\"><td>7</td><td colspan=\"5\">we reflect on the vast diversity of the plants and animals which have been cultivated, and which have varied during all ages under the most different climates and treatment,</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>I think</td><td>I think</td><td>I think</td><td>I think</td><td></td></td>\n<tr class=\"aligned\"><td>8</td><td colspan=\"5\">we are driven to conclude that this</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>greater</td><td>great</td><td>great</td><td>great</td><td>great</td></td>\n<tr class=\"aligned\"><td>9</td><td colspan=\"5\">variability is</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>simply</td><td>simply</td><td>simply</td><td>simply</td><td></td></td>\n<tr class=\"aligned\"><td>10</td><td colspan=\"5\">due to our domestic productions having been raised under conditions of life not so uniform as, and somewhat different from, those to which the parent-species</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>have</td><td>have</td><td>have</td><td>have</td><td>had</td></td>\n<tr class=\"aligned\"><td>11</td><td colspan=\"5\">been exposed under nature. There</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>is,</td><td>is</td><td>is</td><td>is</td><td>is</td></td>\n<tr class=\"aligned\"><td>12</td><td colspan=\"5\">also, I think, some probability in the view propounded by Andrew Knight, that this variability may be partly connected with excess of food. It seems</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>pretty</td><td>pretty</td><td>pretty</td><td>pretty</td><td></td></td>\n<tr class=\"aligned\"><td>13</td><td colspan=\"5\">clear that organic beings must be exposed during several generations to</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>the</td><td>the</td><td>the</td><td>the</td><td></td></td>\n<tr class=\"aligned\"><td>14</td><td colspan=\"5\">new conditions</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>of life</td><td>of life</td><td>of life</td><td>of life</td><td></td></td>\n<tr class=\"aligned\"><td>15</td><td colspan=\"5\">to cause any appreciable amount of variation; and</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>that</td><td>that</td><td>that</td><td>that,</td><td>that,</td></td>\n<tr class=\"aligned\"><td>16</td><td colspan=\"5\">when the organisation has once begun to vary, it generally continues</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td></td><td></td><td></td><td></td><td>varying for many generations. No case is on record of a variable organism ceasing</td></td>\n<tr class=\"aligned\"><td>17</td><td colspan=\"5\">to vary</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td></td><td></td><td></td><td></td><td></td></td>\n<tr class=\"aligned\"><td>18</td><td colspan=\"5\">for many generations. No case is on record of a variable</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>being</td><td>being</td><td>being</td><td>being</td><td>organism</td></td>\n<tr class=\"aligned\"><td>19</td><td colspan=\"5\">ceasing to</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>be variable</td><td>be variable</td><td>be variable</td><td>be variable</td><td>vary</td></td>\n<tr class=\"aligned\"><td>20</td><td colspan=\"5\">under cultivation. Our oldest cultivated plants, such as wheat, still</td></tr>\n<tr class=\"nonaligned\"><td>Nonaligned</td><td>often</td><td>often</td><td>often</td><td>often</td><td></td></td>\n<tr class=\"aligned\"><td>21</td><td colspan=\"5\">yield new varieties: our oldest domesticated animals are still capable of rapid improvement or modification.</td></tr></table></body></html>",
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# largest_blocks={7: (7, [16, 0]), 15: (7, [24, 8])}\n",
    "from IPython.core.display import HTML\n",
    "table_top = \"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <style type=\"text/css\">\n",
    "                table, tr, th, td {border: 1px solid black; border-collapse: collapse;}\n",
    "                th, td {padding: 3px;}\n",
    "                td:first-child {text-align: right;}\n",
    "                .aligned { background-color: beige;}\n",
    "                .nonaligned { background-color: lightgray;}\n",
    "            </style></head><body><table><tr><th>Row</th>\n",
    "    \"\"\" + '\\n'.join(['<th>w' + str(i) + '</th>' for i in range(len(witnesses))]) + '</tr>'\n",
    "table_bottom = '</table></body></html>'\n",
    "table_contents = ''\n",
    "rows = []\n",
    "sorted_keys = sorted(largest_blocks.keys())\n",
    "#\n",
    "# Check for leading nonaligned tokens\n",
    "#\n",
    "first_block_offsets = [token_witness_offset_array[i] for i in sorted(largest_blocks[sorted_keys[0]][1])]\n",
    "leading_nonaligned_block = []\n",
    "leading_nonaligned_witness_count = 0\n",
    "leading_tokens = ''\n",
    "for witness_number in range(len(first_block_offsets)):\n",
    "    if first_block_offsets[witness_number] == 0:\n",
    "        content = '[None]'\n",
    "    else:\n",
    "        content = \" \".join(witnesses[witness_number][0:first_block_offsets[witness_number]])\n",
    "        leading_nonaligned_witness_count += 1\n",
    "    leading_nonaligned_block.append('<td>' + content + '</td>')\n",
    "if leading_nonaligned_witness_count:\n",
    "    leading_tokens = '<tr class=\"nonaligned\"><td>Nonaligned</td>' + \"\".join(leading_nonaligned_block) + '</tr>'\n",
    "#\n",
    "# Creates aligned rows (preceded by any non-aligned tokens)\n",
    "#\n",
    "row_number = -1\n",
    "for key_position, sorted_key in enumerate(sorted_keys):\n",
    "    block_data = largest_blocks[sorted_key]\n",
    "    block_length = block_data[0]\n",
    "    token_start_positions = sorted(block_data[1])\n",
    "    #\n",
    "    # Check for preceding non-aligned row\n",
    "    #\n",
    "    preceding_nonaligned_block = ''\n",
    "    if key_position != 0:\n",
    "        end_positions_of_previous_block = []\n",
    "        for start_position in largest_blocks[sorted_keys[key_position - 1]][1]:\n",
    "            end_positions_of_previous_block.append(start_position + largest_blocks[sorted_keys[key_position - 1]][0])\n",
    "        end_positions_of_previous_block.sort()\n",
    "        content = ['<tr class=\"nonaligned\"><td>Nonaligned</td>']\n",
    "        for nonaligned_segment_group in zip(end_positions_of_previous_block, token_start_positions):\n",
    "            content.append('<td>' + \" \".join(token_array[nonaligned_segment_group[0]:nonaligned_segment_group[1]]) + '</td>')\n",
    "        content.append('</td>')\n",
    "        preceding_nonaligned_block = \"\".join(content)\n",
    "    #\n",
    "    # Create aligned row\n",
    "    #\n",
    "    row = []\n",
    "    row_number += 1\n",
    "    row_start = '<tr class=\"aligned\"><td>' + str(row_number) + '</td>'\n",
    "    row_end = '</tr>'\n",
    "    row.append(row_start)\n",
    "    contents = token_array[token_start_positions[0]:token_start_positions[0] + block_length]\n",
    "    row.append('<td colspan=\"' + str(len(token_start_positions)) + '\">' + ' '.join(contents) + '</td>')\n",
    "    row.append(row_end)\n",
    "    rows.append(preceding_nonaligned_block)\n",
    "    rows.append(''.join(row))\n",
    "table_contents = '\\n'.join(rows)\n",
    "#\n",
    "# Check for trailing nonaligned tokens, create row if needed\n",
    "#\n",
    "last_aligned_block = largest_blocks[sorted_keys[-1]]\n",
    "last_aligned_block_length = last_aligned_block[0]\n",
    "last_aligned_block_end_positions = [start_position + last_aligned_block_length - 1 for start_position in sorted(last_aligned_block[1])]\n",
    "witness_lengths = [len(witness) for witness in witnesses]\n",
    "last_aligned_token_pos = [token_witness_offset_array[i] for i in last_aligned_block_end_positions]\n",
    "trailing_unaligned_token_counts = [witness_lengths[i] - last_aligned_token_pos[i] - 1 for i in range(len(witnesses))]\n",
    "\n",
    "trailing_nonaligned_block = []\n",
    "trailing_nonaligned_witness_count = 0\n",
    "trailing_tokens = ''\n",
    "for witness_number, token_count in enumerate(trailing_unaligned_token_counts):\n",
    "    if token_count == 0:\n",
    "        content = \"[None]\"\n",
    "    else:\n",
    "        content = \" \".join(witnesses[witness_number][-token_count])\n",
    "        trailing_nonaligned_witness_count += 1\n",
    "    trailing_nonaligned_block.append('<td>' + content + '</td>')\n",
    "if trailing_nonaligned_witness_count:\n",
    "    trailing_tokens = '<tr class=\"nonaligned\"><td>Nonaligned</td>' + \"\".join(trailing_nonaligned_block) + '</tr>'\n",
    "#\n",
    "# Create and render table\n",
    "#\n",
    "# print(witnesses)\n",
    "HTML(table_top + leading_tokens + table_contents + trailing_tokens + table_bottom)\n",
    "\n",
    "# 2022-06-14\n",
    "#\n",
    "# Where we are today\n",
    "#\n",
    "# We are not checking for transpositions (using the beam search);\n",
    "#   instead we assume no transpositions (correct for our current test data)\n",
    "# We number and output aligned blocks correctly\n",
    "# We output nonaligned blocks correctly\n",
    "#\n",
    "# TODO\n",
    "# Reimplement beam search to check for transpositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RESUME HERE\n",
    "\n",
    "1. We are not yet looking at a skip option, and we should.\n",
    "1. We have duplicate states and we shouldn't. If moving more than one witness takes us to the same state, we should keep only one (arbitrarily).\n",
    "1. We are not yet keeping track of our paths, so we can't reconstruct the best search result from start to finish\n",
    "\n",
    "Ad 2: With our red and black cat example, moving from the initial \"the\" to red and to black produces the same score but different states, so keep both. With our current non-diverging Darwin example, all successors have not only the same score, but also the same state, so we should simplify (in this case, our beam would contain only one item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dependency graph, with lots of simplifying assumptions\n",
    "#\n",
    "# Relations are from larger to smaller\n",
    "# Each value in LCP array corresponds to position in suffix array,\n",
    "#     which corresponds to position in token array,\n",
    "#     which corresponds to position in witness arrays\n",
    "# If there is already a path (chain) from A to B, do not create a direct edge\n",
    "# Requires that the length of the next block be shorter than the current one\n",
    "#   and that the start position in the next block be one less than that in the current block\n",
    "# The witnesses for a dependent block must be a subset (possibly equivalent) of the source of the dependency\n",
    "# dependencies = {}\n",
    "# for block_position in range(len(sorted_blocks) - 1):\n",
    "#     current_block = sorted_blocks[block_position]\n",
    "#     next_block = sorted_blocks[block_position + 1]\n",
    "#     if current_block.token_count > next_block.token_count \\\n",
    "#     and current_block.all_start_positions[0] == next_block.all_start_positions[0] - 1 \\\n",
    "#     and current_block.witnesses.issuperset(next_block.witnesses):\n",
    "#         dependencies[block_position] = block_position + 1\n",
    "# # print(dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# What to do next (in order)\n",
    "\n",
    "- [x] Create dataclass for beam option \\(initially score, witness, state\\)\n",
    "- [x] Remove witness identifier from beam option object; keep only score and state\n",
    "- [x] Save state as tuple instead of list\n",
    "- [x] Save history as tuple of tuples \\(stack\\), where most recent value is the current state\n",
    "- [x] Deduplicate options where score plus most recent state are the same. Earlier history may be different, but subsequent history can't be different if the most recent state is the same. We keep **a** \\(potential\\) best path, but not **all** potential best paths of the same score.\n",
    "- [x] Create visualization (vertical alignment table)\n",
    "- [ ] Process skip-one options for each witness\n",
    "- [ ] Deduplicate skip-one options before processing\n",
    "- [ ] Make beam size variable, depending on nature and extent of options (currently keep β best results, choosing arbitrarily in case of ties; perhaps keep β best scores, which may have more than β options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('i', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 96, 96, 40, 0, 5, 0, 1, 1, 1, 1, 0, 151, 95, 9, 1, 24, 105, 105, 49, 0, 2, 83, 83, 27, 0, 14, 95, 95, 39, 0, 39, 39, 39, 8, 0, 24, 24, 24, 8, 0, 27, 108, 52, 1, 0, 3, 0, 2, 232, 176, 32, 0, 181, 125, 39, 0, 4, 185, 129, 43, 1, 33, 33, 33, 2, 0, 159, 103, 17, 8, 0, 160, 104, 18, 9, 0, 25, 106, 106, 50, 0, 60, 60, 4, 16, 0, 171, 115, 29, 20, 1, 9, 215, 159, 15, 1, 9, 122, 66, 15, 1, 57, 57, 1, 13, 1, 153, 97, 11, 2, 1, 165, 109, 23, 14, 0, 9, 9, 9, 9, 1, 170, 114, 28, 19, 0, 8, 214, 158, 14, 0, 62, 62, 6, 18, 1, 10, 191, 135, 49, 0, 61, 61, 5, 17, 0, 9, 148, 92, 6, 1, 8, 8, 8, 8, 0, 19, 19, 19, 3, 0, 10, 123, 67, 16, 0, 6, 75, 75, 19, 1, 9, 90, 90, 34, 1, 28, 28, 28, 0, 167, 111, 25, 16, 1, 31, 112, 56, 5, 1, 19, 132, 76, 25, 0, 50, 50, 50, 6, 0, 31, 31, 31, 0, 8, 77, 77, 21, 0, 16, 97, 97, 41, 0, 6, 6, 6, 6, 0, 38, 38, 38, 7, 0, 63, 63, 7, 19, 0, 30, 30, 30, 2, 0, 11, 80, 80, 24, 0, 154, 98, 12, 3, 0, 0, 6, 145, 89, 3, 0, 16, 129, 73, 22, 3, 67, 67, 11, 1, 0, 7, 88, 88, 32, 0, 45, 45, 45, 1, 0, 11, 217, 161, 17, 1, 22, 22, 22, 6, 0, 166, 110, 24, 15, 0, 25, 25, 25, 9, 0, 1, 201, 145, 1, 0, 155, 99, 13, 4, 1, 7, 120, 64, 13, 0, 175, 119, 33, 24, 0, 14, 195, 139, 53, 0, 22, 135, 79, 28, 0, 10, 10, 10, 10, 0, 8, 147, 91, 5, 0, 25, 138, 82, 31, 0, 161, 105, 19, 10, 1, 4, 73, 73, 17, 0, 2, 198, 142, 1, 0, 5, 86, 86, 30, 0, 5, 74, 74, 18, 1, 30, 111, 55, 4, 0, 4, 210, 154, 10, 0, 3, 84, 84, 28, 0, 42, 42, 42, 11, 0, 3, 199, 143, 2, 0, 6, 119, 63, 12, 0, 46, 46, 46, 2, 1, 2, 202, 146, 2, 0, 2, 71, 71, 15, 0, 40, 40, 40, 9, 0, 3, 142, 86, 0, 0, 0, 52, 52, 52, 8, 0, 168, 112, 26, 17, 2, 113, 57, 6, 1, 163, 107, 21, 12, 0, 20, 133, 77, 26, 0, 0, 3, 3, 3, 3, 0, 5, 186, 130, 44, 1, 20, 101, 101, 45, 0, 12, 193, 137, 51, 2, 21, 227, 171, 27, 0, 26, 107, 51, 1, 1, 37, 37, 37, 6, 1, 140, 84, 33, 0, 1, 5, 205, 149, 5, 0, 47, 47, 47, 3, 0, 14, 127, 71, 20, 1, 65, 65, 9, 0, 230, 174, 30, 0, 41, 41, 41, 10, 0, 10, 91, 91, 35, 0, 1, 1, 1, 1, 0, 200, 144, 3, 1, 0, 156, 100, 14, 5, 0, 0, 7, 76, 76, 20, 0, 1, 28, 109, 53, 2, 1, 182, 126, 40, 0, 68, 68, 12, 2, 1, 14, 14, 14, 14, 0, 13, 126, 70, 19, 0, 4, 1, 34, 34, 34, 3, 1, 11, 192, 136, 50, 1, 4, 85, 85, 29, 1, 15, 128, 72, 21, 2, 66, 66, 10, 1, 2, 183, 127, 41, 1, 14, 220, 164, 20, 1, 5, 5, 5, 5, 1, 6, 212, 156, 12, 2, 174, 118, 32, 23, 2, 20, 226, 170, 26, 1, 59, 59, 3, 15, 0, 16, 16, 16, 0, 12, 218, 162, 18, 0, 23, 23, 23, 7, 1, 11, 11, 11, 11, 0, 36, 36, 36, 5, 1, 178, 122, 36, 27, 0, 51, 51, 51, 7, 0, 7, 213, 157, 13, 1, 9, 190, 134, 48, 0, 2, 2, 2, 2, 1, 16, 222, 166, 22, 1, 7, 188, 132, 46, 0, 9, 78, 78, 22, 0, 53, 53, 53, 9, 0, 0, 1, 197, 141, 0, 0, 23, 136, 80, 29, 1, 13, 219, 163, 19, 1, 12, 12, 12, 12, 0, 1, 114, 58, 7, 0, 8, 89, 89, 33, 0, 172, 116, 30, 21, 2, 10, 216, 160, 16, 0, 21, 21, 21, 5, 0, 3, 209, 153, 9, 0, 81, 81, 25, 0, 21, 102, 102, 46, 0, 21, 134, 78, 27, 0, 17, 98, 98, 42, 0, 18, 131, 75, 24, 0, 4, 4, 4, 4, 0, 35, 35, 35, 4, 0, 179, 123, 37, 28, 0, 18, 224, 168, 24, 0, 1, 82, 82, 26, 0, 3, 72, 72, 16, 0, 139, 83, 32, 0, 12, 125, 69, 18, 0, 22, 103, 103, 47, 0, 8, 121, 65, 14, 0, 8, 189, 133, 47, 0, 3, 184, 128, 42, 0, 7, 7, 7, 7, 1, 17, 17, 17, 1, 0, 1, 207, 151, 7, 0, 15, 221, 165, 21, 0, 20, 20, 20, 4, 0, 15, 196, 140, 54, 0, 10, 79, 79, 23, 1, 4, 204, 148, 4, 1, 5, 144, 88, 2, 2, 13, 94, 94, 38, 1, 56, 56, 0, 12, 0, 5, 211, 155, 11, 1, 13, 194, 138, 52, 3, 22, 228, 172, 28, 1, 157, 101, 15, 6, 1, 69, 69, 13, 1, 54, 54, 54, 10, 1, 2, 115, 59, 8, 1, 173, 117, 31, 22, 1, 19, 225, 169, 25, 1, 177, 121, 35, 26, 1, 19, 100, 100, 44, 0, 3, 203, 147, 3, 0, 150, 94, 8, 0, 23, 104, 104, 48, 0, 4, 143, 87, 1, 1, 12, 93, 93, 37, 0, 5, 118, 62, 11, 0, 29, 29, 29, 1, 64, 64, 8, 20, 1, 7, 146, 90, 4, 1, 1, 24, 137, 81, 30, 1, 229, 173, 29, 2, 70, 70, 14, 1, 44, 44, 44, 2, 1, 49, 49, 49, 5, 1, 4, 117, 61, 10, 0, 152, 96, 10, 1, 0, 17, 130, 74, 23, 1, 26, 26, 26, 10, 1, 29, 110, 54, 3, 1, 158, 102, 16, 7, 0, 11, 124, 68, 17, 0, 0, 206, 150, 6, 0, 2, 141, 85, 34, 1, 11, 92, 92, 36, 0, 32, 32, 32, 1, 1, 27, 27, 27, 0, 58, 58, 2, 14, 0, 162, 106, 20, 11, 0, 13, 13, 13, 13, 0, 6, 187, 131, 45, 1, 17, 223, 167, 23, 0, 43, 43, 43, 1, 0, 48, 48, 48, 4, 0, 0, 176, 120, 34, 25, 0, 18, 99, 99, 43, 0, 10, 149, 93, 7, 1, 1, 231, 175, 31, 1, 180, 124, 38, 29, 0, 18, 18, 18, 2, 0, 55, 55, 55, 11, 0, 169, 113, 27, 18, 2, 164, 108, 22, 13, 1, 2, 208, 152, 8, 1, 3, 116, 60, 9, 0, 6, 87, 87, 31, 0, 15, 15, 15, 15])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_array._LCP_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are:\n",
      "  offset into suffix array,\n",
      "  value in suffix array (pointer to first token in token_array),\n",
      "  value in LCP array (length of prefix),\n",
      "  tokens (if any)\n",
      "\n",
      "87 1072 0 \n",
      "\n",
      "88 129 25 also, I think, some probability in the view propounded by Andrew Knight, that this variability may be partly connected with excess of food. It seems\n",
      "\n",
      "89 364 106 also, I think, some probability in the view propounded by Andrew Knight, that this variability may be partly connected with excess of food. It seems pretty clear that organic beings must be exposed during several generations to the new conditions of life to cause any appreciable amount of variation; and that when the organisation has once begun to vary, it generally continues to vary for many generations. No case is on record of a variable being ceasing to be variable under cultivation. Our oldest cultivated plants, such as wheat, still often yield new varieties: our oldest domesticated animals are still capable of rapid improvement or modification.\n",
      "\n",
      "90 599 106 also, I think, some probability in the view propounded by Andrew Knight, that this variability may be partly connected with excess of food. It seems pretty clear that organic beings must be exposed during several generations to the new conditions of life to cause any appreciable amount of variation; and that when the organisation has once begun to vary, it generally continues to vary for many generations. No case is on record of a variable being ceasing to be variable under cultivation. Our oldest cultivated plants, such as wheat, still often yield new varieties: our oldest domesticated animals are still capable of rapid improvement or modification.\n",
      "\n",
      "91 837 50 also, I think, some probability in the view propounded by Andrew Knight, that this variability may be partly connected with excess of food. It seems pretty clear that organic beings must be exposed during several generations to the new conditions of life to cause any appreciable amount of variation; and\n",
      "\n",
      "92 175 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1072\t'also,Ithin'\tLCP=0 \n",
    "# 129\t'also,Ithin'\tLCP=25 \n",
    "# 364\t'also,Ithin'\tLCP=106 \n",
    "# 599\t'also,Ithin'\tLCP=106 \n",
    "# 837\t'also,Ithin'\tLCP=50 \n",
    "\n",
    "print(\"Values are:\\n  offset into suffix array,\\n  value in suffix array (pointer to first token in token_array),\\n  value in LCP array (length of prefix),\\n  tokens (if any)\")\n",
    "print()\n",
    "for offset, sa_item in enumerate(suffix_array.SA):\n",
    "    lcp_value = suffix_array._LCP_values[offset]\n",
    "    tokens = token_array[sa_item: sa_item + lcp_value]\n",
    "    if offset in range(87, 93):\n",
    "        print(offset, sa_item, lcp_value, \" \".join(tokens))\n",
    "        print()\n",
    "\n",
    "# 2022-06-17 RESUME HERE\n",
    "# For each step in this interval check accumulator and frequent sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def priority(block: Block) -> float:\n",
    "#     '''Priority ranges from 0 to ∞\n",
    "\n",
    "#     depth (number of witnesses) / (frequency * length)\n",
    "#         modified (by trial and error) to weight the components\n",
    "#     scale: # TODO: how can we set these in a generally meaningful way?\n",
    "#         high depth (more witnesses) is most important\n",
    "#         low frequency (less repetition) is next most important\n",
    "#         high length (token count) is least important\n",
    "#     higher numbers are better\n",
    "#         distance between neighboring values is irrelevant; all that matters is order\n",
    "#     '''\n",
    "#     # score = pow(block.witness_count,4) / (pow(block.frequency,3) * block.token_count)\n",
    "#     score = pow(block.witness_count,6)  * block.token_count / pow(block.frequency,3)\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def sort_blocks_by_priority (_blocks: List[Block]) -> List[tuple]:\n",
    "#     blocks_to_tuples = [(_block, index) for index, _block in enumerate(_blocks)]\n",
    "#     return sorted(blocks_to_tuples, key=lambda x: priority(x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prioritized_blocks = sort_blocks_by_priority(sorted_blocks) # sorted_blocks has been sorted for dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# witness_sigla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prioritized_blocks[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}