{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# We will need to call create blocks.\n",
    "# From the blocks we create the dependency structure.\n",
    "# Easiest is to start from a single witness and go from there.\n",
    "# By going in the token order there is a decent chance that the parent of a adjacent block already exists within the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from typing import List\n",
    "from linsuffarr import SuffixArray\n",
    "from linsuffarr import UNIT_BYTE\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# sigla = ['w0', 'w1', 'w2', 'w3', 'w4', 'w5']\n",
    "sigla = ['w0', 'w1', 'w2', 'w3']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt', 'darwin1861.txt', 'darwin1866.txt', 'darwin1869.txt', 'darwin1872.txt']\n",
    "filenames = ['darwin1859.txt', 'darwin1860.txt', 'darwin1861.txt', 'darwin1866.txt']\n",
    "first_paragraph = 0\n",
    "last_paragraph = 3\n",
    "how_many_paragraphs = last_paragraph - first_paragraph\n",
    "raw_data_dict = {}\n",
    "for siglum, filename in zip(sigla, filenames):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line for line in lines if line != '\\n']\n",
    "        raw_data_dict[siglum] = \" \".join(lines[first_paragraph : last_paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize_witnesses(witness_strings: List[str]): # one string per witness\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    witnesses = []\n",
    "    for witness_string in witness_strings:\n",
    "        witness_tokens = witness_string.split()\n",
    "        witnesses.append(witness_tokens)\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_token_array(witness_token_lists): # list of token lists per witness\n",
    "    '''Create token array (single list, with separator \" # \" between witnesses'''\n",
    "    token_array = []\n",
    "    token_membership_array = []\n",
    "    token_witness_offset_array = []\n",
    "    last_witness_offset = len(witness_token_lists) - 1\n",
    "    for index, witness_token_list in enumerate(witness_token_lists):\n",
    "        token_array.extend(witness_token_list)\n",
    "        for token_offset, token in enumerate(witness_token_list):\n",
    "            token_witness_offset_array.append(token_offset)\n",
    "        token_membership_array.extend([index for token in witness_token_list])\n",
    "        if index < last_witness_offset:\n",
    "            separator = \" #\" + str(index + 1) + \" \"\n",
    "            token_array.append(separator)\n",
    "            token_membership_array.append(separator)\n",
    "            token_witness_offset_array.append(-1)\n",
    "    return token_array, token_membership_array, token_witness_offset_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "witness_sigla = [key for key in raw_data_dict.keys()]\n",
    "witnesses = tokenize_witnesses([value for value in raw_data_dict.values()]) # strings\n",
    "# token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "token_array, token_membership_array, token_witness_offset_array = create_token_array(witnesses)\n",
    "# print(f\"{token_array=}\")\n",
    "# print(f\"{token_membership_array=}\")\n",
    "# print(f\"{token_witness_offset_array=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "suffix_array = SuffixArray(token_array, unit=UNIT_BYTE)\n",
    "# print(suffix_array)\n",
    "# LCP=0 means that the block has nothing in common with the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "lcp_array = suffix_array._LCP_values\n",
    "# lcp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create Block dataclass\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not into token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    witnesses: set\n",
    "    witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    # how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_blocks (_lcp_array):\n",
    "    '''Create blocks from lcp array\n",
    "\n",
    "    Skip first lcp value, which is a fake; otherwise compare lcp value to length of block at top of stack.\n",
    "    Four possibilities:\n",
    "\n",
    "        stack is empty\n",
    "            * if lcp value == 0, proceed to next lcp value (continue)\n",
    "            * if lcp value > 0, create block and push onto stack, then proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) matches block length at top of stack\n",
    "            * proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) is longer than block length at top of stack\n",
    "            * create and push new block\n",
    "\n",
    "        lcp value is shorter than block length at top of stack\n",
    "            * (recursive) if block at top of stack is longer than current lcp value, pop and append to _blocks\n",
    "            * if block at top of stack is equal to lcp value, proceed to next lcp value (continue)\n",
    "            * if block at top of stack is shorter than current lcp value ...\n",
    "            *   create and push new block starting at start position of most recently closed block, then proceed to next lcp value (continue)\n",
    "\n",
    "    In other words:\n",
    "\n",
    "        We proceed to next lcp value if:\n",
    "            * stack is empty and lcp value == 0\n",
    "            * lcp value matches block length at top of stack (can we combine this with the preceding, since an empty stack effectively has a zero-length block on top?)\n",
    "\n",
    "        We push a new value on stack and then proceed to next lcp value if:\n",
    "            * stack is empty and lcp value > 0\n",
    "            * lcp value is longer than block length at top of stack (where is the start position?)\n",
    "\n",
    "        We pop from the stack to _blocks and then check the next stack value (stick with same lcp) if:\n",
    "            * lcp value is shorter than current block value\n",
    "\n",
    "cases (occurrences are always one more than number of repetitions):\n",
    "    5 5 2     --> 1 block of 5 occures 3 times, 1 block of 2 occures 4 times\n",
    "    2 5 5 2   --> 1 block of 2 occures 5 times, 1 block of 5 occures 3 times\n",
    "    5 5 0 2   --> 1 block of 5 occures 3 times, 1 block of 2 occures 2 times\n",
    "    2 5 5 2 3 --> \n",
    "\n",
    "\n",
    "Nested while structures:\n",
    "\n",
    "(Create blocks in two places because they have different start positions)\n",
    "(Nested while loops because we traverse two things: lcp array and, sometimes, stack)\n",
    "\n",
    "while next-lcp-value: # traverse lcp array\n",
    "    if something\n",
    "    elif something else\n",
    "    elif perhaps yet another something else\n",
    "    else: # possible hidden block (or possibly not)\n",
    "        while something-on-the-stack: # traverse stack for some lcp value situations\n",
    "            pop larger values\n",
    "        if hidden-block:\n",
    "            create and push\n",
    "clean-up-stack-after-last-lcp-value # or tack a 0 onto the end of the lcp to avoid extra clean-up code\n",
    "'''\n",
    "    from collections import deque # deque has faster append and pop than list\n",
    "    _blocks = []\n",
    "    open_block_stack = deque()\n",
    "    for offset, lcp in enumerate(lcp_array):\n",
    "        # three situations: next one is same value, higher that last, or lower than last\n",
    "        # if same value: same pattern\n",
    "        # if higher or lower, new pattern (may overlap with previous, unless one or the other value is 0)\n",
    "        peek = open_block_stack[-1] if open_block_stack else None\n",
    "        peek_token_count = peek.token_count if peek else 0\n",
    "        if offset == 0: # skip the first one, which is a transition from a fake start value\n",
    "            continue # resume loop with next item in lcp array\n",
    "        elif lcp == peek_token_count:\n",
    "            pass # same pattern (happens with repetition), so do nothing\n",
    "        elif lcp > peek_token_count: # new prefix is longer than previous one, so start new pattern\n",
    "            # can fill in end_position and frequency only when we encounter a shorter value in the LCP array\n",
    "            # start_position is number of patterns that are the same \n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = offset - 1, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "        else: # new prefix is shorter than previous one, so:\n",
    "                # 1. close open blocks with higher values\n",
    "                # 2. do something else\n",
    "            while open_block_stack and open_block_stack[-1].token_count > lcp: # if an open block is longer than the current length, pop and close it\n",
    "                block_being_modified = open_block_stack.pop()\n",
    "                block_being_modified.end_position = offset - 1\n",
    "                block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "                _blocks.append(block_being_modified)\n",
    "            if lcp > 0 and (not open_block_stack or open_block_stack[-1].token_count < lcp):\n",
    "                open_block_stack.append(Block(token_count = lcp, start_position = _blocks[-1].start_position, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "\n",
    "    while open_block_stack: # pop anything left in open_block_stack\n",
    "        block_being_modified = open_block_stack.pop()\n",
    "        block_being_modified.end_position = len(lcp_array) - 1\n",
    "        block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "        _blocks.append(block_being_modified)\n",
    "\n",
    "    # add all_start_positions and then witness_count properties to blocks\n",
    "    for _index, _block in enumerate(_blocks):\n",
    "        # block_start_position through block_end_position gives offsets of all start positions in suffix_array\n",
    "        _block.all_start_positions = sorted([suffix_array.SA[x] for x in range(_block.start_position,_block.end_position + 1)])\n",
    "        # use all start positions to find witness count\n",
    "        _block.witnesses = set(token_membership_array[offset] for offset in _block.all_start_positions)\n",
    "        _block.witness_count = len(_block.witnesses)\n",
    "    return _blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "blocks = create_blocks(lcp_array)\n",
    "# pp.pprint(blocks) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "blocks_we_care_about = [block for block in blocks if block.witness_count == block.frequency] # remove repetition\n",
    "# sort by token order in w0\n",
    "sorted_blocks = sorted(blocks_we_care_about, key=lambda x: x.all_start_positions[0])\n",
    "# pp.pprint(sorted_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# for index, block in enumerate(sorted_blocks):\n",
    "#     print('block', index, 'occurs', int(block.frequency / 2), 'time(s) per witness and contains',token_array[block.all_start_positions[0]:block.all_start_positions[0] + block.token_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# block.witnesses is a set of witnesses represented in the block\n",
    "# pp.pprint(sorted_blocks[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# Dependency graph, with lots of simplifying assumptions\n",
    "#\n",
    "# Relations are from larger to smaller\n",
    "# If there is already a path (chain) from A to B, do not create a direct edge\n",
    "# Requires that the length of the next block be shorter than the current one\n",
    "#   and that the start position in the next block be one less than that in the current block\n",
    "# The witnesses for a dependent block must be a subset (possibly equivalent) of the source of the dependency\n",
    "dependencies = {}\n",
    "for block_position in range(len(sorted_blocks) - 1):\n",
    "    current_block = sorted_blocks[block_position]\n",
    "    next_block = sorted_blocks[block_position + 1]\n",
    "    if current_block.token_count > next_block.token_count \\\n",
    "    and current_block.all_start_positions[0] == next_block.all_start_positions[0] - 1 \\\n",
    "    and current_block.witnesses.issuperset(next_block.witnesses):\n",
    "        dependencies[block_position] = block_position + 1\n",
    "# print(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Do the following in the calling notebook (not here)\n",
    "#\n",
    "# We can skip a block iff we have already aligned (not just seen) a block that controls it\n",
    "#\n",
    "# Keep track of blocks already processed (bitarray)\n",
    "# Traverse blocks in priority queue (not the same as the sorted order for identifying dependencies)\n",
    "# When we process a block:\n",
    "#     If marked as already processed in bitarray, move on\n",
    "#     Otherwise mark its dependencies as processed recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def priority(block: Block) -> float:\n",
    "    '''Priority ranges from 0 to ∞\n",
    "\n",
    "    depth (number of witnesses) / (frequency * length)\n",
    "        modified (by trial and error) to weight the components\n",
    "    scale: # TODO: how can we set these in a generally meaningful way?\n",
    "        high depth (more witnesses) is most important\n",
    "        low frequency (less repetition) is next most important\n",
    "        high length (token count) is least important\n",
    "    higher numbers are better\n",
    "        distance between neighboring values is irrelevant; all that matters is order\n",
    "    '''\n",
    "    # score = pow(block.witness_count,4) / (pow(block.frequency,3) * block.token_count)\n",
    "    score = pow(block.witness_count,6)  * block.token_count / pow(block.frequency,3)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def sort_blocks_by_priority (_blocks: List[Block]) -> List[tuple]:\n",
    "    blocks_to_tuples = [(_block, index) for index, _block in enumerate(_blocks)]\n",
    "    return sorted(blocks_to_tuples, key=lambda x: priority(x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "prioritized_blocks = sort_blocks_by_priority(sorted_blocks) # sorted_blocks has been sorted for dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# prioritized_blocks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# len(blocks), len(sorted_blocks), len(prioritized_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# dependencies.clear()\n",
    "# print(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w0', 'w1', 'w2', 'w3']"
      ]
     },
     "execution_count": 24,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "witness_sigla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}