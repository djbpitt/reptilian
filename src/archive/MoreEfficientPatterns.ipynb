{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Store patterns efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# two witnesses, with repetition and transposition\n",
    "\n",
    "# Original example, single leaf node\n",
    "w1 = '''the red and the black cat'''\n",
    "w2 = '''the black and the red cat'''\n",
    "\n",
    "# Adjacent transposition\n",
    "# w1 = '''the red striped cat'''\n",
    "# w2 = '''the striped red cat'''\n",
    "\n",
    "# Two leaf nodes\n",
    "# w1 = '''cat red black'''\n",
    "# w2 = '''cat black red'''\n",
    "\n",
    "# Branches meet in the middle at koala and then split again, with two leaf nodes\n",
    "# w1 = \"\"\"cat red black koala brown gray\"\"\"\n",
    "# w2 = \"\"\"cat black red koala gray brown\"\"\"\n",
    "\n",
    "# Two split and rejoin\n",
    "# w1 = '''the gray koala'''\n",
    "# w2 = '''the brown koala'''\n",
    "\n",
    "# medium example\n",
    "# w1 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ much more from each other, than do the individuals of any one\n",
    "# species or variety in a state of nature.'''\n",
    "# w2 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ more from each other than do the individuals of any one\n",
    "# species or variety in a state of nature.'''\n",
    "\n",
    "# Larger example\n",
    "# w1 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ much more from each other, than do the individuals of any one\n",
    "# species or variety in a state of nature. When we reflect on the vast diversity of the\n",
    "# plants and animals which have been cultivated, and which have varied during all ages\n",
    "# under the most different climates and treatment, I think we are driven to conclude that\n",
    "# this greater variability is simply due to our domestic productions having been raised\n",
    "# under conditions of life not so uniform as, and somewhat different from, those to which\n",
    "# the parent-species have been exposed under nature. There is, also, I think, some\n",
    "# probability in the view propounded by Andrew Knight, that this variability may be partly\n",
    "# connected with excess of food. It seems pretty clear that organic beings must be exposed\n",
    "# during several generations to the new conditions of life to cause any appreciable amount\n",
    "# of variation; and that when the organisation has once begun to vary, it generally\n",
    "# continues to vary for many generations. No case is on record of a variable being ceasing\n",
    "# to be variable under cultivation. Our oldest cultivated plants, such as wheat, still\n",
    "# often yield new varieties: our oldest domesticated animals are still capable of rapid\n",
    "# improvement or modification.'''\n",
    "# w2 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ more from each other than do the individuals of any one\n",
    "# species or variety in a state of nature. When we reflect on the vast diversity of the\n",
    "# plants and animals which have been cultivated, and which have varied during all ages\n",
    "# under the most different climates and treatment, I think we are driven to conclude that\n",
    "# this great variability is simply due to our domestic productions having been raised\n",
    "# under conditions of life not so uniform as, and somewhat different from, those to which\n",
    "# the parent-species have been exposed under nature. There is also, I think, some\n",
    "# probability in the view propounded by Andrew Knight, that this variability may be partly\n",
    "# connected with excess of food. It seems pretty clear that organic beings must be exposed\n",
    "# during several generations to the new conditions of life to cause any appreciable amount\n",
    "# of variation; and that when the organisation has once begun to vary, it generally\n",
    "# continues to vary for many generations. No case is on record of a variable being ceasing\n",
    "# to be variable under cultivation. Our oldest cultivated plants, such as wheat, still\n",
    "# often yield new varieties: our oldest domesticated animals are still capable of rapid\n",
    "# improvement or modification'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Work plan\n",
    "\n",
    "1. Create token array (Python **list**)\n",
    "1. Create suffix array\n",
    "1. Create LCP (**longest common prefix**) array\n",
    "1. Calculate LCP intervals\n",
    "1. Create patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Construct list of ngrams shared by witnesses\n",
    "\n",
    "Find ngrams and positions in witnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize_witnesses(w1_string, w2_string):\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    w1_tokens = w1.split()\n",
    "    w2_tokens = w2.split()\n",
    "    witnesses = [w1_tokens, w2_tokens]\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'red', 'and', 'the', 'black', 'cat'], ['the', 'black', 'and', 'the', 'red', 'cat']]\n"
     ]
    }
   ],
   "source": [
    "witnesses = tokenize_witnesses(w1, w2)\n",
    "print(witnesses) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'the'),\n",
       " (1, 'red'),\n",
       " (2, 'and'),\n",
       " (3, 'the'),\n",
       " (4, 'black'),\n",
       " (5, 'cat'),\n",
       " (6, ' # '),\n",
       " (7, 'the'),\n",
       " (8, 'black'),\n",
       " (9, 'and'),\n",
       " (10, 'the'),\n",
       " (11, 'red'),\n",
       " (12, 'cat')]"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create token array\n",
    "# All tokens from both witnesses in a single list, with a separator (\" # \") between witnesses\n",
    "token_array = []\n",
    "token_array.extend(witnesses[0])\n",
    "token_array.append(\" # \")\n",
    "token_array.extend(witnesses[1])\n",
    "[(index, value) for index,value in enumerate(token_array)] # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# determine suffixes of token array\n",
    "# output with print() is diagnostic\n",
    "# for index, token in enumerate(token_array):\n",
    "#     suffix = token_array[index:]\n",
    "#     print(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# suffix array is sorted alphabetically\n",
    "# tuples of list of tokens (suffix) and position in original list of tokens\n",
    "# TODO: less naive implementation\n",
    "suffixes = []\n",
    "for index, token in enumerate(token_array):\n",
    "    suffix = token_array[index:]\n",
    "    suffixes.append((suffix, index))\n",
    "suffixes.sort() # sort in place\n",
    "# suffixes # take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that suffixes that start at position 2 and 9 both start with \"and the\", which tells us that:\n",
    "\n",
    "1. There is a repeated suffix \"and the\"\n",
    "1. \"and\" appears without \"the\"\n",
    "\n",
    "Ergo, we don't need a unigram \"and\".\n",
    "\n",
    "Similarly, other ngrams occur repeatedly: \"the red\" twice, \"the\" four times, etc. Occurrences of \"and\" are easy because they are only \"and the\", while \"the\" occurs in different contexts, e.g., twice in \"the red\", twice in \"the black\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suffix array is list of offsets of start positions of suffixes sorted alphabetically\n",
    "# Suffix array is economical because it is equal to the sum of the lengths of witnesses plus the number of witnesses - 1 (for the separators)\n",
    "suffix_array = []\n",
    "for suffix, index in suffixes:\n",
    "    suffix_array.append(index)\n",
    "suffix_array[80:90] # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# compute LCP array\n",
    "# sequential pairs of values in suffix array, which are two offsets in the sorted suffixes\n",
    "# for i in range(0, len(suffix_array) - 1):\n",
    "#     print (suffix_array[i:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# compute LCP array, which is a sequence of integers representing the number of tokens shared by consecutive alphabetically sorted suffixes\n",
    "# sequential pairs of values in suffix array, which are two offsets in the sorted suffixes\n",
    "# TODO: we now need the longest prefix\n",
    "lcp_array = [0]\n",
    "for i in range(0, len(suffix_array) - 1): # for each pair of suffixes, retrieve list of tokens starting at that position\n",
    "    pair = suffix_array[i:i+2] # for each pair of suffixes\n",
    "    suffix_1 = token_array[pair[0]:] # tokens starting at first position\n",
    "    suffix_2 = token_array[pair[1]:] # tokens starting at second position\n",
    "    # print(suffix_1, suffix_2) # diagnostic: verify that they're paired correctly\n",
    "    lcp_value = next(filter(lambda t: t[1][0] != t[1][1], enumerate(zip(suffix_1, suffix_2))), min(len(suffix_1), len(suffix_2))) # pair the tokens up by position, return (number of matches, first non-match)\n",
    "    lcp_array.append(lcp_value[0] if type(lcp_value) == tuple else lcp_value) # most are tuples, but some are just an integer\n",
    "# lcp_array # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Use LCP array to calculate patterns (efficiently)\n",
    "# Values in LCP array represent lengths of matching ngrams\n",
    "#   e.g., the three values of 2 are \"and the\", \"the black\", \"the red\"\n",
    "#   \"the\" is harder: unigram appears four times, plus \"the black\" and \"the red\"\n",
    "#\n",
    "# Of interest:\n",
    "#   1. 0 means that whatever follows will have nothing in common with it\n",
    "#   2. Repetition of same number (doesn't happen here) means same pattern\n",
    "#   3. Consecutive non-zero values identify how much of the pattern they have in common, \n",
    "#      e.g., end goes from \"the black\" (2) to \"the\" (1) to \"the red\" (2)\n",
    "#         Counts are always +1, so there must be two instances of \"the red\", two of \"the black\", \n",
    "#      and four of \"the\" (two unigrams and two embedded in \"the red\" and \"the black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create Block dataclass\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not in token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    # witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (2, 2), (3, 0), (4, 1), (5, 0), (6, 1), (7, 0), (8, 1), (9, 0), (10, 2), (11, 1), (12, 2)]\n",
      "Block(token_count=2, start_position=1, end_position=2, all_start_positions=[], frequency=2, how_created=1)\n",
      "['and', 'the']\n",
      "Block(token_count=1, start_position=3, end_position=4, all_start_positions=[], frequency=2, how_created=1)\n",
      "['black']\n",
      "Block(token_count=1, start_position=5, end_position=6, all_start_positions=[], frequency=2, how_created=1)\n",
      "['cat']\n",
      "Block(token_count=1, start_position=7, end_position=8, all_start_positions=[], frequency=2, how_created=1)\n",
      "['red']\n",
      "Block(token_count=2, start_position=9, end_position=10, all_start_positions=[], frequency=2, how_created=1)\n",
      "['the', 'black']\n",
      "Block(token_count=2, start_position=11, end_position=12, all_start_positions=[], frequency=2, how_created=1)\n",
      "['the', 'red']\n",
      "Block(token_count=1, start_position=9, end_position=12, all_start_positions=[], frequency=4, how_created=2)\n",
      "['the']\n"
     ]
    }
   ],
   "source": [
    "# create blocks from the lcp array\n",
    "from collections import deque # faster append and pop than list\n",
    "blocks = []\n",
    "open_block_stack = deque()\n",
    "print(list(enumerate(lcp_array)))\n",
    "for offset, lcp in enumerate(lcp_array):\n",
    "    # three situations: next one is same value, higher that last, or lower than last\n",
    "    # if same value: same pattern\n",
    "    # if higher or lower, new pattern (may overlap with previous, unless one or the other value is 0)\n",
    "    if offset == 0: # skip the first one, which is a transition from a fake start value\n",
    "        continue # resume loop with next item in lcp array\n",
    "    elif lcp == lcp_array[offset - 1]:\n",
    "        pass # same pattern (happens with repetition), so do nothing\n",
    "    elif lcp > lcp_array[offset - 1]: # new prefix is longer than previous one, so start new pattern\n",
    "        # can fill in end_position and frequency only when we encounter a shorter value in the LCP array\n",
    "        # start_position is number of patterns that are the same \n",
    "        open_block_stack.append(Block(token_count = lcp, start_position = offset - 1, end_position = -1, all_start_positions = [], frequency = -1, how_created = 1))\n",
    "    else: # new prefix is shorter than previous one, so:\n",
    "            # 1. close open blocks with higher values\n",
    "            # 2. do something else\n",
    "        while open_block_stack and open_block_stack[-1].token_count > lcp: # if an open block is longer than the current length, pop and close it\n",
    "            block_being_modified = open_block_stack.pop()\n",
    "            block_being_modified.end_position = offset - 1\n",
    "            block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "            blocks.append(block_being_modified)\n",
    "        if lcp == 0: # if 0, stop after clearing the stack\n",
    "            continue\n",
    "        # not 0, and: either 1) stack is empty, or 2) top has an lcp value equal to current, or 3) an lcp value less than current\n",
    "        if not open_block_stack: # stack is empty, so hidden shorter block; create new block that starts at start position of last closed block\n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = blocks[-1].start_position, end_position = -1, all_start_positions = [], frequency = -1, how_created = 2))\n",
    "        elif open_block_stack[-1].token_count == lcp: # stack has value same length as current, so do nothing\n",
    "            pass\n",
    "        else: # stack has value less than current, so extends shorter block; create new block, but where?\n",
    "            # TODO: why does this work?\n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = blocks[-1].start_position, end_position = -1, all_start_positions = [], frequency = -1, how_created = 3))\n",
    "            # print(open_block_stack)\n",
    "            # print(blocks[-1])\n",
    "        # if equal to current length, do nothing; it's open, but we can't close it yet\n",
    "while open_block_stack: # pop anything left in open_block_stack\n",
    "    block_being_modified = open_block_stack.pop()\n",
    "    block_being_modified.end_position = len(lcp_array) - 1\n",
    "    block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "    blocks.append(block_being_modified)\n",
    "\n",
    "for block in blocks: # diagnostic\n",
    "    # suffix_array is offsets of start positions of suffixes (in alphabetical order) into token_array\n",
    "    # block.start_position is offset of suffix into suffix_array, which is one less than offset into lcp_array\n",
    "    # we slice token_array:\n",
    "    #   start of slice is offset into token_array (by way of suffix_array by way of lcp_array) of first ngram token\n",
    "    #   length of slice adds the length of the lcp interval (part of lcp array that represents pattern, which = length of ngram) to the start of the slice\n",
    "    print(block)\n",
    "    print(token_array[suffix_array[block.start_position]:suffix_array[block.start_position] + block.token_count])\n",
    "if open_block_stack: # diagnostic; should be empty\n",
    "    print('ERROR: open_block_stack should be empty')\n",
    "    print(open_block_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Taking stock\n",
    "\n",
    "* Previously we had a list of ngrams, with locations in witnesses. This **new implementation gives us the ngrams in the patterns**.\n",
    "* Previously repeated tokens were separate ngrams, while here they are combined in a block. The **blocks in the new version, then, don't record the locations of the ngrams in the witnesses**, and therefore also do not record repetition.\n",
    "* Before we can produce an alignment, then **we need to get from the blocks to the locations in the witnesses**.\n",
    "\n",
    "Each pattern is part of the suffix array, and the entries in the suffix array know which tokens in the witnesses correspond to the pattern. The blocks record start and end positions in the suffix array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 9, 8, 4, 12, 5, 1, 11, 7, 3, 0, 10]"
      ]
     },
     "execution_count": 13,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Block 1 starts at suffix_array position 1 and ends at 2. This means that tokens 2 (position 1) and 9 (position 2) are the (only) two start positions of the two instances of the bigram 'and the' in the token_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'the'),\n",
       " (1, 'red'),\n",
       " (2, 'and'),\n",
       " (3, 'the'),\n",
       " (4, 'black'),\n",
       " (5, 'cat'),\n",
       " (6, ' # '),\n",
       " (7, 'the'),\n",
       " (8, 'black'),\n",
       " (9, 'and'),\n",
       " (10, 'the'),\n",
       " (11, 'red'),\n",
       " (12, 'cat')]"
      ]
     },
     "execution_count": 14,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we enumerate the token array and look at tokens 2 and 9, we see the two instances of the bigram 'and the'.\n",
    "\n",
    "[(index, value) for index,value in enumerate(token_array)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we iterate over the blocks, for each block we can get its start position(s) in the token array and the ngrams themselves (not in token-array order, but we can sort later). We can then sort them by token position to get a list of ngrams for both witnesses in token order (which we need because we do our alignment from left to right). The token array doesn't distinguish the witnesses explicitly, but if we subtract the length of w1 from a position, add 1, and get a positive number, we're in w2.\n",
    "\n",
    "We call each occurrence of a pattern at a location in a witness an **instance**. We need to build a data structure that stores instances, which will store ngram, offset, and witness identifier (for convenience; we compute it from the offset into the token_array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from typing import List\n",
    "@dataclass\n",
    "class Instance:\n",
    "    instance_offset: int\n",
    "    block_id: int # offset into list of blocks\n",
    "    start_token: int # start position in token array\n",
    "    ngram_length: int # length of ngram\n",
    "    witness: int # 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "instances = []\n",
    "for index, block in enumerate(blocks): # compute all start tokens just once for each block\n",
    "    _all_start_tokens = suffix_array[block.start_position: block.end_position + 1]\n",
    "    _all_start_tokens.sort()\n",
    "    block.all_start_positions = _all_start_tokens # save it to the block\n",
    "    for suffix_array_offset in range(block.start_position, block.end_position+1): # each will be a unique instance\n",
    "        _instance_offset = -1\n",
    "        _block_id = index\n",
    "        _start_token = suffix_array[suffix_array_offset]\n",
    "        _ngram_length = block.token_count\n",
    "        _witness = 1 if _start_token > len(witnesses[0]) else 0\n",
    "        instances.append(Instance(instance_offset = _instance_offset, block_id = _block_id, start_token = _start_token, ngram_length = _ngram_length, witness = _witness))\n",
    "instances.sort(key = lambda x: x.start_token)\n",
    "instance_by_instance_offset = {}\n",
    "for index, instance in enumerate(instances):\n",
    "    instance.instance_offset = index\n",
    "    instance_by_instance_offset[index] = instance\n",
    "pointer_w0 = 0\n",
    "pointer_w1 = len(list(filter(lambda x: x.witness == 0, instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Block(token_count=2, start_position=1, end_position=2, all_start_positions=[2, 9], frequency=2, how_created=1),\n",
       " Block(token_count=1, start_position=3, end_position=4, all_start_positions=[4, 8], frequency=2, how_created=1),\n",
       " Block(token_count=1, start_position=5, end_position=6, all_start_positions=[5, 12], frequency=2, how_created=1),\n",
       " Block(token_count=1, start_position=7, end_position=8, all_start_positions=[1, 11], frequency=2, how_created=1),\n",
       " Block(token_count=2, start_position=9, end_position=10, all_start_positions=[3, 7], frequency=2, how_created=1),\n",
       " Block(token_count=2, start_position=11, end_position=12, all_start_positions=[0, 10], frequency=2, how_created=1),\n",
       " Block(token_count=1, start_position=9, end_position=12, all_start_positions=[0, 3, 7, 10], frequency=4, how_created=2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Instance(instance_offset=0, block_id=5, start_token=0, ngram_length=2, witness=0),\n",
       " Instance(instance_offset=1, block_id=6, start_token=0, ngram_length=1, witness=0),\n",
       " Instance(instance_offset=2, block_id=3, start_token=1, ngram_length=1, witness=0),\n",
       " Instance(instance_offset=3, block_id=0, start_token=2, ngram_length=2, witness=0),\n",
       " Instance(instance_offset=4, block_id=4, start_token=3, ngram_length=2, witness=0),\n",
       " Instance(instance_offset=5, block_id=6, start_token=3, ngram_length=1, witness=0),\n",
       " Instance(instance_offset=6, block_id=1, start_token=4, ngram_length=1, witness=0),\n",
       " Instance(instance_offset=7, block_id=2, start_token=5, ngram_length=1, witness=0),\n",
       " Instance(instance_offset=8, block_id=4, start_token=7, ngram_length=2, witness=1),\n",
       " Instance(instance_offset=9, block_id=6, start_token=7, ngram_length=1, witness=1),\n",
       " Instance(instance_offset=10, block_id=1, start_token=8, ngram_length=1, witness=1),\n",
       " Instance(instance_offset=11, block_id=0, start_token=9, ngram_length=2, witness=1),\n",
       " Instance(instance_offset=12, block_id=5, start_token=10, ngram_length=2, witness=1),\n",
       " Instance(instance_offset=13, block_id=6, start_token=10, ngram_length=1, witness=1),\n",
       " Instance(instance_offset=14, block_id=3, start_token=11, ngram_length=1, witness=1),\n",
       " Instance(instance_offset=15, block_id=2, start_token=12, ngram_length=1, witness=1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Instance(instance_offset=0, block_id=5, start_token=0, ngram_length=2, witness=0),\n",
       " 1: Instance(instance_offset=1, block_id=6, start_token=0, ngram_length=1, witness=0),\n",
       " 2: Instance(instance_offset=2, block_id=3, start_token=1, ngram_length=1, witness=0),\n",
       " 3: Instance(instance_offset=3, block_id=0, start_token=2, ngram_length=2, witness=0),\n",
       " 4: Instance(instance_offset=4, block_id=4, start_token=3, ngram_length=2, witness=0),\n",
       " 5: Instance(instance_offset=5, block_id=6, start_token=3, ngram_length=1, witness=0),\n",
       " 6: Instance(instance_offset=6, block_id=1, start_token=4, ngram_length=1, witness=0),\n",
       " 7: Instance(instance_offset=7, block_id=2, start_token=5, ngram_length=1, witness=0),\n",
       " 8: Instance(instance_offset=8, block_id=4, start_token=7, ngram_length=2, witness=1),\n",
       " 9: Instance(instance_offset=9, block_id=6, start_token=7, ngram_length=1, witness=1),\n",
       " 10: Instance(instance_offset=10, block_id=1, start_token=8, ngram_length=1, witness=1),\n",
       " 11: Instance(instance_offset=11, block_id=0, start_token=9, ngram_length=2, witness=1),\n",
       " 12: Instance(instance_offset=12, block_id=5, start_token=10, ngram_length=2, witness=1),\n",
       " 13: Instance(instance_offset=13, block_id=6, start_token=10, ngram_length=1, witness=1),\n",
       " 14: Instance(instance_offset=14, block_id=3, start_token=11, ngram_length=1, witness=1),\n",
       " 15: Instance(instance_offset=15, block_id=2, start_token=12, ngram_length=1, witness=1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_by_instance_offset # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 16\n"
     ]
    }
   ],
   "source": [
    "w0_last_instance_offset = len(list(filter(lambda x: x.witness == 0, instances)))\n",
    "w1_last_instance_offset = len(instances)\n",
    "print(w0_last_instance_offset, w1_last_instance_offset) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {5: [0, 12],\n",
       "             6: [1, 5, 9, 13],\n",
       "             3: [2, 14],\n",
       "             0: [3, 11],\n",
       "             4: [4, 8],\n",
       "             1: [6, 10],\n",
       "             2: [7, 15]})"
      ]
     },
     "execution_count": 21,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create mapping from blocks to instances\n",
    "from collections import defaultdict\n",
    "instances_by_block = defaultdict(list)\n",
    "for index, instance in enumerate(instances):\n",
    "    instances_by_block[instance.block_id].append(index)\n",
    "instances_by_block # take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create decision graph\n",
    "\n",
    "Three options are first in w0, first in w1, and first in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create dataclasses for decision graph and its nodes and edges\n",
    "\n",
    "from typing import Set\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Decision_Graph_Node:\n",
    "    id: int\n",
    "    witness_0_instance_offset: int # we can get the start token from the instance\n",
    "    witness_1_instance_offset: int\n",
    "    ngram_length: int # same in both witness because same ngram ... doh!\n",
    "\n",
    "@dataclass\n",
    "class Decision_Graph_Edge:\n",
    "    source: int # node id\n",
    "    target: int # node id\n",
    "\n",
    "@dataclass\n",
    "class Decision_Graph:\n",
    "    nodes: Set[Decision_Graph_Node]\n",
    "    edges: Set[Decision_Graph_Edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 16\n"
     ]
    }
   ],
   "source": [
    "# we need to know when we've seen all of the instances for a witness\n",
    "w0_instance_end = len(list(filter(lambda x: x.witness == 0,instances))) # offset of last w0 instance in instances list\n",
    "w1_instance_end = len(instances) # offset of last w1 instance in instances list\n",
    "print(w0_instance_end, w1_instance_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create root node in decision graph\n",
    "# the instance offsets are one before the first real instance, so \n",
    "#   -1 to represent the position before w0\n",
    "#   the last instance offset for w0 to represent the position before w1\n",
    "# the ngram length for the root, 1, is a fake to make the stupid arithmetic work\n",
    "decision_graph = Decision_Graph(nodes = set(), edges = set())\n",
    "root_node = Decision_Graph_Node(id = 0, witness_0_instance_offset = -1, witness_1_instance_offset = w0_instance_end, ngram_length = 1)\n",
    "decision_graph.nodes.add(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Decision_Graph_Node(id=0, witness_0_instance_offset=-1, witness_1_instance_offset=8, ngram_length=1)}"
      ]
     },
     "execution_count": 25,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_graph.nodes # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# generator function to loop over w1 instances looking for match\n",
    "def next_matching_w1_instance(w1_instance_offset):\n",
    "    while w1_instance_offset < w1_last_instance_offset:\n",
    "        yield w1_instance_offset\n",
    "        w1_instance_offset -= 1\n",
    "\n",
    "# create other nodes in decision graph\n",
    "def add_children(parent: Decision_Graph_Node): # modifies global in place, does not return\n",
    "    # create three nodes: next in w0, next in w1, next in both, but ...\n",
    "    #   ... node may already exist\n",
    "    # reset pointers to after current instance (i.e., don't always increment by 1)\n",
    "    # recur if there are still instances to consider\n",
    "    #\n",
    "    ######\n",
    "    # add next in w0 and recur if necessary\n",
    "    ######\n",
    "    #\n",
    "    # find first next instance in w0 that has a matching instance for w1 that is after the parent instance in w1 offset\n",
    "    #\n",
    "    # for each position to the right in w0\n",
    "    #   (inner loop)\n",
    "    #   find next matching instance in w1 until end of w1 instances\n",
    "    #   if w1 instance is to the right of w1 position of parent, create a node and break out of inner loop, returning break signal\n",
    "    #   otherwise loop\n",
    "    #   (end of inner loop)\n",
    "    #   if break signal, break out of outer loop\n",
    "    #   otherwise loop to check next position in w0\n",
    "\n",
    "    for w0_offset in range(parent.witness_0_instance_offset, w0_last_instance_offset):\n",
    "        print(w0_offset)\n",
    "        w1_instances = next_matching_w1_instance(parent.witness_1_instance_offset)\n",
    "        print(w1_instances.__next__())\n",
    "\n",
    "\n",
    "#     next_instance_for_w0 = instances[parent.witness_0_instance_offset + 1:w0_instance_end][0]\n",
    "#     matching_instance_in_w1 = instances[next(filter(lambda x: instances[x].witness == 1, instances_by_block[next_instance_for_w0.block_id]))]\n",
    "#     if matching_instance_in_w1.instance_offset > parent.witness_1_instance_offset:\n",
    "#         _new_node_id = len(decision_graph.nodes)\n",
    "#         _new_node = Decision_Graph_Node(id = _new_node_id, witness_0_instance_offset = next_instance_for_w0.instance_offset, witness_1_instance_offset = matching_instance_in_w1.instance_offset, ngram_length = next_instance_for_w0.ngram_length)\n",
    "#         decision_graph.nodes.add(_new_node)\n",
    "#         # diagnostic\n",
    "#         _current_ngram = witnesses[0][next_instance_for_w0.start_token:next_instance_for_w0.start_token + next_instance_for_w0.ngram_length]\n",
    "#         print('w0: ', next_instance_for_w0)\n",
    "#         print('w1: ', matching_instance_in_w1)\n",
    "#         print(_new_node, _current_ngram)\n",
    "#         # recur if there's more to do\n",
    "#         if next_instance_for_w0.instance_offset < w0_last_instance_offset and matching_instance_in_w1.instance_offset < w1_last_instance_offset:\n",
    "#             add_children(_new_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "8\n",
      "0\n",
      "8\n",
      "1\n",
      "8\n",
      "2\n",
      "8\n",
      "3\n",
      "8\n",
      "4\n",
      "8\n",
      "5\n",
      "8\n",
      "6\n",
      "8\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "add_children(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Decision_Graph_Node(id=0, witness_0_instance_offset=-1, witness_1_instance_offset=8, ngram_length=1)}"
      ]
     },
     "execution_count": 28,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}