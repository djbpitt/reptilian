{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Python wrapper for Java VPMF implementation\n",
    "\n",
    "## Built-in I/O assumptions\n",
    "\n",
    "* Input must be a file from the filesystem with integers (not word tokens)\n",
    "* Output must be a file written to disk with integers\n",
    "\n",
    "## Method\n",
    "\n",
    "1. Read in input file that contains string data\n",
    "1. Create dictionary with mapping from unique strings to unique integers\n",
    "1. Use dictionary to convert token strings to integers and store as *input.txt*\n",
    "1. Run VSMP, which creates *output.txt* with integer data\n",
    "1. Read in *output.txt* and convert integers back to work tokens\n",
    "\n",
    "## Data samples\n",
    "\n",
    "1. cat1.txt, cat2.txt, cat3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# NO INTERNAL REFERENCE\n",
    "# from https://github.com/fandu/maximal-sequential-patterns-mining\n",
    "import subprocess\n",
    "\n",
    "# TODO: Rename class\n",
    "class Vmsp:\n",
    "    def __init__(self):\n",
    "        self._executable = \"spmf.jar\"\n",
    "        self._input = \"input.txt\"\n",
    "        self._output = \"output.txt\"\n",
    "\n",
    "    def run(self, min_supp=0.5): # originally min_supp=0.5; change to 1 means must be in all witnesses\n",
    "        # java -jar spmf.jar run PrefixSpan contextPrefixSpan.txt output.txt 50%\n",
    "              #  subprocess.call([\"java\", \"-jar\", self._executable, \"run\", \"TSP_nonClosed\", self._input, self._output, \"5\"])\n",
    "\n",
    "\n",
    "\n",
    "#                subprocess.call([\"java\", \"-jar\", self._executable, \"run\", \"TKS\", self._input, self._output, \"1\"])\n",
    "       subprocess.call([\"java\", \"-jar\", self._executable, \"run\", \"BIDE+\", self._input, self._output, \"1\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def encode_input(self, data):\n",
    "        pass\n",
    "\n",
    "    def decode_output(self):\n",
    "        # read\n",
    "        lines = []\n",
    "        try:\n",
    "            with open(self._output, \"r\") as f: # modified to remove deprecated U mode\n",
    "                lines = f.readlines()\n",
    "        except:\n",
    "            print(\"read_output error\") # modified to add parentheses\n",
    "\n",
    "        # decode\n",
    "        patterns = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            patterns.append(line.split(\" -1 \"))\n",
    "\n",
    "        return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[['Causes', 'of', 'Variability.', 'WHEN', 'we', 'look', 'to', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub-variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals,', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us,', 'is,', 'that', 'they', 'generally', 'differ', 'more', 'from', 'each', 'other', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature.', 'When', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated,', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment,', 'I', 'think', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'great', 'variability', 'is', 'simply', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as,', 'and', 'somewhat', 'different', 'from,', 'those', 'to', 'which', 'the', 'parent-species', 'have', 'been', 'exposed', 'under', 'nature.', 'There', 'is', 'also,', 'I', 'think,', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight,', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food.', 'It', 'seems', 'pretty', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'the', 'new', 'conditions', 'of', 'life', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation;', 'and', 'that,', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary,', 'it', 'generally', 'continues', 'to', 'vary', 'for', 'many', 'generations.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'being', 'ceasing', 'to', 'be', 'variable', 'under', 'cultivation.', 'Our', 'oldest', 'cultivated', 'plants,', 'such', 'as', 'wheat,', 'still', 'often', 'yield', 'new', 'varieties:', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification.'], ['WHEN', 'we', 'look', 'to', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub-variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals,', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us,', 'is,', 'that', 'they', 'generally', 'differ', 'more', 'from', 'each', 'other', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature.', 'When', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated,', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment,', 'I', 'think', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'great', 'variability', 'is', 'simply', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as,', 'and', 'somewhat', 'different', 'from,', 'those', 'to', 'which', 'the', 'parent-species', 'have', 'been', 'exposed', 'under', 'nature.', 'There', 'is', 'also,', 'I', 'think,', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight,', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food.', 'It', 'seems', 'pretty', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'the', 'new', 'conditions', 'of', 'life', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation;', 'and', 'that', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary,', 'it', 'generally', 'continues', 'to', 'vary', 'for', 'many', 'generations.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'being', 'ceasing', 'to', 'be', 'variable', 'under', 'cultivation.', 'Our', 'oldest', 'cultivated', 'plants,', 'such', 'as', 'wheat,', 'still', 'often', 'yield', 'new', 'varieties:', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification.']]\n"
     ]
    }
   ],
   "source": [
    "# Create list of lists with word tokens\n",
    "text_data = []\n",
    "\n",
    "# for i in range(1,4): # Small example with cats\n",
    "#     with open('cat' + str(i) + '.txt', 'r') as f:\n",
    "#         text_data.append([token for token in f.read().split()])\n",
    "#\n",
    "# Find all six Darwin witnesses; filename is darwin18\\d\\d.txt\n",
    "# Each paragraph is one line\n",
    "#\n",
    "import glob\n",
    "filenames = glob.glob('darwin18??.txt')\n",
    "for filename in filenames[:2]: # each of six files\n",
    "    file_tokens = [] # all tokens for single file\n",
    "    with open(filename, 'r') as f:\n",
    "        for paragraph in range(2): # read two paragraphs\n",
    "            file_tokens.extend([token for token in f.readline().rstrip().split()])\n",
    "        text_data.append(file_tokens)\n",
    "print(len(text_data))\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '1', '2', '3', '4', '5', '6', '7', '8', '1', '7', '9', '10', '11', '12', '1', '13', '14', '15', '16', '17', '18', '19', '1', '7', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '7', '8', '1', '36', '19', '37', '11', '10', '38', '39', '40', '1', '41', '42', '4', '43', '44', '7', '45', '46', '1', '7', '16', '17', '47', '22', '48', '49', '50', '17', '22', '48', '51', '52', '53', '54', '55', '7', '56', '57', '58', '17', '59', '60', '61', '4', '62', '63', '6', '64', '26', '65', '66', '67', '68', '69', '70', '6', '13', '71', '72', '73', '49', '74', '55', '75', '1', '76', '77', '78', '79', '80', '17', '81', '57', '82', '83', '6', '22', '7', '84', '48', '49', '85', '55', '41', '86', '68', '87', '60', '88', '89', '90', '38', '7', '91', '92', '93', '94', '95', '26', '65', '67', '96', '97', '98', '99', '100', '101', '1', '102', '103', '104', '105', '106', '26', '107', '108', '109', '97', '85', '52', '110', '111', '6', '7', '112', '75', '1', '76', '6', '113', '36', '114', '115', '1', '116', '17', '117', '118', '7', '119', '120', '121', '122', '6', '123', '124', '28', '125', '6', '126', '127', '128', '129', '130', '131', '68', '44', '132', '1', '39', '133', '134', '135', '6', '97', '133', '55', '136', '137', '138', '15', '139', '140', '141', '142', '143', '144', '145', '112', '146', '13', '138', '147', '47', '62', '143', '148', '1', '149', '150', '11', '151'], ['3', '4', '5', '6', '7', '8', '1', '7', '9', '10', '11', '12', '1', '13', '14', '15', '16', '17', '18', '19', '1', '7', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '7', '8', '1', '36', '19', '37', '11', '10', '38', '39', '40', '1', '41', '42', '4', '43', '44', '7', '45', '46', '1', '7', '16', '17', '47', '22', '48', '49', '50', '17', '22', '48', '51', '52', '53', '54', '55', '7', '56', '57', '58', '17', '59', '60', '61', '4', '62', '63', '6', '64', '26', '65', '66', '67', '68', '69', '70', '6', '13', '71', '72', '73', '49', '74', '55', '75', '1', '76', '77', '78', '79', '80', '17', '81', '57', '82', '83', '6', '22', '7', '84', '48', '49', '85', '55', '41', '86', '68', '87', '60', '88', '89', '90', '38', '7', '91', '92', '93', '94', '95', '26', '65', '67', '96', '97', '98', '99', '100', '101', '1', '102', '103', '104', '105', '106', '26', '107', '108', '109', '97', '85', '52', '110', '111', '6', '7', '112', '75', '1', '76', '6', '113', '36', '114', '115', '1', '116', '17', '26', '118', '7', '119', '120', '121', '122', '6', '123', '124', '28', '125', '6', '126', '127', '128', '129', '130', '131', '68', '44', '132', '1', '39', '133', '134', '135', '6', '97', '133', '55', '136', '137', '138', '15', '139', '140', '141', '142', '143', '144', '145', '112', '146', '13', '138', '147', '47', '62', '143', '148', '1', '149', '150', '11', '151']]\n",
      "{'Causes': 0, 'of': 1, 'Variability.': 2, 'WHEN': 3, 'we': 4, 'look': 5, 'to': 6, 'the': 7, 'individuals': 8, 'same': 9, 'variety': 10, 'or': 11, 'sub-variety': 12, 'our': 13, 'older': 14, 'cultivated': 15, 'plants': 16, 'and': 17, 'animals,': 18, 'one': 19, 'first': 20, 'points': 21, 'which': 22, 'strikes': 23, 'us,': 24, 'is,': 25, 'that': 26, 'they': 27, 'generally': 28, 'differ': 29, 'more': 30, 'from': 31, 'each': 32, 'other': 33, 'than': 34, 'do': 35, 'any': 36, 'species': 37, 'in': 38, 'a': 39, 'state': 40, 'nature.': 41, 'When': 42, 'reflect': 43, 'on': 44, 'vast': 45, 'diversity': 46, 'animals': 47, 'have': 48, 'been': 49, 'cultivated,': 50, 'varied': 51, 'during': 52, 'all': 53, 'ages': 54, 'under': 55, 'most': 56, 'different': 57, 'climates': 58, 'treatment,': 59, 'I': 60, 'think': 61, 'are': 62, 'driven': 63, 'conclude': 64, 'this': 65, 'great': 66, 'variability': 67, 'is': 68, 'simply': 69, 'due': 70, 'domestic': 71, 'productions': 72, 'having': 73, 'raised': 74, 'conditions': 75, 'life': 76, 'not': 77, 'so': 78, 'uniform': 79, 'as,': 80, 'somewhat': 81, 'from,': 82, 'those': 83, 'parent-species': 84, 'exposed': 85, 'There': 86, 'also,': 87, 'think,': 88, 'some': 89, 'probability': 90, 'view': 91, 'propounded': 92, 'by': 93, 'Andrew': 94, 'Knight,': 95, 'may': 96, 'be': 97, 'partly': 98, 'connected': 99, 'with': 100, 'excess': 101, 'food.': 102, 'It': 103, 'seems': 104, 'pretty': 105, 'clear': 106, 'organic': 107, 'beings': 108, 'must': 109, 'several': 110, 'generations': 111, 'new': 112, 'cause': 113, 'appreciable': 114, 'amount': 115, 'variation;': 116, 'that,': 117, 'when': 118, 'organisation': 119, 'has': 120, 'once': 121, 'begun': 122, 'vary,': 123, 'it': 124, 'continues': 125, 'vary': 126, 'for': 127, 'many': 128, 'generations.': 129, 'No': 130, 'case': 131, 'record': 132, 'variable': 133, 'being': 134, 'ceasing': 135, 'cultivation.': 136, 'Our': 137, 'oldest': 138, 'plants,': 139, 'such': 140, 'as': 141, 'wheat,': 142, 'still': 143, 'often': 144, 'yield': 145, 'varieties:': 146, 'domesticated': 147, 'capable': 148, 'rapid': 149, 'improvement': 150, 'modification.': 151}\n"
     ]
    }
   ],
   "source": [
    "# Create integer data\n",
    "token_to_integer = {}\n",
    "integer_data = []\n",
    "for witness_data in text_data:\n",
    "    witness_integers = []\n",
    "    for token in witness_data:\n",
    "        if token not in token_to_integer:\n",
    "            token_to_integer[token] = len(token_to_integer) # add value to dictionary, use len() for unique value\n",
    "        witness_integers.append(str(token_to_integer[token]))\n",
    "    integer_data.append(witness_integers)\n",
    "print(integer_data)\n",
    "print(token_to_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Write integer data to disk as 'input.txt'\n",
    "# Each witness is a line\n",
    "with open('input.txt', 'w') as f:\n",
    "    for witness in integer_data:\n",
    "        f.write(\" -1 \".join(witness))\n",
    "        f.write(' -1 -2\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 -1 1 -1 2 -1 3 -1 4 -1 5 -1 6 -1 7 -1 8 -1 1 -1 7 -1 9 -1 10 -1 11 -1 12 -1 1 -1 13 -1 14 -1 15 -1 16 -1 17 -1 18 -1 19 -1 1 -1 7 -1 20 -1 21 -1 22 -1 23 -1 24 -1 25 -1 26 -1 27 -1 28 -1 29 -1 30 -1 31 -1 32 -1 33 -1 34 -1 35 -1 7 -1 8 -1 1 -1 36 -1 19 -1 37 -1 11 -1 10 -1 38 -1 39 -1 40 -1 1 -1 41 -1 42 -1 4 -1 43 -1 44 -1 7 -1 45 -1 46 -1 1 -1 7 -1 16 -1 17 -1 47 -1 22 -1 48 -1 49 -1 50 -1 17 -1 22 -1 48 -1 51 -1 52 -1 53 -1 54 -1 55 -1 7 -1 56 -1 57 -1 58 -1 17 -1 59 -1 60 -1 61 -1 4 -1 62 -1 63 -1 6 -1 64 -1 26 -1 65 -1 66 -1 67 -1 68 -1 69 -1 70 -1 6 -1 13 -1 71 -1 72 -1 73 -1 49 -1 74 -1 55 -1 75 -1 1 -1 76 -1 77 -1 78 -1 79 -1 80 -1 17 -1 81 -1 57 -1 82 -1 83 -1 6 -1 22 -1 7 -1 84 -1 48 -1 49 -1 85 -1 55 -1 41 -1 86 -1 68 -1 87 -1 60 -1 88 -1 89 -1 90 -1 38 -1 7 -1 91 -1 92 -1 93 -1 94 -1 95 -1 26 -1 65 -1 67 -1 96 -1 97 -1 98 -1 99 -1 100 -1 101 -1 1 -1 102 -1 103 -1 104 -1 105 -1 106 -1 26 -1 107 -1 108 -1 109 -1 97 -1 85 -1 52 -1 110 -1 111 -1 6 -1 7 -1 112 -1 75 -1 1 -1 76 -1 6 -1 113 -1 36 -1 114 -1 115 -1 1 -1 116 -1 17 -1 117 -1 118 -1 7 -1 119 -1 120 -1 121 -1 122 -1 6 -1 123 -1 124 -1 28 -1 125 -1 6 -1 126 -1 127 -1 128 -1 129 -1 130 -1 131 -1 68 -1 44 -1 132 -1 1 -1 39 -1 133 -1 134 -1 135 -1 6 -1 97 -1 133 -1 55 -1 136 -1 137 -1 138 -1 15 -1 139 -1 140 -1 141 -1 142 -1 143 -1 144 -1 145 -1 112 -1 146 -1 13 -1 138 -1 147 -1 47 -1 62 -1 143 -1 148 -1 1 -1 149 -1 150 -1 11 -1 151 -1 -2\\n', '3 -1 4 -1 5 -1 6 -1 7 -1 8 -1 1 -1 7 -1 9 -1 10 -1 11 -1 12 -1 1 -1 13 -1 14 -1 15 -1 16 -1 17 -1 18 -1 19 -1 1 -1 7 -1 20 -1 21 -1 22 -1 23 -1 24 -1 25 -1 26 -1 27 -1 28 -1 29 -1 30 -1 31 -1 32 -1 33 -1 34 -1 35 -1 7 -1 8 -1 1 -1 36 -1 19 -1 37 -1 11 -1 10 -1 38 -1 39 -1 40 -1 1 -1 41 -1 42 -1 4 -1 43 -1 44 -1 7 -1 45 -1 46 -1 1 -1 7 -1 16 -1 17 -1 47 -1 22 -1 48 -1 49 -1 50 -1 17 -1 22 -1 48 -1 51 -1 52 -1 53 -1 54 -1 55 -1 7 -1 56 -1 57 -1 58 -1 17 -1 59 -1 60 -1 61 -1 4 -1 62 -1 63 -1 6 -1 64 -1 26 -1 65 -1 66 -1 67 -1 68 -1 69 -1 70 -1 6 -1 13 -1 71 -1 72 -1 73 -1 49 -1 74 -1 55 -1 75 -1 1 -1 76 -1 77 -1 78 -1 79 -1 80 -1 17 -1 81 -1 57 -1 82 -1 83 -1 6 -1 22 -1 7 -1 84 -1 48 -1 49 -1 85 -1 55 -1 41 -1 86 -1 68 -1 87 -1 60 -1 88 -1 89 -1 90 -1 38 -1 7 -1 91 -1 92 -1 93 -1 94 -1 95 -1 26 -1 65 -1 67 -1 96 -1 97 -1 98 -1 99 -1 100 -1 101 -1 1 -1 102 -1 103 -1 104 -1 105 -1 106 -1 26 -1 107 -1 108 -1 109 -1 97 -1 85 -1 52 -1 110 -1 111 -1 6 -1 7 -1 112 -1 75 -1 1 -1 76 -1 6 -1 113 -1 36 -1 114 -1 115 -1 1 -1 116 -1 17 -1 26 -1 118 -1 7 -1 119 -1 120 -1 121 -1 122 -1 6 -1 123 -1 124 -1 28 -1 125 -1 6 -1 126 -1 127 -1 128 -1 129 -1 130 -1 131 -1 68 -1 44 -1 132 -1 1 -1 39 -1 133 -1 134 -1 135 -1 6 -1 97 -1 133 -1 55 -1 136 -1 137 -1 138 -1 15 -1 139 -1 140 -1 141 -1 142 -1 143 -1 144 -1 145 -1 112 -1 146 -1 13 -1 138 -1 147 -1 47 -1 62 -1 143 -1 148 -1 1 -1 149 -1 150 -1 11 -1 151 -1 -2\\n']\n"
     ]
    }
   ],
   "source": [
    "# Check new integer file\n",
    "with open('input.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: _JAVA_OPTIONS=-Djava.io.tmpdir=/home/user/tmp -Xms700m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/home/user/tmp -Xms700m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">/home/user/our_experiment/spmf.jar\n"
     ]
    }
   ],
   "source": [
    "%env _JAVA_OPTIONS=-Djava.io.tmpdir=/home/user/tmp -Xms700m\n",
    "\n",
    "\n",
    "# Do the work\n",
    "if __name__ == \"__main__\":\n",
    "    vmsp = Vmsp()\n",
    "    vmsp.encode_input([])\n",
    "    vmsp.run()\n",
    "    print(vmsp.decode_output()) # modified to add parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Convert integer output back to strings\n",
    "integer_to_token = {v:k for k, v in token_to_integer.items()} # invert dictionary to decode\n",
    "results = [] # hold results as list of lists\n",
    "with open('output.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        results.append([integer_to_token[int(token)] for token in line.split()[:-2] if int(token) != -1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}