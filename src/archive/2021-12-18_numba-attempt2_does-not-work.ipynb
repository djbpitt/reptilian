{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This time for sure\n",
    "\n",
    "Alignment is modeled as masked array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "from typing import Set, List\n",
    "from dataclasses import dataclass\n",
    "import networkx as nx\n",
    "import re\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import numpy.ma as ma\n",
    "\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "debug = True\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def np_apply_along_axis(func1d, axis, arr):\n",
    "  assert arr.ndim == 2\n",
    "  assert axis in [0, 1]\n",
    "  if axis == 0:\n",
    "    result = np.empty(arr.shape[1])\n",
    "    for i in range(len(result)):\n",
    "      result[i] = func1d(arr[:, i])\n",
    "  else:\n",
    "    result = np.empty(arr.shape[0])\n",
    "    for i in range(len(result)):\n",
    "      result[i] = func1d(arr[i, :])\n",
    "  return result\n",
    "\n",
    "@nb.njit\n",
    "def np_apply_along_axis_nonparallel(func1d, axis, arr):\n",
    "  assert arr.ndim == 2\n",
    "  assert axis in [0, 1]\n",
    "  if axis == 0:\n",
    "    result = np.empty(arr.shape[1])\n",
    "    for i in range(len(result)):\n",
    "      result[i] = func1d(arr[:, i])\n",
    "  else:\n",
    "    result = np.empty(arr.shape[0])\n",
    "    for i in range(len(result)):\n",
    "      result[i] = func1d(arr[i, :])\n",
    "  return result\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def np_mean(array, axis):\n",
    "  return np_apply_along_axis(np.mean, axis, array)\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def np_std(array, axis):\n",
    "  return np_apply_along_axis(np.std, axis, array)\n",
    "\n",
    "@nb.njit # errors out with parallel=True\n",
    "def np_prod(array, axis):\n",
    "  return np_apply_along_axis_nonparallel(np.prod, axis, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# RESUME HERE\n",
    "#\n",
    "# Issue: numba cannot operate on masked arrays\n",
    "#\n",
    "# Possible work-around:\n",
    "#   separate data and mask\n",
    "#   separate compiled numpy functions for data and mask\n",
    "#   reassemble into new masked array\n",
    "#\n",
    "# Documentation note:\n",
    "#\n",
    "# Matrix = two-dimensional\n",
    "# Vector = one-dimensional\n",
    "# We do not use the term \"array\"\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def ma_subtract(matrix, vector): #todo\n",
    "    # subtract vector from each row of matrix\n",
    "    # input is matrix of numbers, vector of numbers\n",
    "    # axis does not need to be specified\n",
    "    # returns matrix of numbers\n",
    "    return matrix - vector\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def ma_compute_either_masked(matrix, vector): # ready for testing\n",
    "    # return True (masked) if either value is masked\n",
    "    # input is matrix of booleans, vector of booleans\n",
    "    # axis does not need to be specified\n",
    "    # returns matrix of booleans\n",
    "    return matrix | vector\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def ma_compute_both_masked(matrix, vector): # ready for testing\n",
    "    # return True (masekd) only if both values are masked\n",
    "    # input is matrix of booleans, vector of booleans\n",
    "    # axis does not need to be specified\n",
    "    # returns matrix of booleans\n",
    "    return matrix * vector\n",
    "\n",
    "@nb.njit\n",
    "def ma_prod(matrix, axis): # ready for testing\n",
    "    # multiply across dimension\n",
    "    # input is matrix of numbers\n",
    "    # axis is 0 or 1\n",
    "    # return vector of numbers\n",
    "    return np_prod(matrix, 1)\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def ma_compare_to_zero(vector): # ready for testing\n",
    "    # input is vector of numbers (vector) and vector of booleans (mask)\n",
    "    # evaluate to True if product of non-masked values == 0, otherwise false\n",
    "    # return vector of booleans\n",
    "    return vector == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    -9    -98   -997  -9996 -99995]\n",
      " [    -9    -98   -997  -9996 -99995]\n",
      " [    -9    -98   -997  -9996 -99995]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    -9    -98   -997  -9996 -99995]\n",
      " [    -9    -98   -997  -9996 -99995]\n",
      " [    -9    -98   -997  -9996 -99995]]\n"
     ]
    }
   ],
   "source": [
    "# test of ma_subtract()\n",
    "#\n",
    "# Input is matrix[int], vector[int]\n",
    "# Return is matrix[int]\n",
    "#\n",
    "a = np.array([[1,2,3,4,5],\n",
    "              [1,2,3,4,5],\n",
    "              [1,2,3,4,5]\n",
    "             ])\n",
    "v = np.array([10, 100, 1000, 10000, 100000])\n",
    "print(a - v)\n",
    "print(ma_subtract(a, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, False],\n",
       "       [ True,  True,  True, False],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of ma_compute_either_masked()\n",
    "# return True if either is True\n",
    "#\n",
    "# Input is matrix[bool], vector[bool]\n",
    "# Return is matrix[bool]\n",
    "#\n",
    "a = np.array([[True, True, False, False],\n",
    "              [True, True, True, False],\n",
    "              [False, True, False, True]\n",
    "             ])\n",
    "b = np.array([True, False, True, False])\n",
    "ma_compute_either_masked(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False],\n",
       "       [ True, False,  True, False],\n",
       "       [False, False, False, False]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of ma_compute_both_masked()\n",
    "# return True iff both are True\n",
    "#\n",
    "# Input is matrix[bool], vector[bool]\n",
    "# Return is matrix[bool]\n",
    "#\n",
    "a = np.array([[True, True, False, False],\n",
    "              [True, True, True, False],\n",
    "              [False, True, False, True]\n",
    "             ])\n",
    "b = np.array([True, False, True, False])\n",
    "ma_compute_both_masked(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120.,  60.,  36.])"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of ma_prod()\n",
    "#\n",
    "# input is matrix of numbers\n",
    "# axis must be specified\n",
    "# return vector of numbers\n",
    "\n",
    "a = np.array([[1,2,3,4,5],\n",
    "              [1,3,5,2,2],\n",
    "              [3,2,1,2,3]\n",
    "             ])\n",
    "ma_prod(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False,  True, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of ma_compare_to_zero()\n",
    "# input is vector of numbers\n",
    "# return vector of booleans\n",
    "#\n",
    "a = np.array([1,2,3,0,4,5,0,6])\n",
    "ma_compare_to_zero(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def increase_matrix_size(current_matrix):\n",
    "    \"\"\"Double row count of matrix, retaining old data\n",
    "\n",
    "    Parameter: current_matrix\n",
    "\n",
    "    Returns: updated current_matrix with additional rows\n",
    "\n",
    "    TODO: filter out fully masked rows (broadcast) during copying\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print('Increasing matrix size from', current_matrix.shape)\n",
    "    current_matrix_row_count, column_count = current_matrix.shape # rows, then columns\n",
    "    new_matrix_row = ma.MaskedArray(\n",
    "        data = [-1] * column_count,\n",
    "        mask = [True] * column_count\n",
    "    )\n",
    "    new_matrix = ma.MaskedArray(\n",
    "        data = np.append(\n",
    "            current_matrix.data,\n",
    "            [new_matrix_row] * current_matrix_row_count,\n",
    "            0 # rows, not columns\n",
    "        ),\n",
    "        mask = np.append(\n",
    "            current_matrix.mask,\n",
    "            [new_matrix_row.mask] * current_matrix_row_count,\n",
    "            0\n",
    "        )\n",
    "    )\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 paragraphs from 3 witnesses\n",
      "len(prioritized_blocks)=1063\n",
      "len(lcp_array)=1916\n",
      "aligning 2 paragraphs\n"
     ]
    }
   ],
   "source": [
    "%run create-blocks.ipynb # data will be in prioritized_blocks\n",
    "print('aligning', how_many_paragraphs, 'paragraphs') # confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create witness_node (dataclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "@dataclass(unsafe_hash=True)\n",
    "class WitnessNode:\n",
    "    \"\"\"TODO: Write a docstring some day\"\"\"\n",
    "    token_string: str\n",
    "    witness: str\n",
    "    witness_offset: int\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.token_string\n",
    "\n",
    "# Subclass of WitnessNodeEnd so that our constructed end node will be able to identify itself\n",
    "class WitnessNodeEnd(WitnessNode):\n",
    "    \"\"\"TODO: Write a docstring some day\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Store all witness nodes in dictionary of lists\n",
    "witness_node_lists = defaultdict(list) # one per witness (keys will be sigla)\n",
    "# create, for each witness:\n",
    "#   nodes for real tokens\n",
    "#   END node\n",
    "# A start node is not created. That is on purpose.\n",
    "for index, witness_siglum in enumerate(witness_sigla):\n",
    "    # witness_sigla is a global set when the input data is read\n",
    "    for witness_token_offset, witness_token in enumerate(witnesses[index]): # list of tokens in a single witness\n",
    "        witness_node_lists[witness_siglum].append(WitnessNode(witness_token, witness_siglum, witness_token_offset))\n",
    "    witness_node_lists[witness_siglum].append(WitnessNodeEnd('END', witness_siglum, len(token_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create candidate vectors from block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_candidate_vectors(block):\n",
    "    \"\"\"Returns list of data for block, one numpy array per hyperedge\"\"\"\n",
    "    candidate_vectors = [] # list of individual vectors, not a matrix\n",
    "    for token_offset_in_block in range(block.token_count):\n",
    "        # first add the token_offset_in_block to the block start positions\n",
    "        offset_start_positions = [value + token_offset_in_block for value in block.all_start_positions]\n",
    "        tokens_to_place = [ # list of all token positions, not just first in each witness\n",
    "            (\n",
    "                token_membership_array[token_array_index],\n",
    "                token_witness_offset_array[token_array_index]\n",
    "            )\n",
    "            for token_array_index in offset_start_positions\n",
    "        ]\n",
    "\n",
    "        data_for_new_vector = [0] * len(witness_sigla) # initialize to meaningless values\n",
    "        mask_for_new_vector = [True] * len(witness_sigla) # we'll unmask individual values as needed\n",
    "        for witness_number, witness_offset in tokens_to_place:\n",
    "            data_for_new_vector[witness_number] = witness_offset\n",
    "            mask_for_new_vector[witness_number] = False\n",
    "        candidate_vectors.append(ma.MaskedArray(data=data_for_new_vector, mask=mask_for_new_vector))\n",
    "\n",
    "    return candidate_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Place vectors if allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def check_whether_okay_to_place(current_vectors, potential_vector) -> bool:\n",
    "    \"\"\"Return True iff we can add row without creating transpositions\n",
    "\n",
    "    current_vectors: vector space before new addition\n",
    "    potential_vector: we check whether this can be added\n",
    "\n",
    "    If subtracting a potential vector from any existing vector would return\n",
    "    values that diverge in sign, the potential would cross the existing one\n",
    "\n",
    "    If it's okay to place, we need to call merge_vector() to see whether we\n",
    "    should merge\"\"\"\n",
    "    subtractionResult = current_vectors - potential_vector\n",
    "    signs = np.sign(subtractionResult)\n",
    "    okayToPlace = (signs.min(axis=1) == signs.max(axis=1)).all()\n",
    "    return True if okayToPlace is ma.masked else okayToPlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def merge_vectors(existing_vector: ma.MaskedArray, new_vector: ma.MaskedArray) -> ma.MaskedArray:\n",
    "    \"\"\"Combine non-masked values of two vectors, returns one vector\n",
    "\n",
    "    Sample input:\n",
    "        v_candidate = ma.MaskedArray(data=[-1, 21, 22, -1], mask=[True, False, False, True])\n",
    "        v_existing = ma.MaskedArray(data=[-1, -1, 22, 23], mask=[True, True, False, False])\n",
    "\n",
    "    Sample result:\n",
    "        masked_array(data=[--, 21, 22, 23], mask=[ True, False, False, False])\n",
    "        data: non-masked values of two vectors, some of which were already in both\n",
    "        mask: mask only positions that were masked in both input vectors\n",
    "\n",
    "    NB:\n",
    "        does not trap bad data (input vectors that have different non-masked values in same positions)\n",
    "\n",
    "    \"\"\"\n",
    "    v_new = ma.MaskedArray(\n",
    "        data=np.maximum(\n",
    "            existing_vector.filled(-1).data,\n",
    "            new_vector.filled(-1).data),\n",
    "        mask=(\n",
    "            existing_vector.mask * new_vector.mask),\n",
    "        fill_value=-1\n",
    "    )\n",
    "    return v_new\n",
    "\n",
    "\n",
    "# ma1_data = ma1.filled(-1)\n",
    "# ma2_data = ma2.filled(-1)\n",
    "# max_values = np.maximum(ma1_data, ma2_data)\n",
    "# merged_mask = ma1.mask * ma2.mask\n",
    "# merged_result = ma.MaskedArray(data=max_values, mask=merged_mask, fill_value=-1)\n",
    "# print(merged_result.data)\n",
    "# merged_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def add_new_vector(input_tuple, new_vector):\n",
    "    \"\"\"Add row to matrix and update pointer to next empty row\n",
    "\n",
    "    Parameters:\n",
    "        input_tuple : current_matrix, pointer to next empty row\n",
    "        new_vector : masked array vector to add as new row\n",
    "\n",
    "    Returns tuple of:\n",
    "        updated matrix, updated pointer\n",
    "    \"\"\"\n",
    "    current_matrix, pointer = input_tuple\n",
    "    current_matrix_row_count = current_matrix.shape[0]\n",
    "    if pointer == current_matrix_row_count: # need more rows now!!!\n",
    "        current_matrix = increase_matrix_size(current_matrix)\n",
    "    current_matrix[pointer] = new_vector\n",
    "    pointer += 1\n",
    "    return (current_matrix, pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Operates on masked arrays, so cannot be compiled with numba\n",
    "# def create_filter(current_matrix, candidate):\n",
    "#     #\n",
    "#     # For each step, handle mask separately from data\n",
    "#     #\n",
    "#     # Subtract candidate from matrix (matrix of numbers)\n",
    "#     #\n",
    "#     subtracted_data = ma_subtract(current_matrix.data, candidate.data)\n",
    "#     subtracted_mask = ma_compute_either_masked(current_matrix.mask, candidate.mask)\n",
    "#     subtraction_result = ma.MaskedArray(data=subtracted_data, mask=subtracted_mask, fill_value=-1)\n",
    "#     # print(subtraction_result)\n",
    "#     #\n",
    "#     # Compute product by row of result of subtraction (masked vector of numbers)\n",
    "#     #\n",
    "#     product_data = ma_prod(np.asarray(subtraction_result.filled(1).data), 1)\n",
    "#     product_mask = np.all(subtraction_result.mask, 1)\n",
    "#     #\n",
    "#     # Compute whether row contains zero value (non-masked vector of booleans)\n",
    "#     #\n",
    "#     zero_test_data = ma_compare_to_zero(product_data)\n",
    "#     # print(f\"{zero_test_data=}\")\n",
    "#     # print(f\"{product_mask=}\")\n",
    "#     zero_test_result = zero_test_data & np.logical_not(product_mask)\n",
    "#     #     return np_prod(current_matrix - candidate, 1) == 0 # vector of booleans, second argument is axis\n",
    "#     return zero_test_result # vector of booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "@nb.njit(parallel=True)\n",
    "def create_filter(current_matrix_data, current_matrix_mask, candidate_data, candidate_mask):\n",
    "    filter = [] # will hold vector of booleans, True if row contains unmasked zero value\n",
    "    row_count = current_matrix_data.shape[0]\n",
    "    subtracted_data = current_matrix_data - candidate_data\n",
    "    subtracted_mask = current_matrix_mask | candidate_mask\n",
    "    for row_number in nb.prange(row_count):\n",
    "        current = zip(subtracted_data[row_number], subtracted_mask[row_number])\n",
    "        non_masked = [item[0] for item in current if not item[1]] # item[1] is True if value is masked\n",
    "        if 0 in non_masked:\n",
    "            filter.append(True)\n",
    "        else:\n",
    "            filter.append(False)\n",
    "    return filter\n",
    "# Would this be better without any numpy functions, e.g., not parallelizing subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=[True, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "t_current_matrix = ma.MaskedArray(\n",
    "    data= [[1, 2, 3, 4],\n",
    "          [-1, -1, -1, -1],\n",
    "          [1, -1, -1, -1],\n",
    "          [3,5,6,4]],\n",
    "    mask=[[False, False, False, False],\n",
    "         [True, True, True, True],\n",
    "         [False, True, True, True],\n",
    "         [False, False, False, False]],\n",
    "    fill_value = -1\n",
    ")\n",
    "t_candidate = ma.MaskedArray(\n",
    "    data=[1, 2,- 1, -1],\n",
    "    mask=[False, False, True, True],\n",
    "    fill_value = -1\n",
    ")\n",
    "result = create_filter(t_current_matrix.data, t_current_matrix.mask, t_candidate.data, t_candidate.mask)\n",
    "print(f\"{result=}\")\n",
    "\n",
    "# This is done. Does it work?\n",
    "#\n",
    "# Rewritten create_filter() function should return vector of booleans:\n",
    "# True iff the product of the row is 0\n",
    "# False if the product of the row is not 0\n",
    "# False if the entire row is masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def add_or_merge_new_vector_into_matrix(input_tuple, candidate):\n",
    "    \"\"\"Return max row values (copy) and indices of rows to update in existing matrix\n",
    "\n",
    "    Parameters:\n",
    "        input_tuple : (existing matrix, pointer to next empty row in matrix)\n",
    "        candidate: new vector\n",
    "\n",
    "    Returns tuple of:\n",
    "        merged_vector : vector to replace first row to update\n",
    "        indices : vector of offsets of rows to update (first) or mask (others)\n",
    "\n",
    "    filter contains vector of booleans, with True for rows in current that are merge candidates\n",
    "    \"\"\"\n",
    "\n",
    "    current_matrix, pointer = input_tuple\n",
    "    # Use arithmetic instead of comparison where possible\n",
    "    filter = create_filter(current_matrix.data, current_matrix.mask, candidate.data, candidate.mask)\n",
    "#    filter = np.prod(current_matrix - candidate, axis=1) == 0 # vector of booleans\n",
    "#    filter = np.prod(current_matrix[:pointer, :] - candidate, axis=1) == 0\n",
    "    indices = np.where(filter == True)[0] # row numbers where boolean is True\n",
    "    if indices.size == 0: # if indices is empty, add new row, update current_matrix and pointer\n",
    "        current_matrix, pointer = add_new_vector(input_tuple, candidate)\n",
    "        return (current_matrix, pointer)\n",
    "    else: # if indices is populated, we merge\n",
    "#        max_row_values = ma.max(current_matrix[:pointer, :][filter], axis=0) # merger of existing, not yet candidate\n",
    "        max_row_values = ma.max(current_matrix[filter], axis=0) # merger of existing, not yet candidate\n",
    "#         if max_row_values.count() == current_matrix.shape[1]: # if max_row_values is fully populated, no need to merge\n",
    "#             pass\n",
    "#         else:\n",
    "        merged_vector = merge_vectors(max_row_values, candidate) # eventual replacement for one of the existing candidates\n",
    "        new_row = merged_vector\n",
    "        rows_to_change = indices\n",
    "        current_matrix[rows_to_change[0]] = new_row # replace first row to replace with merge\n",
    "        column_count = current_matrix.shape[1] # get column count\n",
    "        current_matrix[rows_to_change[1:]] = ma.MaskedArray( # mask other rows to replace\n",
    "            data=[-1] * column_count,\n",
    "            mask=[True] * column_count\n",
    "        )\n",
    "        return (current_matrix, pointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Process blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def process_blocks(input_tuple, selected_blocks):\n",
    "    #current_matrix, pointer = input_tuple\n",
    "    for index, selected_block in enumerate(selected_blocks):\n",
    "        # Create vectors for entire block\n",
    "        new_vectors = create_candidate_vectors(selected_block)\n",
    "        # Check only first and last, merge is both are okay\n",
    "        # NB: Checking only first and last to test entire block will break with discontinuous blocks\n",
    "        #   (should we ever switch to working with discontinuous blocks)\n",
    "        if input_tuple[1] == 0: # no need to check for transpositions the first time; just place\n",
    "            for potential_vector in new_vectors:\n",
    "                input_tuple = add_new_vector(input_tuple, potential_vector)\n",
    "        else:\n",
    "            no_transposition = \\\n",
    "                check_whether_okay_to_place(input_tuple[0], new_vectors[0]) and \\\n",
    "                check_whether_okay_to_place(input_tuple[0], new_vectors[-1])\n",
    "            if no_transposition: # all-or-nothing\n",
    "                for potential_vector in new_vectors:\n",
    "                    input_tuple = add_or_merge_new_vector_into_matrix(input_tuple, potential_vector)\n",
    "            else:\n",
    "                pass\n",
    "    return input_tuple[0] # return updated matrix; TODO: do we need to return pointer, too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# create all vectors (gingerly)\n",
    "# We exclude blocks with repetition (temporarily?)\n",
    "selected_blocks = filter(lambda x: len(x.all_start_positions) == x.witness_count, prioritized_blocks)\n",
    "\n",
    "# set up matrix for vectors\n",
    "# column count is number of witnesses\n",
    "# initial row count is equal to length of longest witness\n",
    "witness_count = len(witness_node_lists)\n",
    "max_witness_length = max([len(witness_node_lists[w]) for w in witness_node_lists])\n",
    "alignment_matrix = ma.MaskedArray(\n",
    "    data = [ma.MaskedArray(\n",
    "        data = [-1] * witness_count,\n",
    "        mask = [True] * witness_count\n",
    "    )] * max_witness_length\n",
    ")\n",
    "pointer = 0\n",
    "\n",
    "%lprun -f add_or_merge_new_vector_into_matrix process_blocks((alignment_matrix, pointer), selected_blocks)\n",
    "# alignment_matrix = process_blocks((alignment_matrix, pointer), selected_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "alignment_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create variant graph from numpy masked array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class VG_node():\n",
    "    \"\"\"Variant graph node\"\"\"\n",
    "    def __init__(self, token_string, data): # dictionary of siglum:witness_offset\n",
    "        self.token_string = token_string\n",
    "        self.__dict__.update(**data)\n",
    "        self._sigla = [key for key in data.keys()]\n",
    "    def __repr__(self):\n",
    "        return self.token_string + \"~\" + \"|\".join([\":\".join([str(key), str(getattr(self, key))]) for key in self.sigla()])\n",
    "    def __setitem__(self, key, value):\n",
    "        self._sigla.append(key)\n",
    "        self.__dict__[key] = value\n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "    def __contains__(self, key):\n",
    "        return key in self.__dict__\n",
    "    def sigla(self):\n",
    "        return self._sigla\n",
    "    def values(self):\n",
    "        return [self[key] for key in self._sigla]\n",
    "    def items(self):\n",
    "        return [(key, self.__dict__[key]) for key in self._sigla]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create networkx variant graph from our (not networkx) alignment graph\n",
    "# Create variant graph edges for start and end nodes (only)\n",
    "# We've treated VG as a global and we refer to it in the earlier functions; should we pass it instead?\n",
    "def augment_or_add_edge_without_conversion(siglum, source_VG_node, target_VG_node):\n",
    "    if VG.has_edge(source_VG_node, target_VG_node):\n",
    "        VG[source_VG_node][target_VG_node][\"siglum\"].append(siglum)\n",
    "    else:\n",
    "        VG.add_edge(\n",
    "            source_VG_node,\n",
    "            target_VG_node,\n",
    "            siglum = [siglum]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Use alignment graph node to look up variant graph node\n",
    "def alignment_node_to_VG_node(alignment_node: WitnessNode) -> VG_node:\n",
    "    global witness_offset_to_VG_node\n",
    "    return witness_offset_to_VG_node[alignment_node.witness][alignment_node.witness_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create variant graph data edges (except start and end nodes)\n",
    "def augment_or_add_edge(siglum, source, target):\n",
    "    source_VG_node = alignment_node_to_VG_node(source)\n",
    "    target_VG_node = alignment_node_to_VG_node(target)\n",
    "    if VG.has_edge(source_VG_node, target_VG_node):\n",
    "        VG[source_VG_node][target_VG_node][\"siglum\"].append(siglum)\n",
    "    else:\n",
    "        VG.add_edge(\n",
    "            source_VG_node,\n",
    "            target_VG_node,\n",
    "            siglum = [siglum]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_variant_graph_from_vector_space(alignment_matrix):\n",
    "    # create variant graph and add start and end nodes\n",
    "\n",
    "    start_node = VG_node(\"START\", {})\n",
    "    end_node = VG_node(\"END\", {})\n",
    "    global VG # does this have to be global; we return it at the end, but we also use it in functions above\n",
    "    VG = nx.DiGraph(start = start_node, end = end_node) # create start and end properties to find terminal nodes\n",
    "    VG.add_node(start_node)\n",
    "    VG.add_node(end_node)\n",
    "\n",
    "    # keep track of which witness nodes (and therefore also hyperedges)\n",
    "    # have been added to VG\n",
    "    # does this duplicate information available from the following structure, since\n",
    "    #   if we assign a value below to replace the None, this doesn't seem to add anything\n",
    "    #   Can we get rid of this and use witness_offset_to_VG_node for this purpose instead?\n",
    "    # NB: Includes START and END, which we will later ignore\n",
    "    from bitarray import bitarray\n",
    "    global VG_tracking # keep this, but should we return it instead of making it global?\n",
    "    VG_tracking = {}\n",
    "    for siglum, witness_node_list in witness_node_lists.items():\n",
    "        VG_tracking[siglum] = bitarray(len(witness_node_list))\n",
    "        VG_tracking[siglum].setall(0)\n",
    "\n",
    "    # map from witness token node to variant graph node (needed to construct edges)\n",
    "    # values will be added as we create variant graph nodes\n",
    "    # NB: Includes START and END, which we will later ignore\n",
    "    global witness_offset_to_VG_node # does this have to be global?\n",
    "    witness_offset_to_VG_node = {}\n",
    "    for siglum in witness_node_lists.keys():\n",
    "        witness_offset_to_VG_node[siglum] = [None] * (len(witness_node_lists[siglum]))\n",
    "\n",
    "    # Replaces old code in following cell to build variant graph,\n",
    "    #   this time using vector space as main data source\n",
    "\n",
    "    # 1. Convert vectors to variant graph nodes\n",
    "\n",
    "    for row in alignment_matrix:\n",
    "        if debug:\n",
    "            print(' '.join(('Processing', str(row))))\n",
    "        if row.all() is ma.masked: # don't process fully masked rows\n",
    "            continue\n",
    "        else:\n",
    "            # create dictionary of siglum:value for node (variable name: data) and update globals\n",
    "            data = {}\n",
    "            for index, value in enumerate(row):\n",
    "                if value != alignment_matrix.fill_value:\n",
    "                    siglum = sigla[index]\n",
    "                    value = int(value)\n",
    "                    data[siglum] = value # add to eventually data for new VG node\n",
    "                    VG_tracking[siglum][value] = 1 # update global; do we need to adjust by 1?\n",
    "            # get token string for node (variable name: token_string)\n",
    "            siglum, offset = next(iter(data.items()))\n",
    "            token_string = witness_node_lists[siglum][int(offset)].token_string\n",
    "            # create and add new VG_node\n",
    "            new_VG_node = VG_node(token_string, data)\n",
    "            VG.add_node(new_VG_node)\n",
    "            for siglum, offset in data.items(): # Eek! Another for loop! How embarrassing!\n",
    "                witness_offset_to_VG_node[siglum][offset] = new_VG_node # update other global;\n",
    "\n",
    "    # 2. Traverse variant graph (in arbitrary order), each of which contains witness nodes.\n",
    "    #    Draw outgoing edges, which point to nodes with next tokens in each witness present\n",
    "    #      on the variant graph node.\n",
    "    #    Creating new variant graph nodes for witness tokens not in a hyperedge.\n",
    "    #    Use queue because we're iterating over a dynamic structure (inventory of variant graph nodes).\n",
    "\n",
    "    VG_node_queue = queue.Queue()\n",
    "    #\n",
    "    # Temporarily skipping START and END, which are the first two nodes\n",
    "    #\n",
    "    for node in VG.nodes(): # add all initial VG nodes to queue except START and END\n",
    "        VG_node_queue.put(node)\n",
    "        if debug:\n",
    "            print(f\"Adding node to VG_node_queue; length is {VG_node_queue.qsize()=}\")\n",
    "    ignore_start = VG_node_queue.get() # temporarily ignore START node when creating edges\n",
    "    ignore_end = VG_node_queue.get() # temporarily ignore END node when creating edges\n",
    "    if debug:\n",
    "        print(f\"Removed START and END; length is {VG_node_queue.qsize()=}\")\n",
    "    while not VG_node_queue.empty():\n",
    "        if debug:\n",
    "            print(f\"Processing source node from VG_node_queue; length is {VG_node_queue.qsize()=}\")\n",
    "        source_node = VG_node_queue.get()\n",
    "        targets = set()\n",
    "        edge_labels = defaultdict(list) # key is target VG node, value is list of sigla\n",
    "        source_sigla = source_node._sigla # all sigla on source node\n",
    "        for siglum in source_sigla:\n",
    "            # target may or may not already exist as node in VG\n",
    "            source_offset = source_node[siglum]\n",
    "            target_offset = source_offset + 1\n",
    "            target = witness_offset_to_VG_node[siglum][target_offset]\n",
    "            if target: # does the target already exist?:\n",
    "                targets.add(target)\n",
    "                edge_labels[target].append(siglum)\n",
    "            else: # 1) create target, 2) add to targets, 3) add to queue of nodes, and 4 + 5) update globals\n",
    "                # TODO: Do we need to update globals? We shouldn't need to return to nodes we add here.\n",
    "                # NB: If real target object is of type WitnessNodeEnd, don't add it to the queue or create an edge\n",
    "                # print(f\"{type(witness_node_lists[siglum][target_offset])=}\")\n",
    "                witness_node_target = witness_node_lists[siglum][target_offset]\n",
    "                if type(witness_node_target) != WitnessNodeEnd:\n",
    "                    target_token_string = witness_node_target.token_string\n",
    "                    new_VG_node = VG_node(target_token_string, {siglum: target_offset}) # 1 create target\n",
    "                    targets.add(new_VG_node) # 2 add to targets\n",
    "                    edge_labels[new_VG_node].append(siglum)\n",
    "                    VG_node_queue.put(new_VG_node) # 3 add to queue of nodes\n",
    "                    VG_tracking[siglum][target_offset] = 1 # 4 update first global\n",
    "                    witness_offset_to_VG_node[siglum][target_offset] = new_VG_node # 5. update second global\n",
    "        for target in targets:\n",
    "            VG.add_edge(source_node, target, label=\",\".join(edge_labels[target]))\n",
    "        #\n",
    "        # Add edges for start VG nodes\n",
    "        #\n",
    "        all_first_data_nodes = defaultdict(list)\n",
    "        for siglum in witness_sigla:\n",
    "            key =  witness_offset_to_VG_node[siglum][0]\n",
    "            all_first_data_nodes[key].append(siglum)\n",
    "        for key,value in all_first_data_nodes.items():\n",
    "            VG.add_edge(start_node, key, label=\",\".join(value))\n",
    "\n",
    "        #\n",
    "        # Add edges for end VG node\n",
    "        #\n",
    "        all_end_data_nodes = defaultdict(list)\n",
    "        for siglum in witness_sigla:\n",
    "            # Figure out why -2 works and clean up as needed\n",
    "            key = witness_offset_to_VG_node[siglum][-2]\n",
    "            all_end_data_nodes[key].append(siglum)\n",
    "        for key,value in all_end_data_nodes.items():\n",
    "            VG.add_edge(key, end_node, label=\",\".join(value))\n",
    "\n",
    "    # we're done! return the result\n",
    "    return VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# create output\n",
    "VG = create_variant_graph_from_vector_space(alignment_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualize variant graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# # node id values must be strings for graphviz\n",
    "a = graphviz.Digraph(format=\"svg\", name=\"variant_graph_unjoined\")\n",
    "a.attr(rankdir = \"LR\")\n",
    "a.attr(rank = 'same')\n",
    "a.attr(compound='true')\n",
    "\n",
    "# plot nodes, building {node: id} for lookup\n",
    "node_to_id = {}\n",
    "for index, node in enumerate(VG.nodes()):\n",
    "    node_id = str(index)\n",
    "    node_to_id[node] = node_id\n",
    "    if node is None:\n",
    "        node_text = 'NONE'\n",
    "    else:\n",
    "        node_text = node.token_string + \" (\" + node_id + \")\"\n",
    "    a.node(node_id, label=node_text)\n",
    "\n",
    "# plot edges\n",
    "for edge in VG.edges(data=True):\n",
    "    # edge is a three-item tuple: source, target, dictionary of properties\n",
    "    a.edge(node_to_id[edge[0]], node_to_id[edge[1]], label=edge[2][\"label\"])\n",
    "\n",
    "# print('aligning', how_many_paragraphs, 'paragraphs') # confirm\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#  This function joins the variant graph in place.\n",
    "#  This function is a straight port of the Java version of CollateX.\n",
    "\n",
    "def join(graph):\n",
    "    processed = set()\n",
    "    end = graph.graph[\"end\"]\n",
    "    queue = deque()\n",
    "    for (_, neighbor) in graph.out_edges(graph.graph[\"start\"]):\n",
    "        queue.appendleft(neighbor)\n",
    "    while queue:\n",
    "        vertex = queue.popleft()\n",
    "        out_edges = graph.out_edges(vertex)\n",
    "        if len(out_edges) == 1:\n",
    "            (_, join_candidate) = next(iter(out_edges))\n",
    "            can_join = join_candidate != end and len(graph.in_edges(join_candidate)) == 1\n",
    "            if can_join:\n",
    "                join_vertex_and_join_candidate(graph, join_candidate, vertex)\n",
    "                # we have merged join_candidate (tokens originally to the right) into vertex (tokens originally to the left)\n",
    "                # (for now, join_candidate node and all of its edges are still there, and we need to remove edges before\n",
    "                #   we can remove node, which we do in a for loop)\n",
    "                #\n",
    "                # RESUME HERE: both of the following branches are wrong, the first cryptically (it shows sigla, but not\n",
    "                #   all sigla) and the second conspicuously\n",
    "                #\n",
    "                for (_, neighbor, data) in list(graph.out_edges(join_candidate, data=True)):\n",
    "                    graph.remove_edge(join_candidate, neighbor)\n",
    "                    if graph.has_edge(vertex, neighbor):\n",
    "                        continue # TODO: this looks wrong\n",
    "#                         graph[source][target][\"siglum\"].append(data[\"siglum\"])\n",
    "                    else:\n",
    "#                         graph.add_edge(vertex, neighbor, siglum=data[\"siglum\"])\n",
    "                         graph.add_edge(vertex, neighbor,label=data[\"label\"])\n",
    "                graph.remove_edge(vertex, join_candidate)\n",
    "                graph.remove_node(join_candidate)\n",
    "                queue.appendleft(vertex)\n",
    "                continue\n",
    "        processed.add(vertex)\n",
    "        for (_, neighbor) in out_edges:\n",
    "            # FIXME: Why do we run out of memory in some cases here, if this is not checked?\n",
    "            if neighbor not in processed:\n",
    "                queue.appendleft(neighbor)\n",
    "\n",
    "\n",
    "def join_vertex_and_join_candidate(graph, join_candidate, vertex):\n",
    "    # Note: since there is no normalized/non normalized content in the graph\n",
    "    # a space character is added here for non punctuation tokens\n",
    "\n",
    "    if re.match(r'^\\W', join_candidate.token_string):\n",
    "        vertex.token_string += join_candidate.token_string\n",
    "    else:\n",
    "        vertex.token_string += (\" \" + join_candidate.token_string)\n",
    "    # join_candidate must have exactly one token (inside a list); left item may have more\n",
    "#     for siglum, token in join_candidate.tokens.items():\n",
    "#         vertex.add_token(siglum, token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# len(nx.algorithms.cycles.find_cycle(VG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# nx.algorithms.cycles.find_cycle(VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "join(VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# pp.pprint([edge for edge in VG.edges()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "## node id values must be strings for graphviz\n",
    "a = graphviz.Digraph(format=\"svg\", name=\"variant_graph_joined\")\n",
    "a.attr(rankdir = \"LR\")\n",
    "a.attr(rank = 'same')\n",
    "a.attr(compound='true')\n",
    "\n",
    "# plot nodes, building {node: id} for lookup\n",
    "node_to_id = {}\n",
    "for index, node in enumerate(VG.nodes()):\n",
    "    node_id = str(index)\n",
    "    node_to_id[node] = node_id\n",
    "    if node is None:\n",
    "        node_text = 'None'\n",
    "    else:\n",
    "        node_text = node.token_string + \" (\" + node_id + \")\"\n",
    "    a.node(node_id, label=node_text)\n",
    "\n",
    "# plot edges\n",
    "for edge in VG.edges(data=True):\n",
    "    # edge is a three-item tuple: source, target, dictionary of properties\n",
    "#     label = \"(all)\" if len(edge[2][\"siglum\"]) == len(witness_sigla) else \",\".join(sorted(edge[2][\"siglum\"]))\n",
    "    label = edge[2][\"label\"]\n",
    "    a.edge(node_to_id[edge[0]], node_to_id[edge[1]], label=label)\n",
    "\n",
    "print('aligning', how_many_paragraphs, 'paragraphs') # confirm\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Find first non-masked value in each column to avoid traversing entire column\n",
    "# In our matrix, all non-masked column values for rows that are merge candidates\n",
    "#   have the same value. Previously we found max(), which had to look at them all\n",
    "#   and compare, but the first value will necessarily also be the max().\n",
    "\n",
    "current_matrix = ma.MaskedArray(\n",
    "    data =[[-1, 2, 3], [4, -1,6], [7,8,-1]],\n",
    "    mask=[[True, False, False],[False, True, False],[False, False, True]],\n",
    "    fill_value=-1\n",
    ")\n",
    "columns = current_matrix.T # swap rows and columns and process (new) rows\n",
    "[print(column) for column in columns]\n",
    "first_non_masked = [column[column.mask == False][0] for column in columns]\n",
    "first_non_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}