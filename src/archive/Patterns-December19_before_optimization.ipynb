{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Collation with decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Find ngrams and positions in witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# two witnesses, with repetition and transposition\n",
    "w1 = \"the red and the black cat\"\n",
    "w2 = \"the black and the red cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize_witnesses(w1_string, w2_string):\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    w1_tokens = w1.split()\n",
    "    w2_tokens = w2.split()\n",
    "    witnesses = [w1_tokens, w2_tokens]\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'red', 'and', 'the', 'black', 'cat'], ['the', 'black', 'and', 'the', 'red', 'cat']]\n"
     ]
    }
   ],
   "source": [
    "witnesses = tokenize_witnesses(w1, w2)\n",
    "print(witnesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "# create a function that creates n-grams and returns the offsets\n",
    "def compute_ngrams(witness, n):\n",
    "   output = defaultdict(list)\n",
    "   for i in range(len(witness)-n+1):\n",
    "       g = ' '.join(witness[i:i+n])\n",
    "       output[g].append(i)\n",
    "   return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'the': [0, 3], 'red': [1], 'and': [2], 'black': [4], 'cat': [5]})\n",
      "defaultdict(<class 'list'>, {'the red': [0], 'red and': [1], 'and the': [2], 'the black': [3], 'black cat': [4]})\n",
      "defaultdict(<class 'list'>, {'the': [0, 3], 'black': [1], 'and': [2], 'red': [4], 'cat': [5]})\n",
      "defaultdict(<class 'list'>, {'the black': [0], 'black and': [1], 'and the': [2], 'the red': [3], 'red cat': [4]})\n"
     ]
    }
   ],
   "source": [
    "for witness in witnesses:\n",
    "    print(compute_ngrams(witness, 1))\n",
    "    print(compute_ngrams(witness, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the red', 'and the', 'the black'}\n"
     ]
    }
   ],
   "source": [
    "# find bigrams common to the two witnesses\n",
    "w1_bigrams = compute_ngrams(witnesses[0], 2)\n",
    "w2_bigrams = compute_ngrams(witnesses[1], 2)\n",
    "shared_keys = w1_bigrams.keys() & w2_bigrams.keys()\n",
    "print(shared_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_ngrams_all(witness):\n",
    "   '''Create a function that creates n-grams and returns the offsets'''\n",
    "   output = defaultdict(list)\n",
    "   output2 = {}\n",
    "   for n in range(1, len(witness) + 1):\n",
    "       for i in range(len(witness)-n+1):\n",
    "           g = ' '.join(witness[i:i+n])\n",
    "           output[g].append(i)\n",
    "           output2[g] = n\n",
    "   return output, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 defaultdict(<class 'list'>, {'the': [0, 3], 'red': [1], 'and': [2], 'black': [4], 'cat': [5], 'the red': [0], 'red and': [1], 'and the': [2], 'the black': [3], 'black cat': [4], 'the red and': [0], 'red and the': [1], 'and the black': [2], 'the black cat': [3], 'the red and the': [0], 'red and the black': [1], 'and the black cat': [2], 'the red and the black': [0], 'red and the black cat': [1], 'the red and the black cat': [0]})\n",
      "w2 defaultdict(<class 'list'>, {'the': [0, 3], 'black': [1], 'and': [2], 'red': [4], 'cat': [5], 'the black': [0], 'black and': [1], 'and the': [2], 'the red': [3], 'red cat': [4], 'the black and': [0], 'black and the': [1], 'and the red': [2], 'the red cat': [3], 'the black and the': [0], 'black and the red': [1], 'and the red cat': [2], 'the black and the red': [0], 'black and the red cat': [1], 'the black and the red cat': [0]})\n",
      "{'the': 1, 'red': 1, 'and': 1, 'black': 1, 'cat': 1, 'the red': 2, 'red and': 2, 'and the': 2, 'the black': 2, 'black cat': 2, 'the red and': 3, 'red and the': 3, 'and the black': 3, 'the black cat': 3, 'the red and the': 4, 'red and the black': 4, 'and the black cat': 4, 'the red and the black': 5, 'red and the black cat': 5, 'the red and the black cat': 6, 'black and': 2, 'red cat': 2, 'the black and': 3, 'black and the': 3, 'and the red': 3, 'the red cat': 3, 'the black and the': 4, 'black and the red': 4, 'and the red cat': 4, 'the black and the red': 5, 'black and the red cat': 5, 'the black and the red cat': 6}\n"
     ]
    }
   ],
   "source": [
    "# compute ngrams for all (both) witnesses\n",
    "ngram_offset_by_witness_dict = {}\n",
    "ngram_length = {}\n",
    "for index, witness in enumerate(witnesses):\n",
    "    map1, map2 = compute_ngrams_all(witness)\n",
    "    ngram_offset_by_witness_dict['w' + str(index + 1)] = map1\n",
    "    ngram_length.update(map2)\n",
    "\n",
    "for key in ngram_offset_by_witness_dict.keys():\n",
    "    print(key, ngram_offset_by_witness_dict[key])\n",
    "\n",
    "print(ngram_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the red', 'and the', 'and', 'cat', 'red', 'the black', 'the', 'black'}\n"
     ]
    }
   ],
   "source": [
    "# find keys shared by *all* witnesses\n",
    "shared_ngrams = set(ngram_offset_by_witness_dict[\"w1\"].keys())\n",
    "for value in ngram_offset_by_witness_dict.values():\n",
    "    shared_ngrams = shared_ngrams.intersection(value.keys())\n",
    "\n",
    "print(shared_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'the red': [(0, 3)], 'and the': [(2, 2)], 'and': [(2, 2)], 'cat': [(5, 5)], 'red': [(1, 4)], 'the black': [(3, 0)], 'the': [(0, 0), (0, 3), (3, 0), (3, 3)], 'black': [(4, 1)]})\n"
     ]
    }
   ],
   "source": [
    "# use shared keys to find potential alignments\n",
    "# NB: works for only two witnesses\n",
    "# output format: {ngram : [(0,1), (2,3)]}, where \n",
    "#   the two entries in each tuple are for witnesses A and B\n",
    "from collections import defaultdict\n",
    "\n",
    "potential_alignments = defaultdict(list)\n",
    "for ngram in shared_ngrams:\n",
    "    for w1_offset in ngram_offset_by_witness_dict['w1'][ngram]:\n",
    "        for w2_offset in ngram_offset_by_witness_dict['w2'][ngram]:\n",
    "            potential_alignments[ngram].append((w1_offset, w2_offset))\n",
    "\n",
    "print(potential_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {(0, 3): ['the red', 'the'], (2, 2): ['and the', 'and'], (5, 5): ['cat'], (1, 4): ['red'], (3, 0): ['the black', 'the'], (0, 0): ['the'], (3, 3): ['the'], (4, 1): ['black']})\n"
     ]
    }
   ],
   "source": [
    "# Find actual alignments (ngrams with positions in witnesses)\n",
    "#\n",
    "# Build dictionary of offset_tuple : list_of_ngrams\n",
    "#\n",
    "# Source: https://stackoverflow.com/questions/6190331/how-to-implement-an-ordered-default-dict\n",
    "#\n",
    "# agglomerate = collections.OrderedDict()\n",
    "# for i, x in some_generator():\n",
    "#     agglomerate.setdefault(i, []).append(x)\n",
    "#\n",
    "# Tuples of offsets are keys, sorted; values are lists of ngrams at those offsets\n",
    "\n",
    "alignments = defaultdict(list)\n",
    "for key,value in potential_alignments.items():\n",
    "    for t in value:\n",
    "        alignments[t].append(key)\n",
    "\n",
    "print(alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, ('the',)), (0, 3, ('the red', 'the')), (1, 4, ('red',)), (2, 2, ('and the', 'and')), (3, 0, ('the black', 'the')), (3, 3, ('the',)), (4, 1, ('black',)), (5, 5, ('cat',))]\n"
     ]
    }
   ],
   "source": [
    "# Create list of tuples (A-position:int, B-position:int, (ngrams), sorted by A position\n",
    "# Sort ngrams by token count from high to low\n",
    "# ngrams are in a tuple, rather than list, to make them hashable, and therefore set-able\n",
    "#\n",
    "# sorted(alignments.items()) # sorts, but outputs tuples\n",
    "import collections\n",
    "sorted_alignments_witness_A = []\n",
    "for key in sorted(alignments):\n",
    "    sorted_alignments_witness_A.append((key[0],key[1], tuple(sorted(alignments[key], key=lambda x: ngram_length[x], reverse=True ))))\n",
    "\n",
    "print(sorted_alignments_witness_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, ('the',))\n",
      "(0, 3, ('the red', 'the'))\n",
      "(1, 4, ('red',))\n",
      "(2, 2, ('and the', 'and'))\n",
      "(3, 0, ('the black', 'the'))\n",
      "(3, 3, ('the',))\n",
      "(4, 1, ('black',))\n",
      "(5, 5, ('cat',))\n"
     ]
    }
   ],
   "source": [
    "# Does it work?\n",
    "for entry in sorted_alignments_witness_A:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, ('the',)), (3, 0, ('the black', 'the')), (4, 1, ('black',)), (2, 2, ('and the', 'and')), (0, 3, ('the red', 'the')), (3, 3, ('the',)), (1, 4, ('red',)), (5, 5, ('cat',))]\n"
     ]
    }
   ],
   "source": [
    "# TODO: No need to flip and then flip again\n",
    "# Sort dictionary of offset_tuple : list_of_ngrams by tuple; Prefer witness B offsets over witness A offsets\n",
    "# Sort ngrams by length \n",
    "#\n",
    "flipped_witness_offsets = { (k[1], k[0]): v for k,v in alignments.items() }\n",
    "# print(flipped_witness_offsets)\n",
    "\n",
    "sorted_alignments_witness_B = []\n",
    "for key in sorted(flipped_witness_offsets): # flip them back to write A before B\n",
    "    sorted_alignments_witness_B.append((key[1], key[0], tuple(sorted(flipped_witness_offsets[key], key=lambda x: ngram_length[x], reverse=True ))))\n",
    "\n",
    "print(sorted_alignments_witness_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A decision tree node stores the following properties:\n",
    "* unique id\n",
    "* current location in witness A\n",
    "* current location in witness B\n",
    "* parent (except root)\n",
    "* children\n",
    "* aligned patterns\n",
    "* potential alignments by A\n",
    "* potential alignments by B\n",
    "\n",
    "Edges are not stored separately; edge information is represented by parent and children properties of node\n",
    "\n",
    "### Implement with nested dictionaries:\n",
    "* {id:int : properties:dict} for each node\n",
    "* properties (7) : \n",
    "  * current-location-in-A:int (id of node)\n",
    "  * current-location-in-B:int (id of node)\n",
    "  * parent : id:int\n",
    "  * children : [id:int]\n",
    "  * aligned-patterns : [(offsetA:int, offsetB:int, ngram:str)]\n",
    "  * potential-alignments-by-A : [(offsetA:int, offsetB:int, ngram:str)]\n",
    "  * potential-alignments-by-B : [(offsetB:int, offsetA:int, ngram:str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create outer dictionary for decision tree nodes\n",
    "decision_tree = {}\n",
    "\n",
    "# Add root node\n",
    "decision_tree[0] = {}\n",
    "\n",
    "# Supply properties for root node\n",
    "decision_tree[0]['id'] = 0 # what's my key?\n",
    "decision_tree[0]['current-location-in-A'] = -1 # 0 would be the first position\n",
    "decision_tree[0]['current-location-in-B'] = -1\n",
    "decision_tree[0]['parent'] = None # integer\n",
    "decision_tree[0]['children'] = []\n",
    "decision_tree[0]['aligned-patterns'] = []\n",
    "decision_tree[0]['transposed-patterns'] = []\n",
    "decision_tree[0]['potential-alignments-by-A'] = sorted_alignments_witness_A\n",
    "decision_tree[0]['potential-alignments-by-B'] = sorted_alignments_witness_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'current-location-in-A': -1,\n",
       " 'current-location-in-B': -1,\n",
       " 'parent': None,\n",
       " 'children': [],\n",
       " 'aligned-patterns': [],\n",
       " 'transposed-patterns': [],\n",
       " 'potential-alignments-by-A': [(0, 0, ('the',)),\n",
       "  (0, 3, ('the red', 'the')),\n",
       "  (1, 4, ('red',)),\n",
       "  (2, 2, ('and the', 'and')),\n",
       "  (3, 0, ('the black', 'the')),\n",
       "  (3, 3, ('the',)),\n",
       "  (4, 1, ('black',)),\n",
       "  (5, 5, ('cat',))],\n",
       " 'potential-alignments-by-B': [(0, 0, ('the',)),\n",
       "  (3, 0, ('the black', 'the')),\n",
       "  (4, 1, ('black',)),\n",
       "  (2, 2, ('and the', 'and')),\n",
       "  (0, 3, ('the red', 'the')),\n",
       "  (3, 3, ('the',)),\n",
       "  (1, 4, ('red',)),\n",
       "  (5, 5, ('cat',))]}"
      ]
     },
     "execution_count": 205,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look\n",
    "decision_tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Function to add child nodes recursively\n",
    "# Within children:\n",
    "#   Add parent\n",
    "#   Update current location in A and B\n",
    "#   Update aligned patterns\n",
    "#   Update potential alignments for A and B\n",
    "#\n",
    "# For each of the (three) items in the set of nearest_matches, create a child\n",
    "# Assign consecutive id values by counting size of dictionary = could break with multithreading\n",
    "\n",
    "def add_children(parent_id: int) -> dict:\n",
    "    # get nearest matches in new child node and prepare to recur over its children\n",
    "    # Sort the union of nearest matches for consistency during development; does not affect logic\n",
    "    nearest_A_matches = [item for item in decision_tree[parent_id]['potential-alignments-by-A'] if item[0] == decision_tree[parent_id]['potential-alignments-by-A'][0][0]]\n",
    "    nearest_B_matches = [item for item in decision_tree[parent_id]['potential-alignments-by-B'] if item[1] == decision_tree[parent_id]['potential-alignments-by-B'][0][1]]\n",
    "    nearest_matches = sorted({item for item in nearest_A_matches + nearest_B_matches})\n",
    "\n",
    "    for child in nearest_matches:\n",
    "        # child looks like: (1, 2, ('hi mon', 'hi'))\n",
    "        # start location in A, in B, ngrams at that location\n",
    "        child = (child[0], child[1], child[2][0]) # prune ngrams to keep only longest; NB: redefining \"child\" variable\n",
    "        current_ngram_length = ngram_length[child[2]]\n",
    "        id = len(decision_tree)\n",
    "        decision_tree[id] = {}\n",
    "        decision_tree[id]['id'] = id\n",
    "        decision_tree[id]['current-location-in-A'] = child[0] + current_ngram_length - 1\n",
    "        decision_tree[id]['current-location-in-B'] = child[1] + current_ngram_length - 1\n",
    "        decision_tree[id]['parent'] = parent_id\n",
    "        decision_tree[id]['children'] = []\n",
    "        aligned_patterns = decision_tree[parent_id]['aligned-patterns'].copy()\n",
    "        aligned_patterns.append(child)\n",
    "        decision_tree[id]['aligned-patterns'] = aligned_patterns\n",
    "        transposed_patterns = decision_tree[parent_id]['transposed-patterns'].copy()\n",
    "        decision_tree[id]['transposed-patterns'] = transposed_patterns\n",
    "\n",
    "        # TODO: Because lists are sorted, once one is far enough to the right to avoid transposition,\n",
    "        #   all following ones are also okay (in this case, though, must process A and B separately)\n",
    "        # Similarly, once one ngram is safe, all shorter ones are also safe, and don't need to be checked\n",
    "        decision_tree[id]['potential-alignments-by-A'] = []\n",
    "\n",
    "        # while figuring the new potential alignments by a and b we will find transpositions that we need to store.\n",
    "        # We use bitarrays to track (avoid) overlap (subsequences) between detected transposed patterns.\n",
    "        # ba1 and ba2 record transposed patterns\n",
    "        from bitarray import bitarray\n",
    "        ba1 = bitarray(len(witnesses[0]))\n",
    "        ba1.setall(0)\n",
    "        ba2 = bitarray(len(witnesses[1]))\n",
    "        ba2.setall(0)\n",
    "\n",
    "        #TEMP: transposed-patterns': [(2, 2, 'and the'), (4, 1, 'black')],\n",
    "        for tp in decision_tree[id]['transposed-patterns']:\n",
    "            # now we need to fill the bitarray; We need the start position in each witness and the length of the pattern.\n",
    "            tp_ngram_length = ngram_length[tp[2]]\n",
    "            ba1[tp[0]:tp[0]+tp_ngram_length] = 1\n",
    "            ba2[tp[1]:tp[1]+tp_ngram_length] = 1\n",
    "\n",
    "        # ba3 and ba4 record aligned pattern being added\n",
    "        ba3 = bitarray(len(witnesses[0]))\n",
    "        ba3.setall(0)\n",
    "        ba4 = bitarray(len(witnesses[1]))\n",
    "        ba4.setall(0)\n",
    "        ba3[child[0]:child[0]+current_ngram_length] = 1\n",
    "        ba4[child[1]:child[1]+current_ngram_length] = 1\n",
    "\n",
    "        for p in decision_tree[parent_id]['potential-alignments-by-A']: # throw away potentials where no ngram can work\n",
    "            #########################################################################\n",
    "            # TODO; need to distinguish transpositions from tokens already processed\n",
    "            # transpositions are to left in one witness and to right in other witness\n",
    "            #########################################################################\n",
    "            if p[0] > decision_tree[id]['current-location-in-A'] and p[1] > decision_tree[id]['current-location-in-B']:\n",
    "                # both A and B are to the right of the current pattern, so it remains potential\n",
    "                new_ngram_list = [] # check each ngram in each remaining potential starting from the longest (left)\n",
    "                for q in p[2]:\n",
    "                    q_ngram_length = ngram_length[q] # length of current ngram inside current potential\n",
    "                    if p[0] + q_ngram_length > decision_tree[id]['current-location-in-A'] and p[1] + q_ngram_length > decision_tree[id]['current-location-in-B']: # keep the ones that don't entail transposition\n",
    "                        new_ngram_list.append(q)\n",
    "                decision_tree[id]['potential-alignments-by-A'].append((p[0], p[1], tuple(new_ngram_list))) # add potential, with pruned list of ngrams, to new potentials\n",
    "            # check whether the potential pattern overlaps with the last selected pattern\n",
    "            else:\n",
    "                # iterate over the different n-grams\n",
    "                for q in p[2]:\n",
    "                    q_ngram_length = ngram_length[q] # length of current ngram inside current potential\n",
    "                    if ba3[p[0]:p[0]+q_ngram_length].any() or ba4[p[1]:p[1]+q_ngram_length].any():\n",
    "                        continue\n",
    "                    else:\n",
    "                        # determined that the potential pattern is a transposition. However we need to check first whether the instance of this pattern isn't                             # already covered in the transposed patterns property.\n",
    "                        if ba1[p[0]:p[0]+q_ngram_length].any() or ba2[p[1]:p[1]+q_ngram_length].any():\n",
    "                            continue\n",
    "                        decision_tree[id]['transposed-patterns'].append((p[0], p[1], q))\n",
    "                        # need to update the bitarrays with the new transposed pattern\n",
    "                        ba1[p[0]:p[0]+q_ngram_length] = 1\n",
    "                        ba2[p[1]:p[1]+q_ngram_length] = 1\n",
    "\n",
    "        # potentials by A and B are the same tuples, but sorted differently\n",
    "        decision_tree[id]['potential-alignments-by-B'] = sorted(decision_tree[id]['potential-alignments-by-A'], key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        decision_tree[parent_id]['children'].append(id) # add new child to parent\n",
    "        add_children(id) # recur to process children of new child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'id': 0,\n",
       "  'current-location-in-A': -1,\n",
       "  'current-location-in-B': -1,\n",
       "  'parent': None,\n",
       "  'children': [1, 6, 8],\n",
       "  'aligned-patterns': [],\n",
       "  'transposed-patterns': [],\n",
       "  'potential-alignments-by-A': [(0, 0, ('the',)),\n",
       "   (0, 3, ('the red', 'the')),\n",
       "   (1, 4, ('red',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (3, 0, ('the black', 'the')),\n",
       "   (3, 3, ('the',)),\n",
       "   (4, 1, ('black',)),\n",
       "   (5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(0, 0, ('the',)),\n",
       "   (3, 0, ('the black', 'the')),\n",
       "   (4, 1, ('black',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (0, 3, ('the red', 'the')),\n",
       "   (3, 3, ('the',)),\n",
       "   (1, 4, ('red',)),\n",
       "   (5, 5, ('cat',))]},\n",
       " 1: {'id': 1,\n",
       "  'current-location-in-A': 0,\n",
       "  'current-location-in-B': 0,\n",
       "  'parent': 0,\n",
       "  'children': [2, 4],\n",
       "  'aligned-patterns': [(0, 0, 'the')],\n",
       "  'transposed-patterns': [],\n",
       "  'potential-alignments-by-A': [(1, 4, ('red',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (3, 3, ('the',)),\n",
       "   (4, 1, ('black',)),\n",
       "   (5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(4, 1, ('black',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (3, 3, ('the',)),\n",
       "   (1, 4, ('red',)),\n",
       "   (5, 5, ('cat',))]},\n",
       " 2: {'id': 2,\n",
       "  'current-location-in-A': 1,\n",
       "  'current-location-in-B': 4,\n",
       "  'parent': 1,\n",
       "  'children': [3],\n",
       "  'aligned-patterns': [(0, 0, 'the'), (1, 4, 'red')],\n",
       "  'transposed-patterns': [(2, 2, 'and the'), (4, 1, 'black')],\n",
       "  'potential-alignments-by-A': [(5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(5, 5, ('cat',))]},\n",
       " 3: {'id': 3,\n",
       "  'current-location-in-A': 5,\n",
       "  'current-location-in-B': 5,\n",
       "  'parent': 2,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(0, 0, 'the'), (1, 4, 'red'), (5, 5, 'cat')],\n",
       "  'transposed-patterns': [(2, 2, 'and the'), (4, 1, 'black')],\n",
       "  'potential-alignments-by-A': [],\n",
       "  'potential-alignments-by-B': []},\n",
       " 4: {'id': 4,\n",
       "  'current-location-in-A': 4,\n",
       "  'current-location-in-B': 1,\n",
       "  'parent': 1,\n",
       "  'children': [5],\n",
       "  'aligned-patterns': [(0, 0, 'the'), (4, 1, 'black')],\n",
       "  'transposed-patterns': [(1, 4, 'red'), (2, 2, 'and the')],\n",
       "  'potential-alignments-by-A': [(5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(5, 5, ('cat',))]},\n",
       " 5: {'id': 5,\n",
       "  'current-location-in-A': 5,\n",
       "  'current-location-in-B': 5,\n",
       "  'parent': 4,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(0, 0, 'the'), (4, 1, 'black'), (5, 5, 'cat')],\n",
       "  'transposed-patterns': [(1, 4, 'red'), (2, 2, 'and the')],\n",
       "  'potential-alignments-by-A': [],\n",
       "  'potential-alignments-by-B': []},\n",
       " 6: {'id': 6,\n",
       "  'current-location-in-A': 1,\n",
       "  'current-location-in-B': 4,\n",
       "  'parent': 0,\n",
       "  'children': [7],\n",
       "  'aligned-patterns': [(0, 3, 'the red')],\n",
       "  'transposed-patterns': [(2, 2, 'and'), (3, 0, 'the black')],\n",
       "  'potential-alignments-by-A': [(5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(5, 5, ('cat',))]},\n",
       " 7: {'id': 7,\n",
       "  'current-location-in-A': 5,\n",
       "  'current-location-in-B': 5,\n",
       "  'parent': 6,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(0, 3, 'the red'), (5, 5, 'cat')],\n",
       "  'transposed-patterns': [(2, 2, 'and'), (3, 0, 'the black')],\n",
       "  'potential-alignments-by-A': [],\n",
       "  'potential-alignments-by-B': []},\n",
       " 8: {'id': 8,\n",
       "  'current-location-in-A': 4,\n",
       "  'current-location-in-B': 1,\n",
       "  'parent': 0,\n",
       "  'children': [9],\n",
       "  'aligned-patterns': [(3, 0, 'the black')],\n",
       "  'transposed-patterns': [(0, 3, 'the red'), (2, 2, 'and')],\n",
       "  'potential-alignments-by-A': [(5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(5, 5, ('cat',))]},\n",
       " 9: {'id': 9,\n",
       "  'current-location-in-A': 5,\n",
       "  'current-location-in-B': 5,\n",
       "  'parent': 8,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(3, 0, 'the black'), (5, 5, 'cat')],\n",
       "  'transposed-patterns': [(0, 3, 'the red'), (2, 2, 'and')],\n",
       "  'potential-alignments-by-A': [],\n",
       "  'potential-alignments-by-B': []}}"
      ]
     },
     "execution_count": 207,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the root node to start building the tree recursively\n",
    "\n",
    "# Update root node to add children (recursively, to bottom of tree)\n",
    "add_children(0) # function uses global \"decision_tree\" dictionary\n",
    "\n",
    "# take a look\n",
    "decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# get ready to visualize the decision tree in SVG\n",
    "import graphviz\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "82870be7fdb35bf76e95aae8fa94ba3355c09b0c",
      "text/plain": "<IPython.core.display.SVG object>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create graph (based on https://github.com/interedition/collatex/blob/master/collatex-pythonport/collatex/display_module.py)\n",
    "# node id values must be strings\n",
    "a = graphviz.Digraph(format=\"svg\")\n",
    "for key,value in decision_tree.items():\n",
    "    if value['id'] != 0:\n",
    "        node_id = str(value['id'])\n",
    "        a.node(node_id, label=node_id + ':' + value['aligned-patterns'][len(value['aligned-patterns']) - 1][2])\n",
    "        a.edge(str(value['parent']), str(value['id']))\n",
    "SVG(a.view())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}