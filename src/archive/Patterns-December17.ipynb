{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Collation with decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Find ngrams and positions in witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# two witnesses, with repetition and transposition\n",
    "w1 = \"the red and the black cat\"\n",
    "w2 = \"the black and the red cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize_witnesses(w1_string, w2_string):\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    w1_tokens = w1.split()\n",
    "    w2_tokens = w2.split()\n",
    "    witnesses = [w1_tokens, w2_tokens]\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'red', 'and', 'the', 'black', 'cat'], ['the', 'black', 'and', 'the', 'red', 'cat']]\n"
     ]
    }
   ],
   "source": [
    "witnesses = tokenize_witnesses(w1, w2)\n",
    "print(witnesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "# create a function that creates n-grams and returns the offsets\n",
    "def compute_ngrams(witness, n):\n",
    "   output = defaultdict(list)\n",
    "   for i in range(len(witness)-n+1):\n",
    "       g = ' '.join(witness[i:i+n])\n",
    "       output[g].append(i)\n",
    "   return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'the': [0, 3], 'red': [1], 'and': [2], 'black': [4], 'cat': [5]})\n",
      "defaultdict(<class 'list'>, {'the red': [0], 'red and': [1], 'and the': [2], 'the black': [3], 'black cat': [4]})\n",
      "defaultdict(<class 'list'>, {'the': [0, 3], 'black': [1], 'and': [2], 'red': [4], 'cat': [5]})\n",
      "defaultdict(<class 'list'>, {'the black': [0], 'black and': [1], 'and the': [2], 'the red': [3], 'red cat': [4]})\n"
     ]
    }
   ],
   "source": [
    "for witness in witnesses:\n",
    "    print(compute_ngrams(witness, 1))\n",
    "    print(compute_ngrams(witness, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the red', 'and the', 'the black'}\n"
     ]
    }
   ],
   "source": [
    "# find bigrams common to the two witnesses\n",
    "w1_bigrams = compute_ngrams(witnesses[0], 2)\n",
    "w2_bigrams = compute_ngrams(witnesses[1], 2)\n",
    "shared_keys = w1_bigrams.keys() & w2_bigrams.keys()\n",
    "print(shared_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_ngrams_all(witness):\n",
    "   '''Create a function that creates n-grams and returns the offsets'''\n",
    "   output = defaultdict(list)\n",
    "   output2 = {}\n",
    "   for n in range(1, len(witness) + 1):\n",
    "       for i in range(len(witness)-n+1):\n",
    "           g = ' '.join(witness[i:i+n])\n",
    "           output[g].append(i)\n",
    "           output2[g] = n\n",
    "   return output, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 defaultdict(<class 'list'>, {'the': [0, 3], 'red': [1], 'and': [2], 'black': [4], 'cat': [5], 'the red': [0], 'red and': [1], 'and the': [2], 'the black': [3], 'black cat': [4], 'the red and': [0], 'red and the': [1], 'and the black': [2], 'the black cat': [3], 'the red and the': [0], 'red and the black': [1], 'and the black cat': [2], 'the red and the black': [0], 'red and the black cat': [1], 'the red and the black cat': [0]})\n",
      "w2 defaultdict(<class 'list'>, {'the': [0, 3], 'black': [1], 'and': [2], 'red': [4], 'cat': [5], 'the black': [0], 'black and': [1], 'and the': [2], 'the red': [3], 'red cat': [4], 'the black and': [0], 'black and the': [1], 'and the red': [2], 'the red cat': [3], 'the black and the': [0], 'black and the red': [1], 'and the red cat': [2], 'the black and the red': [0], 'black and the red cat': [1], 'the black and the red cat': [0]})\n",
      "{'the': 1, 'red': 1, 'and': 1, 'black': 1, 'cat': 1, 'the red': 2, 'red and': 2, 'and the': 2, 'the black': 2, 'black cat': 2, 'the red and': 3, 'red and the': 3, 'and the black': 3, 'the black cat': 3, 'the red and the': 4, 'red and the black': 4, 'and the black cat': 4, 'the red and the black': 5, 'red and the black cat': 5, 'the red and the black cat': 6, 'black and': 2, 'red cat': 2, 'the black and': 3, 'black and the': 3, 'and the red': 3, 'the red cat': 3, 'the black and the': 4, 'black and the red': 4, 'and the red cat': 4, 'the black and the red': 5, 'black and the red cat': 5, 'the black and the red cat': 6}\n"
     ]
    }
   ],
   "source": [
    "# compute ngrams for all (both) witnesses\n",
    "ngram_offset_by_witness_dict = {}\n",
    "ngram_length = {}\n",
    "for index, witness in enumerate(witnesses):\n",
    "    map1, map2 = compute_ngrams_all(witness)\n",
    "    ngram_offset_by_witness_dict['w' + str(index + 1)] = map1\n",
    "    ngram_length.update(map2)\n",
    "\n",
    "for key in ngram_offset_by_witness_dict.keys():\n",
    "    print(key, ngram_offset_by_witness_dict[key])\n",
    "\n",
    "print(ngram_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red', 'cat', 'and the', 'black', 'the red', 'the', 'the black', 'and'}\n"
     ]
    }
   ],
   "source": [
    "# find keys shared by *all* witnesses\n",
    "shared_ngrams = set(ngram_offset_by_witness_dict[\"w1\"].keys())\n",
    "for value in ngram_offset_by_witness_dict.values():\n",
    "    shared_ngrams = shared_ngrams.intersection(value.keys())\n",
    "\n",
    "print(shared_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'red': [(1, 4)], 'cat': [(5, 5)], 'and the': [(2, 2)], 'black': [(4, 1)], 'the red': [(0, 3)], 'the': [(0, 0), (0, 3), (3, 0), (3, 3)], 'the black': [(3, 0)], 'and': [(2, 2)]})\n"
     ]
    }
   ],
   "source": [
    "# use shared keys to find potential alignments\n",
    "# NB: works for only two witnesses\n",
    "# output format: {ngram : [(0,1), (2,3)]}, where \n",
    "#   the two entries in each tuple are for witnesses A and B\n",
    "from collections import defaultdict\n",
    "\n",
    "potential_alignments = defaultdict(list)\n",
    "for ngram in shared_ngrams:\n",
    "    for w1_offset in ngram_offset_by_witness_dict['w1'][ngram]:\n",
    "        for w2_offset in ngram_offset_by_witness_dict['w2'][ngram]:\n",
    "            potential_alignments[ngram].append((w1_offset, w2_offset))\n",
    "\n",
    "print(potential_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {(1, 4): ['red'], (5, 5): ['cat'], (2, 2): ['and the', 'and'], (4, 1): ['black'], (0, 3): ['the red', 'the'], (0, 0): ['the'], (3, 0): ['the', 'the black'], (3, 3): ['the']})\n"
     ]
    }
   ],
   "source": [
    "# Find actual alignments (ngrams with positions in witnesses)\n",
    "#\n",
    "# Build dictionary of offset_tuple : list_of_ngrams\n",
    "#\n",
    "# Source: https://stackoverflow.com/questions/6190331/how-to-implement-an-ordered-default-dict\n",
    "#\n",
    "# agglomerate = collections.OrderedDict()\n",
    "# for i, x in some_generator():\n",
    "#     agglomerate.setdefault(i, []).append(x)\n",
    "#\n",
    "# Tuples of offsets are keys, sorted; values are lists of ngrams at those offsets\n",
    "\n",
    "alignments = defaultdict(list)\n",
    "for key,value in potential_alignments.items():\n",
    "    for t in value:\n",
    "        alignments[t].append(key)\n",
    "\n",
    "print(alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, ('the',)), (0, 3, ('the red', 'the')), (1, 4, ('red',)), (2, 2, ('and the', 'and')), (3, 0, ('the black', 'the')), (3, 3, ('the',)), (4, 1, ('black',)), (5, 5, ('cat',))]\n"
     ]
    }
   ],
   "source": [
    "# Create list of tuples (A-position:int, B-position:int, (ngrams), sorted by A position\n",
    "# Sort ngrams by token count from high to low\n",
    "# ngrams are in a tuple, rather than list, to make them hashable, and therefore set-able\n",
    "#\n",
    "# sorted(alignments.items()) # sorts, but outputs tuples\n",
    "import collections\n",
    "sorted_alignments_witness_A = []\n",
    "for key in sorted(alignments):\n",
    "    sorted_alignments_witness_A.append((key[0],key[1], tuple(sorted(alignments[key], key=lambda x: ngram_length[x], reverse=True ))))\n",
    "\n",
    "print(sorted_alignments_witness_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, ('the',))\n",
      "(0, 3, ('the red', 'the'))\n",
      "(1, 4, ('red',))\n",
      "(2, 2, ('and the', 'and'))\n",
      "(3, 0, ('the black', 'the'))\n",
      "(3, 3, ('the',))\n",
      "(4, 1, ('black',))\n",
      "(5, 5, ('cat',))\n"
     ]
    }
   ],
   "source": [
    "# Does it work?\n",
    "for entry in sorted_alignments_witness_A:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, ('the',)), (3, 0, ('the black', 'the')), (4, 1, ('black',)), (2, 2, ('and the', 'and')), (0, 3, ('the red', 'the')), (3, 3, ('the',)), (1, 4, ('red',)), (5, 5, ('cat',))]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: No need to flip and then flip again\n",
    "# Sort dictionary of offset_tuple : list_of_ngrams by tuple; Prefer witness B offsets over witness A offsets\n",
    "# Sort ngrams by length \n",
    "#\n",
    "flipped_witness_offsets = { (k[1], k[0]): v for k,v in alignments.items() }\n",
    "# print(flipped_witness_offsets)\n",
    "\n",
    "sorted_alignments_witness_B = []\n",
    "for key in sorted(flipped_witness_offsets): # flip them back to write A before B\n",
    "    sorted_alignments_witness_B.append((key[1], key[0], tuple(sorted(flipped_witness_offsets[key], key=lambda x: ngram_length[x], reverse=True ))))\n",
    "\n",
    "print(sorted_alignments_witness_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A decision tree node stores the following properties:\n",
    "* unique id\n",
    "* current location in witness A\n",
    "* current location in witness B\n",
    "* parent (except root)\n",
    "* children\n",
    "* aligned patterns\n",
    "* potential alignments by A\n",
    "* potential alignments by B\n",
    "\n",
    "Edges are not stored separately; edge information is represented by parent and children properties of node\n",
    "\n",
    "### Implement with nested dictionaries:\n",
    "* {id:int : properties:dict} for each node\n",
    "* properties (7) : \n",
    "  * current-location-in-A:int (id of node)\n",
    "  * current-location-in-B:int (id of node)\n",
    "  * parent : id:int\n",
    "  * children : [id:int]\n",
    "  * aligned-patterns : [(offsetA:int, offsetB:int, ngram:str)]\n",
    "  * potential-alignments-by-A : [(offsetA:int, offsetB:int, ngram:str)]\n",
    "  * potential-alignments-by-B : [(offsetB:int, offsetA:int, ngram:str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create outer dictionary for decision tree nodes\n",
    "decision_tree = {}\n",
    "\n",
    "# Add root node\n",
    "decision_tree[0] = {}\n",
    "\n",
    "# Supply properties for root node\n",
    "decision_tree[0]['current-location-in-A'] = -1 # 0 would be the first position\n",
    "decision_tree[0]['current-location-in-B'] = -1\n",
    "decision_tree[0]['parent'] = None # integer\n",
    "decision_tree[0]['children'] = []\n",
    "decision_tree[0]['aligned-patterns'] = []\n",
    "decision_tree[0]['potential-alignments-by-A'] = sorted_alignments_witness_A\n",
    "decision_tree[0]['potential-alignments-by-B'] = sorted_alignments_witness_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current-location-in-A': -1,\n",
       " 'current-location-in-B': -1,\n",
       " 'parent': None,\n",
       " 'children': [],\n",
       " 'aligned-patterns': [],\n",
       " 'potential-alignments-by-A': [(0, 0, ('the',)),\n",
       "  (0, 3, ('the red', 'the')),\n",
       "  (1, 4, ('red',)),\n",
       "  (2, 2, ('and the', 'and')),\n",
       "  (3, 0, ('the black', 'the')),\n",
       "  (3, 3, ('the',)),\n",
       "  (4, 1, ('black',)),\n",
       "  (5, 5, ('cat',))],\n",
       " 'potential-alignments-by-B': [(0, 0, ('the',)),\n",
       "  (3, 0, ('the black', 'the')),\n",
       "  (4, 1, ('black',)),\n",
       "  (2, 2, ('and the', 'and')),\n",
       "  (0, 3, ('the red', 'the')),\n",
       "  (3, 3, ('the',)),\n",
       "  (1, 4, ('red',)),\n",
       "  (5, 5, ('cat',))]}"
      ]
     },
     "execution_count": 16,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look\n",
    "decision_tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, ('the',)), (3, 0, ('the black', 'the')), (0, 3, ('the red', 'the'))}\n"
     ]
    }
   ],
   "source": [
    "# Traverse from left to right, so children of root are all\n",
    "#   potential alignments at 0 in one or both witnesses\n",
    "#   In this case: four options, three of which are unique\n",
    "#   First item of first tuple is nearest position, so find all tuples with that value\n",
    "\n",
    "nearest_A_matches = [item for item in decision_tree[0]['potential-alignments-by-A'] if item[0] == decision_tree[0]['potential-alignments-by-A'][0][0]]\n",
    "nearest_B_matches = [item for item in decision_tree[0]['potential-alignments-by-B'] if item[1] == decision_tree[0]['potential-alignments-by-B'][0][1]]\n",
    "nearest_matches = {item for item in nearest_A_matches + nearest_B_matches}\n",
    "print(nearest_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'current-location-in-A': -1,\n",
       "  'current-location-in-B': -1,\n",
       "  'parent': None,\n",
       "  'children': [1, 2, 3],\n",
       "  'aligned-patterns': [],\n",
       "  'potential-alignments-by-A': [(0, 0, ('the',)),\n",
       "   (0, 3, ('the red', 'the')),\n",
       "   (1, 4, ('red',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (3, 0, ('the black', 'the')),\n",
       "   (3, 3, ('the',)),\n",
       "   (4, 1, ('black',)),\n",
       "   (5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(0, 0, ('the',)),\n",
       "   (3, 0, ('the black', 'the')),\n",
       "   (4, 1, ('black',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (0, 3, ('the red', 'the')),\n",
       "   (3, 3, ('the',)),\n",
       "   (1, 4, ('red',)),\n",
       "   (5, 5, ('cat',))]},\n",
       " 1: {'current-location-in-A': 0,\n",
       "  'current-location-in-B': 0,\n",
       "  'parent': 0,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(0, 0, 'the')],\n",
       "  'potential-alignments-by-A': [(1, 4, ('red',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (3, 3, ('the',)),\n",
       "   (4, 1, ('black',)),\n",
       "   (5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(4, 1, ('black',)),\n",
       "   (2, 2, ('and the', 'and')),\n",
       "   (3, 3, ('the',)),\n",
       "   (1, 4, ('red',)),\n",
       "   (5, 5, ('cat',))]},\n",
       " 2: {'current-location-in-A': 4,\n",
       "  'current-location-in-B': 1,\n",
       "  'parent': 0,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(3, 0, 'the black')],\n",
       "  'potential-alignments-by-A': [(5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(5, 5, ('cat',))]},\n",
       " 3: {'current-location-in-A': 1,\n",
       "  'current-location-in-B': 4,\n",
       "  'parent': 0,\n",
       "  'children': [],\n",
       "  'aligned-patterns': [(0, 3, 'the red')],\n",
       "  'potential-alignments-by-A': [(5, 5, ('cat',))],\n",
       "  'potential-alignments-by-B': [(5, 5, ('cat',))]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update root node to add children\n",
    "# Within children:\n",
    "#   Add parent\n",
    "#   Update current location in A and B\n",
    "#   Update aligned patterns\n",
    "#   Update potential alignments for A and B\n",
    "#\n",
    "# For each of the (three) items in the set of nearest_matches, create a child\n",
    "# Assign consecutive id values by counting size of dictionary = could break with multithreading\n",
    "\n",
    "for child in nearest_matches:\n",
    "    child = (child[0], child[1], child[2][0]) # prune ngrams to keep only longest\n",
    "    current_ngram_length = ngram_length[child[2]]\n",
    "    id = len(decision_tree)\n",
    "    decision_tree[id] = {}\n",
    "    decision_tree[id]['current-location-in-A'] = child[0] + current_ngram_length - 1\n",
    "    decision_tree[id]['current-location-in-B'] = child[1] + current_ngram_length - 1\n",
    "    decision_tree[id]['parent'] = 0\n",
    "    decision_tree[id]['children'] = []\n",
    "    aligned_patterns = decision_tree[0]['aligned-patterns'].copy()\n",
    "    aligned_patterns.append(child)\n",
    "    decision_tree[id]['aligned-patterns'] = aligned_patterns\n",
    "    \n",
    "    # TODO: Because lists are sorted, once one is far enough to the right to avoid transposition,\n",
    "    #   all following ones are also okay (in this case, though, must process A and B separately)\n",
    "    # Similarly, once one ngram is safe, all shorter ones are also safe, and don't need to be checked\n",
    "    decision_tree[id]['potential-alignments-by-A'] = []\n",
    "    for p in decision_tree[0]['potential-alignments-by-A']: # throw away potentials where no ngram can work\n",
    "        if p[0] > decision_tree[id]['current-location-in-A'] and p[1] > decision_tree[id]['current-location-in-B']:\n",
    "            new_ngram_list = [] # check each ngram in each remaining potential starting from the longest (left)\n",
    "            for q in p[2]:\n",
    "                q_ngram_length = ngram_length[q] # length of current ngram inside current potential\n",
    "                if p[0] + q_ngram_length > decision_tree[id]['current-location-in-A'] and p[1] + q_ngram_length > decision_tree[id]['current-location-in-B']: # keep the ones that don't entail transposition\n",
    "                    new_ngram_list.append(q)\n",
    "            decision_tree[id]['potential-alignments-by-A'].append((p[0], p[1], tuple(new_ngram_list))) # add potential, with pruned list of ngrams, to new potentials\n",
    "\n",
    "    # potentials by A and B are the same tuples, but sorted differently\n",
    "    decision_tree[id]['potential-alignments-by-B'] = sorted(decision_tree[id]['potential-alignments-by-A'], key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    decision_tree[0]['children'].append(id)\n",
    "\n",
    "# take a look\n",
    "decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}