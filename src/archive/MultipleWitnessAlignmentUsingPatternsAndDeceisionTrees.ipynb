{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Store patterns efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Configure to show multiple value for development and debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "3\n",
      "3\n",
      "[1, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# treeset.py\n",
    "#\n",
    "#\n",
    "# Copyright (C) 2016, Ryosuke Fukatani\n",
    "# License: Apache 2.0\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "import bisect\n",
    "\n",
    "\n",
    "class TreeSet(object):\n",
    "    \"\"\"\n",
    "    Binary-tree set like java Treeset.\n",
    "    Duplicate elements will not be added.\n",
    "    When added new element, TreeSet will be sorted automatically.\n",
    "    \"\"\"\n",
    "    def __init__(self, elements):\n",
    "        self._treeset = []\n",
    "        self.addAll(elements)\n",
    "\n",
    "    def addAll(self, elements):\n",
    "        for element in elements:\n",
    "            if element in self: continue\n",
    "            self.add(element)\n",
    "\n",
    "    def add(self, element):\n",
    "        if element not in self:\n",
    "            bisect.insort(self._treeset, element)\n",
    "\n",
    "    def ceiling(self, e):\n",
    "        if not self._treeset:\n",
    "            return None\n",
    "        index = bisect.bisect_right(self._treeset, e)\n",
    "        if self[index - 1] == e:\n",
    "            return e\n",
    "        try:\n",
    "            return self._treeset[bisect.bisect_right(self._treeset, e)]\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "    def floor(self, e):\n",
    "        if not self._treeset:\n",
    "            return None\n",
    "        index = bisect.bisect_left(self._treeset, e)\n",
    "        if index == len(self._treeset):\n",
    "            return self[index-1]\n",
    "        if self[index] == e:\n",
    "            return e\n",
    "        check = self._treeset[bisect.bisect_left(self._treeset, e) - 1]\n",
    "        if check <= e:\n",
    "            return check\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, num):\n",
    "        return self._treeset[num]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._treeset)\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Delete all elements in TreeSet.\"\"\"\n",
    "        self._treeset = []\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"Return shallow copy of self.\"\"\"\n",
    "        return TreeSet(self._treeset)\n",
    "\n",
    "    def remove(self, element):\n",
    "        \"\"\"Remove element if element in TreeSet.\"\"\"\n",
    "        try:\n",
    "            self._treeset.remove(element)\n",
    "        except ValueError:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Do ascending iteration for TreeSet\"\"\"\n",
    "        for element in self._treeset:\n",
    "            yield element\n",
    "\n",
    "    def pop(self, index):\n",
    "        return self._treeset.pop(index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self._treeset)\n",
    "\n",
    "    def __eq__(self, target):\n",
    "        if isinstance(target, TreeSet):\n",
    "            return self._treeset == target.treeset\n",
    "        elif isinstance(target, list):\n",
    "            return self._treeset == target\n",
    "        return None\n",
    "\n",
    "    def __contains__(self, e):\n",
    "        \"\"\"Fast attribution judgment by bisect\"\"\"\n",
    "        try:\n",
    "            return e == self._treeset[bisect.bisect_left(self._treeset, e)]\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ts = TreeSet([3,7,7,1,3])\n",
    "    print(ts.floor(4))\n",
    "    print(ts.ceiling(4))\n",
    "    print(ts.floor(3))\n",
    "    print(ts.ceiling(3))\n",
    "    print(ts)\n",
    "\n",
    "class TreeMap(dict):\n",
    "    \"\"\"\n",
    "    \"TreeMap\" is a dictionary with sorted keys similar to java TreeMap.\n",
    "    Keys, iteration, items, values will all return values ordered by key.\n",
    "    Otherwise it should behave just like the builtin dict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq=None, **kwargs):\n",
    "        if seq is None:\n",
    "            super().__init__(**kwargs)\n",
    "        else:\n",
    "            super().__init__(seq, **kwargs)\n",
    "        self.sorted_keys = TreeSet(super().keys())\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        super().__setitem__(key, value)\n",
    "        self.sorted_keys.add(key)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        super().__delitem__(key)\n",
    "        self.sorted_keys.remove(key)\n",
    "\n",
    "    def keys(self):\n",
    "        return self.sorted_keys\n",
    "\n",
    "    def items(self):\n",
    "        return [(k, self[k]) for k in self.sorted_keys]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for k in self.sorted_keys:\n",
    "            yield k\n",
    "\n",
    "    def values(self):\n",
    "        for k in self.sorted_keys:\n",
    "            yield self[k]\n",
    "\n",
    "    def clear(self):\n",
    "        super().clear()\n",
    "        self.sorted_keys.clear()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     tm = TreeMap({6: None, 13: None})\n",
    "#     print(tm.keys().floor(13))\n",
    "#     print(tm.keys().ceiling(20))\n",
    "#     print(tm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# two witnesses, with repetition and transposition\n",
    "\n",
    "# Original example, single leaf node\n",
    "# w1 = '''the red and the black cat'''\n",
    "# w2 = '''the black and the red cat'''\n",
    "\n",
    "# Three witnesses\n",
    "w0 = '''the red and the black cat'''\n",
    "w1 = '''the black and the red cat'''\n",
    "w2 = '''the black and red and the blue and green cat'''\n",
    "\n",
    "# Adjacent transposition\n",
    "# w1 = '''the red striped cat'''\n",
    "# w2 = '''the striped red cat'''\n",
    "\n",
    "# Two leaf nodes\n",
    "# w1 = '''cat red black'''\n",
    "# w2 = '''cat black red'''\n",
    "\n",
    "# Branches meet in the middle at koala and then split again, with two leaf nodes\n",
    "# w1 = \"\"\"cat red black koala brown gray\"\"\"\n",
    "# w2 = \"\"\"cat black red koala gray brown\"\"\"\n",
    "\n",
    "# Two split and rejoin\n",
    "# w1 = '''the gray koala'''\n",
    "# w2 = '''the brown koala'''\n",
    "\n",
    "# medium example\n",
    "# w1 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ much more from each other, than do the individuals of any one\n",
    "# species or variety in a state of nature.'''\n",
    "# w2 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ more from each other than do the individuals of any one\n",
    "# species or variety in a state of nature.'''\n",
    "\n",
    "# Larger example\n",
    "# w1 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ much more from each other, than do the individuals of any one\n",
    "# species or variety in a state of nature. When we reflect on the vast diversity of the\n",
    "# plants and animals which have been cultivated, and which have varied during all ages\n",
    "# under the most different climates and treatment, I think we are driven to conclude that\n",
    "# this greater variability is simply due to our domestic productions having been raised\n",
    "# under conditions of life not so uniform as, and somewhat different from, those to which\n",
    "# the parent-species have been exposed under nature. There is, also, I think, some\n",
    "# probability in the view propounded by Andrew Knight, that this variability may be partly\n",
    "# connected with excess of food. It seems pretty clear that organic beings must be exposed\n",
    "# during several generations to the new conditions of life to cause any appreciable amount\n",
    "# of variation; and that when the organisation has once begun to vary, it generally\n",
    "# continues to vary for many generations. No case is on record of a variable being ceasing\n",
    "# to be variable under cultivation. Our oldest cultivated plants, such as wheat, still\n",
    "# often yield new varieties: our oldest domesticated animals are still capable of rapid\n",
    "# improvement or modification.'''\n",
    "# w2 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ more from each other than do the individuals of any one\n",
    "# species or variety in a state of nature. When we reflect on the vast diversity of the\n",
    "# plants and animals which have been cultivated, and which have varied during all ages\n",
    "# under the most different climates and treatment, I think we are driven to conclude that\n",
    "# this great variability is simply due to our domestic productions having been raised\n",
    "# under conditions of life not so uniform as, and somewhat different from, those to which\n",
    "# the parent-species have been exposed under nature. There is also, I think, some\n",
    "# probability in the view propounded by Andrew Knight, that this variability may be partly\n",
    "# connected with excess of food. It seems pretty clear that organic beings must be exposed\n",
    "# during several generations to the new conditions of life to cause any appreciable amount\n",
    "# of variation; and that when the organisation has once begun to vary, it generally\n",
    "# continues to vary for many generations. No case is on record of a variable being ceasing\n",
    "# to be variable under cultivation. Our oldest cultivated plants, such as wheat, still\n",
    "# often yield new varieties: our oldest domesticated animals are still capable of rapid\n",
    "# improvement or modification'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Work plan\n",
    "\n",
    "1. Create token array (Python **list**)\n",
    "1. Create suffix array\n",
    "1. Create LCP (**longest common prefix**) array\n",
    "1. Calculate LCP intervals\n",
    "1. Create patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Construct list of ngrams shared by witnesses\n",
    "\n",
    "Find ngrams and positions in witnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize_witnesses(*witness_strings): # one string per witness\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    witnesses = []\n",
    "    for witness_string in witness_strings:\n",
    "        witness_tokens = witness_string.split()\n",
    "        witnesses.append(witness_tokens)\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'red', 'and', 'the', 'black', 'cat'],\n",
       " ['the', 'black', 'and', 'the', 'red', 'cat'],\n",
       " ['the', 'black', 'and', 'red', 'and', 'the', 'blue', 'and', 'green', 'cat']]"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "witnesses = tokenize_witnesses(w0, w1, w2) # strings\n",
    "witnesses # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_token_array(witness_token_lists): # list of token lists per witness\n",
    "    '''Create token array (single list, with separator \" # \" between witnesses'''\n",
    "    token_array = []\n",
    "    token_membership_array = []\n",
    "    tm = TreeMap()\n",
    "    last_witness_offset = len(witness_token_lists) - 1\n",
    "    for index, witness_token_list in enumerate(witness_token_lists):\n",
    "        token_array.extend(witness_token_list)\n",
    "        token_membership_array.extend([index for token in witness_token_list])\n",
    "        if index < last_witness_offset:\n",
    "            separator = \" #\" + str(index + 1) + \" \"\n",
    "            token_array.append(separator)\n",
    "            token_membership_array.append(separator)\n",
    "            tm[len(token_array) - 1] = None\n",
    "    return token_array, token_membership_array, tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'red', 'and', 'the', 'black', 'cat', ' #1 ', 'the', 'black', 'and', 'the', 'red', 'cat', ' #2 ', 'the', 'black', 'and', 'red', 'and', 'the', 'blue', 'and', 'green', 'cat']\n",
      "[0, 0, 0, 0, 0, 0, ' #1 ', 1, 1, 1, 1, 1, 1, ' #2 ', 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "{6: None, 13: None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('red', 0),\n",
       " ('and', 0),\n",
       " ('the', 0),\n",
       " ('black', 0),\n",
       " ('cat', 0),\n",
       " (' #1 ', ' #1 '),\n",
       " ('the', 1),\n",
       " ('black', 1),\n",
       " ('and', 1),\n",
       " ('the', 1),\n",
       " ('red', 1),\n",
       " ('cat', 1),\n",
       " (' #2 ', ' #2 '),\n",
       " ('the', 2),\n",
       " ('black', 2),\n",
       " ('and', 2),\n",
       " ('red', 2),\n",
       " ('and', 2),\n",
       " ('the', 2),\n",
       " ('blue', 2),\n",
       " ('and', 2),\n",
       " ('green', 2),\n",
       " ('cat', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_array, token_membership_array, tm = create_token_array(witnesses)\n",
    "print(token_array) # take a look\n",
    "print(token_membership_array)\n",
    "print(tm)\n",
    "list(zip(token_array, token_membership_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_suffix_array(_token_array):\n",
    "    '''Add docstring'''\n",
    "    _suffixes = []\n",
    "    for index, tokens in enumerate(_token_array):\n",
    "        suffix = _token_array[index:]\n",
    "        _suffixes.append((suffix, index))\n",
    "    _suffixes.sort() # sort in place\n",
    "    return [x[1] for x in _suffixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 13, 21, 16, 2, 18, 9, 15, 8, 4, 20, 23, 5, 12, 22, 1, 17, 11, 14, 7, 3, 19, 0, 10]\n"
     ]
    }
   ],
   "source": [
    "suffix_array = create_suffix_array(token_array)\n",
    "print(suffix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_lcp_array(_suffix_array, _token_array):\n",
    "    '''compute LCP array\n",
    "    which is a sequence of integers representing the number of tokens shared by consecutive alphabetically sorted suffixes\n",
    "    sequential pairs of values in suffix array, which are two offsets in the sorted suffixes\n",
    "    '''\n",
    "    _lcp_array = [0]\n",
    "    for i in range(0, len(_suffix_array) - 1): # for each pair of suffixes, retrieve list of tokens starting at that position\n",
    "        pair = _suffix_array[i:i+2] # for each pair of suffixes\n",
    "        suffix_1 = _token_array[pair[0]:] # tokens starting at first position\n",
    "        suffix_2 = _token_array[pair[1]:] # tokens starting at second position\n",
    "        # print(suffix_1, suffix_2) # diagnostic: verify that they're paired correctly\n",
    "        # pair the tokens up by position, return (number of matches, first non-match)\n",
    "        _lcp_value = next(filter(lambda t: t[1][0] != t[1][1], enumerate(zip(suffix_1, suffix_2))), min(len(suffix_1), len(suffix_2)))\n",
    "        _lcp_array.append(_lcp_value[0] if type(_lcp_value) == tuple else _lcp_value) # most are tuples, but some are just an integer\n",
    "    return _lcp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 2, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "lcp_array = create_lcp_array(suffix_array, token_array)\n",
    "print(lcp_array) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# uncomment to verify the accuracy of the lcp array\n",
    "# for offset in suffix_array:\n",
    "#     print(token_array[offset: offset + 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Use LCP array to calculate patterns (efficiently)\n",
    "#   1. 0 means that whatever follows will have nothing in common with it\n",
    "#   2. Repetition of same number means same pattern\n",
    "#   3. Consecutive non-zero values identify how much of the pattern they have in common,\n",
    "#      Counts are always +1, so there must be two instances of \"the red\", two of \"the black\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create Block dataclass\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not into token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    witnesses: set\n",
    "    witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    # how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_blocks (_lcp_array):\n",
    "    '''Create blocks from lcp array\n",
    "\n",
    "    Skip first lcp value, which is a fake; otherwise compare lcp value to length of block at top of stack.\n",
    "    Four possibilities:\n",
    "\n",
    "        stack is empty\n",
    "            * if lcp value == 0, proceed to next lcp value (continue)\n",
    "            * if lcp value > 0, create block and push onto stack, then proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) matches block length at top of stack\n",
    "            * proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) is longer than block length at top of stack\n",
    "            * create and push new block\n",
    "\n",
    "        lcp value is shorter than block length at top of stack\n",
    "            * (recursive) if block at top of stack is longer than current lcp value, pop and append to _blocks\n",
    "            * if block at top of stack is equal to lcp value, proceed to next lcp value (continue)\n",
    "            * if block at top of stack is shorter than current lcp value ...\n",
    "            *   create and push new block starting at start position of most recently closed block, then proceed to next lcp value (continue)\n",
    "\n",
    "    In other words:\n",
    "\n",
    "        We proceed to next lcp value if:\n",
    "            * stack is empty and lcp value == 0\n",
    "            * lcp value matches block length at top of stack (can we combine this with the preceding, since an empty stack effectively has a zero-length block on top?)\n",
    "\n",
    "        We push a new value on stack and then proceed to next lcp value if:\n",
    "            * stack is empty and lcp value > 0\n",
    "            * lcp value is longer than block length at top of stack (where is the start position?)\n",
    "\n",
    "        We pop from the stack to _blocks and then check the next stack value (stick with same lcp) if:\n",
    "            * lcp value is shorter than current block value\n",
    "\n",
    "cases (occurrences are always one more than number of repetitions):\n",
    "    5 5 2     --> 1 block of 5 occures 3 times, 1 block of 2 occures 4 times\n",
    "    2 5 5 2   --> 1 block of 2 occures 5 times, 1 block of 5 occures 3 times\n",
    "    5 5 0 2   --> 1 block of 5 occures 3 times, 1 block of 2 occures 2 times\n",
    "    2 5 5 2 3 --> \n",
    "\n",
    "\n",
    "Nested while structures:\n",
    "\n",
    "(Create blocks in two places because they have different start positions)\n",
    "(Nested while loops because we traverse two things: lcp array and, sometimes, stack)\n",
    "\n",
    "while next-lcp-value: # traverse lcp array\n",
    "    if something\n",
    "    elif something else\n",
    "    elif perhaps yet another something else\n",
    "    else: # possible hidden block (or possibly not)\n",
    "        while something-on-the-stack: # traverse stack for some lcp value situations\n",
    "            pop larger values\n",
    "        if hidden-block:\n",
    "            create and push\n",
    "clean-up-stack-after-last-lcp-value # or tack a 0 onto the end of the lcp to avoid extra clean-up code\n",
    "'''\n",
    "    from collections import deque # faster append and pop than list\n",
    "    _blocks = []\n",
    "    open_block_stack = deque()\n",
    "    for offset, lcp in enumerate(lcp_array):\n",
    "        # three situations: next one is same value, higher that last, or lower than last\n",
    "        # if same value: same pattern\n",
    "        # if higher or lower, new pattern (may overlap with previous, unless one or the other value is 0)\n",
    "        peek = open_block_stack[-1] if open_block_stack else None\n",
    "        peek_token_count = peek.token_count if peek else 0\n",
    "        if offset == 0: # skip the first one, which is a transition from a fake start value\n",
    "            continue # resume loop with next item in lcp array\n",
    "        elif lcp == peek_token_count:\n",
    "            pass # same pattern (happens with repetition), so do nothing\n",
    "        elif lcp > peek_token_count: # new prefix is longer than previous one, so start new pattern\n",
    "            # can fill in end_position and frequency only when we encounter a shorter value in the LCP array\n",
    "            # start_position is number of patterns that are the same \n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = offset - 1, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "        else: # new prefix is shorter than previous one, so:\n",
    "                # 1. close open blocks with higher values\n",
    "                # 2. do something else\n",
    "            while open_block_stack and open_block_stack[-1].token_count > lcp: # if an open block is longer than the current length, pop and close it\n",
    "                block_being_modified = open_block_stack.pop()\n",
    "                block_being_modified.end_position = offset - 1\n",
    "                block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "                _blocks.append(block_being_modified)\n",
    "            if lcp > 0 and (not open_block_stack or open_block_stack[-1].token_count < lcp):\n",
    "                open_block_stack.append(Block(token_count = lcp, start_position = _blocks[-1].start_position, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "\n",
    "    while open_block_stack: # pop anything left in open_block_stack\n",
    "        block_being_modified = open_block_stack.pop()\n",
    "        block_being_modified.end_position = len(lcp_array) - 1\n",
    "        block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "        _blocks.append(block_being_modified)\n",
    "\n",
    "    # add all_start_positions and then witness_count properties to blocks\n",
    "    for _block in _blocks:\n",
    "        # block_start_position through block_end_position gives offsets of all start positions in suffix_array\n",
    "        _block.all_start_positions = sorted([suffix_array[x] for x in range(_block.start_position,_block.end_position + 1)])\n",
    "        # use all start positions to find witness count\n",
    "        _block.witnesses = set(token_membership_array[offset] for offset in _block.all_start_positions)\n",
    "        _block.witness_count = len(_block.witnesses)\n",
    "\n",
    "    return _blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Block(token_count=2, start_position=4, end_position=6, all_start_positions=[2, 9, 18], witnesses={0, 1, 2}, witness_count=3, frequency=3),\n",
       " Block(token_count=1, start_position=2, end_position=6, all_start_positions=[2, 9, 16, 18, 21], witnesses={0, 1, 2}, witness_count=3, frequency=5),\n",
       " Block(token_count=2, start_position=7, end_position=8, all_start_positions=[8, 15], witnesses={1, 2}, witness_count=2, frequency=2),\n",
       " Block(token_count=1, start_position=7, end_position=9, all_start_positions=[4, 8, 15], witnesses={0, 1, 2}, witness_count=3, frequency=3),\n",
       " Block(token_count=1, start_position=11, end_position=13, all_start_positions=[5, 12, 23], witnesses={0, 1, 2}, witness_count=3, frequency=3),\n",
       " Block(token_count=3, start_position=15, end_position=16, all_start_positions=[1, 17], witnesses={0, 2}, witness_count=2, frequency=2),\n",
       " Block(token_count=1, start_position=15, end_position=17, all_start_positions=[1, 11, 17], witnesses={0, 1, 2}, witness_count=3, frequency=3),\n",
       " Block(token_count=3, start_position=18, end_position=19, all_start_positions=[7, 14], witnesses={1, 2}, witness_count=2, frequency=2),\n",
       " Block(token_count=2, start_position=18, end_position=20, all_start_positions=[3, 7, 14], witnesses={0, 1, 2}, witness_count=3, frequency=3),\n",
       " Block(token_count=2, start_position=22, end_position=23, all_start_positions=[0, 10], witnesses={0, 1}, witness_count=2, frequency=2),\n",
       " Block(token_count=1, start_position=18, end_position=23, all_start_positions=[0, 3, 7, 10, 14, 19], witnesses={0, 1, 2}, witness_count=3, frequency=6)]"
      ]
     },
     "execution_count": 16,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = create_blocks(lcp_array)\n",
    "blocks # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token array: ['the', 'red', 'and', 'the', 'black', 'cat', ' #1 ', 'the', 'black', 'and', 'the', 'red', 'cat', ' #2 ', 'the', 'black', 'and', 'red', 'and', 'the', 'blue', 'and', 'green', 'cat']\n",
      "['and', 'the']\n",
      "['and']\n",
      "['black', 'and']\n",
      "['black']\n",
      "['cat']\n",
      "['red', 'and', 'the']\n",
      "['red']\n",
      "['the', 'black', 'and']\n",
      "['the', 'black']\n",
      "['the', 'red']\n",
      "['the']\n"
     ]
    }
   ],
   "source": [
    "# diagnostic: are the blocks complete and correct\n",
    "print('Token array:', token_array)\n",
    "for block in blocks:\n",
    "    print(token_array[suffix_array[block.start_position]:suffix_array[block.start_position] + block.token_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----\n",
    "\n",
    "## What to do\n",
    "\n",
    "NOTE: This is a simplified greedy approach; will later be integrated into decision tree / graph\n",
    "\n",
    "### Create data structure to store results\n",
    "\n",
    "Eventually this will be a variant graph, but it's easier to build otherwise and construct the graph later. Interim\n",
    "structure is a list of occurrences, which can be compared to a topological sort of a variant graph, and can be used\n",
    "to construct a full variant graph later.\n",
    "\n",
    "### Assign priority to blocks\n",
    "\n",
    "1. Favor those with high witness count, high token count, low frequency (and no transpositions, which we don't know yet); sort blocks by descending priority\n",
    "1. Priority = number of witnesses (depth) divided by (frequency * length) (er … we no longer remember why we selected this formula, but it seemed like a Good Idea)\n",
    "1. Sort blocks by priority from higher to lower, break ties arbitrarily from beginning of alphabet (blocks are already sorted alphabetically)\n",
    "\n",
    "NOTE: We do not take transpositions into consideration, although ultimately the matter, because in the graph \n",
    "approach we'll consider more than one possibility.\n",
    "\n",
    "### Select highest-priority remaining block\n",
    "\n",
    "1. Take all occurrences of current block\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def priority(block: Block) -> float:\n",
    "    '''Priority ranges from 0 to ∞\n",
    "\n",
    "    depth (number of witnesses) / (frequency * length)\n",
    "        modified (by trial and error) to weight the components\n",
    "    scale: # TODO: how can we set these in a generally meaningful way?\n",
    "        high depth (more witnesses) is most important\n",
    "        low frequency (less repetition) is next most important\n",
    "        high length (token count) is least important\n",
    "    higher numbers are better\n",
    "        distance between neighboring values is irrelevant; all that matters is order\n",
    "    '''\n",
    "    # score = pow(block.witness_count,4) / (pow(block.frequency,3) * block.token_count)\n",
    "    score = pow(block.witness_count,6)  * block.token_count / pow(block.frequency,3)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from typing import List\n",
    "def sort_blocks_by_priority (_blocks: List[Block]) -> List[Block]:\n",
    "    return sorted(_blocks, key=lambda x: priority(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['and', 'the'], 54.0),\n",
       " (['the', 'black'], 54.0),\n",
       " (['black'], 27.0),\n",
       " (['cat'], 27.0),\n",
       " (['red'], 27.0),\n",
       " (['red', 'and', 'the'], 24.0),\n",
       " (['the', 'black', 'and'], 24.0),\n",
       " (['black', 'and'], 16.0),\n",
       " (['the', 'red'], 16.0),\n",
       " (['and'], 5.832),\n",
       " (['the'], 3.375)]"
      ]
     },
     "execution_count": 20,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prioritized_blocks = sort_blocks_by_priority(blocks)\n",
    "# take a look\n",
    "[(token_array[suffix_array[_block.start_position]:suffix_array[_block.start_position] + _block.token_count], priority(_block)) for _block in prioritized_blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Variant graph nodes, which we put into topological order\n",
    "# Node objects contain unique id values token position for one or more witnesses\n",
    "from typing import Tuple\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Variant_Graph_Node:\n",
    "    # __slots__ = ['id', 'token_offsets']\n",
    "    id: int\n",
    "    token_offsets: Tuple[int] # indices into token array (from which we can retrieve witness information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create list to hold variant graph nodes in topological order\n",
    "to_variant_graph_nodes = [] # to = topologically ordered\n",
    "token_to_node_membership = [None] * len(token_array) # same length as token array, to see whether a token has a node\n",
    "# token_to_node_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def process_token_instance(_token_offset: int, _sibling_offsets: List[int]): # void, called by process_block()\n",
    "    '''Add token instance to node, creating node if necessary\n",
    "\n",
    "    Possibilities:\n",
    "        If offset is already in a node, we've already placed it, so do nothing\n",
    "        Elif sibling offset already in node, add new offset to that and modify membership list\n",
    "            E.g., if block.all_start_positions = [2, 9, 18] while processing 2, can add to 9 or 18\n",
    "            Eek! in case of transposition, different siblings will be placed in different nodes\n",
    "        Else create new node and modify membership list\n",
    "\n",
    "    Cannot place tokens out of order, so if we can't place a token, find the highest and lowest\n",
    "        to identify available area and to determine whether sibling is transposed\n",
    "    '''\n",
    "    if token_to_node_membership[_token_offset]: # token is already placed, so do nothing\n",
    "        return\n",
    "    _floor = tm.keys().floor(_token_offset) or -1\n",
    "    _ceiling = tm.keys().ceiling(_token_offset) or len(token_array)\n",
    "    _sibling_nodes = [token_to_node_membership[x] for x in _sibling_offsets if (x != _token_offset and token_to_node_membership[x])]\n",
    "    if not _sibling_nodes:\n",
    "        # token is not placed yet in the variant graph, so we create a node\n",
    "        _new_node = Variant_Graph_Node(id = len(to_variant_graph_nodes), token_offsets = (_token_offset,))\n",
    "        to_variant_graph_nodes.append(_new_node)\n",
    "        token_to_node_membership[_token_offset] = _new_node\n",
    "        tm[_token_offset] = _new_node\n",
    "        return\n",
    "    if len(set([_node.id for _node in _sibling_nodes])) > 1: # placed siblings disagree, so do nothing\n",
    "        print(\"Yell: Due to a previous transposition there is more than one node in the variant graph!\")\n",
    "        return\n",
    "    witness = token_membership_array[_token_offset]\n",
    "    # placed siblings agree, so add to that one node\n",
    "    # There is only one node, with multiple token offsets...\n",
    "    not_allowed_sibling_offsets = [offset for offset in _sibling_nodes[0].token_offsets if (token_membership_array[offset] == witness)]\n",
    "    if not_allowed_sibling_offsets: # we have a transposition\n",
    "        print(\"Info: The current nodes in the variant graph are not suitable for this token. Creating a new one. token_offset: \"+str(_token_offset)+\" floor: \"+str(_floor)+\" ceiling: \"+str(_ceiling)+\" sibling_token_offsets: \"+str(_sibling_nodes[0].token_offsets))\n",
    "        _new_node = Variant_Graph_Node(id = len(tm), token_offsets = (_token_offset,))\n",
    "        token_to_node_membership[_token_offset] = _new_node\n",
    "        tm[_token_offset] = _new_node\n",
    "        return\n",
    "    else:\n",
    "        # reuse an existing node for the token that we want to place\n",
    "        print(\"Info: We are reusing an existing node, which may or may not be between the allowed floor and ceiling: \" + \\\n",
    "              str(_token_offset) + \\\n",
    "              \" (\" + \\\n",
    "              token_array[_token_offset] + \\\n",
    "              \") floor: \"+str(_floor)+\" ceiling: \" + \\\n",
    "              str(_ceiling) + \\\n",
    "              \" sibling_token_offsets: \" + \\\n",
    "              str(_sibling_nodes[0].token_offsets))\n",
    "        _sibling_nodes[0].token_offsets = (*_sibling_nodes[0].token_offsets, _token_offset)\n",
    "        token_to_node_membership[_token_offset] = _sibling_nodes[0]\n",
    "        tm[_token_offset] = _sibling_nodes[0]\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def process_block(_block: Block, witnesses_filter=None): # void, modifies list of nodes in place\n",
    "    '''Process each occurence of each token in block'''\n",
    "\n",
    "    for _offset_within_block in range(0, _block.token_count): # each token in block\n",
    "        for _start_position in _block.all_start_positions:\n",
    "            if not witnesses_filter or token_membership_array[_start_position + _offset_within_block] in witnesses_filter:\n",
    "                process_token_instance(_start_position + _offset_within_block, [_sibling_offset + _offset_within_block for _sibling_offset in _block.all_start_positions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(token_count=2, start_position=4, end_position=6, all_start_positions=[2, 9, 18], witnesses={0, 1, 2}, witness_count=3, frequency=3)"
      ]
     },
     "execution_count": 25,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prioritized_blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: We are reusing an existing node, which may or may not be between the allowed floor and ceiling: 9 (and) floor: 6 ceiling: 13 sibling_token_offsets: (2,)\n",
      "Info: We are reusing an existing node, which may or may not be between the allowed floor and ceiling: 18 (and) floor: 13 ceiling: 24 sibling_token_offsets: (2, 9)\n",
      "Info: We are reusing an existing node, which may or may not be between the allowed floor and ceiling: 10 (the) floor: 9 ceiling: 13 sibling_token_offsets: (3,)\n",
      "Info: We are reusing an existing node, which may or may not be between the allowed floor and ceiling: 19 (the) floor: 18 ceiling: 24 sibling_token_offsets: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "process_block(prioritized_blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19))]"
      ]
     },
     "execution_count": 27,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 27,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_variant_graph_nodes\n",
    "token_to_node_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(token_count=2, start_position=18, end_position=20, all_start_positions=[3, 7, 14], witnesses={0, 1, 2}, witness_count=3, frequency=3)"
      ]
     },
     "execution_count": 28,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prioritized_blocks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: The current nodes in the variant graph are not suitable for this token. Creating a new one. token_offset: 7 floor: 6 ceiling: 9 sibling_token_offsets: (3, 10, 19)\n",
      "Info: We are reusing an existing node, which may or may not be between the allowed floor and ceiling: 8 (black) floor: 7 ceiling: 9 sibling_token_offsets: (4,)\n"
     ]
    }
   ],
   "source": [
    "process_block(prioritized_blocks[1], witnesses_filter=[0, 1]) # eek! transposition alert!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " Variant_Graph_Node(id=2, token_offsets=(4, 8))]"
      ]
     },
     "execution_count": 30,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " Variant_Graph_Node(id=2, token_offsets=(4, 8)),\n",
       " None,\n",
       " None,\n",
       " Variant_Graph_Node(id=8, token_offsets=(7,)),\n",
       " Variant_Graph_Node(id=2, token_offsets=(4, 8)),\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 30,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUG: incorrect results because of transposition\n",
    "to_variant_graph_nodes\n",
    "token_to_node_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'the'),),\n",
       " ((1, 'red'),),\n",
       " ((2, 'and'),),\n",
       " ((3, 'the'),),\n",
       " ((4, 'black'),),\n",
       " ((5, 'cat'),),\n",
       " ((6, ' #1 '),),\n",
       " ((7, 'the'),),\n",
       " ((8, 'black'),),\n",
       " ((9, 'and'),),\n",
       " ((10, 'the'),),\n",
       " ((11, 'red'),),\n",
       " ((12, 'cat'),),\n",
       " ((13, ' #2 '),),\n",
       " ((14, 'the'),),\n",
       " ((15, 'black'),),\n",
       " ((16, 'and'),),\n",
       " ((17, 'red'),),\n",
       " ((18, 'and'),),\n",
       " ((19, 'the'),),\n",
       " ((20, 'blue'),),\n",
       " ((21, 'and'),),\n",
       " ((22, 'green'),),\n",
       " ((23, 'cat'),)]"
      ]
     },
     "execution_count": 31,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(enumerate(token_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " 3: Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " 4: Variant_Graph_Node(id=2, token_offsets=(4, 8)),\n",
       " 6: None,\n",
       " 7: Variant_Graph_Node(id=8, token_offsets=(7,)),\n",
       " 8: Variant_Graph_Node(id=2, token_offsets=(4, 8)),\n",
       " 9: Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " 10: Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " 13: None,\n",
       " 18: Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " 19: Variant_Graph_Node(id=1, token_offsets=(3, 10, 19))}"
      ]
     },
     "execution_count": 32,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm\n",
    "# to_variant_graph_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None,\n",
       " Variant_Graph_Node(id=0, token_offsets=(2, 9, 18)),\n",
       " Variant_Graph_Node(id=1, token_offsets=(3, 10, 19)),\n",
       " Variant_Graph_Node(id=2, token_offsets=(4, 8)),\n",
       " Variant_Graph_Node(id=8, token_offsets=(7,))}"
      ]
     },
     "execution_count": 33,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tm.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ' #1 ',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ' #2 ',\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_membership_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "db7da630e4a16aeb63a0dfa3e66aabb53369373a",
      "text/plain": "<IPython.core.display.SVG object>"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get ready to visualize the decision tree in SVG\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "# TODO: use tm instead of to_variant_graph_nodes because reasons\n",
    "# TODO: add token offsets with witness identifiers to label\n",
    "\n",
    "# node id values must be strings for graphviz\n",
    "a = graphviz.Digraph(format=\"svg\")\n",
    "a.attr(rankdir = \"LR\")\n",
    "# create start and end nodes\n",
    "a.node('START')\n",
    "a.node('END')\n",
    "for vg_node in {x for x in tm.values() if x}:\n",
    "# create nodes\n",
    "    node_id = str(vg_node.id)\n",
    "    token_string = token_array[vg_node.token_offsets[0]]\n",
    "    a.node(node_id, label=(node_id + ':' + token_string))\n",
    "#create edges witness by witness\n",
    "all_token_offsets = [key for key in tm.keys() if tm[key]]\n",
    "for w in range(len(witnesses)): # w is the current witness offset in sequence of witnesses\n",
    "    first = 'START'\n",
    "    for key, value in tm.items():\n",
    "        if token_membership_array[key] == w:\n",
    "            # print('token ' + str(key) + ' is in witness ' + str(w)) # sanity check\n",
    "            second = value.id\n",
    "            edge_label = w\n",
    "            a.edge(str(first), str(second), label=str(edge_label))\n",
    "            first = second\n",
    "    a.edge(str(second), 'END', label=str(w))\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Block',\n",
       " 'In',\n",
       " 'InteractiveShell',\n",
       " 'List',\n",
       " 'Out',\n",
       " 'SVG',\n",
       " 'TreeMap',\n",
       " 'TreeSet',\n",
       " 'Tuple',\n",
       " 'Variant_Graph_Node',\n",
       " '_',\n",
       " '_16',\n",
       " '_20',\n",
       " '_25',\n",
       " '_27',\n",
       " '_28',\n",
       " '_30',\n",
       " '_31',\n",
       " '_32',\n",
       " '_33',\n",
       " '_34',\n",
       " '_35',\n",
       " '_5',\n",
       " '_7',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i25',\n",
       " '_i26',\n",
       " '_i27',\n",
       " '_i28',\n",
       " '_i29',\n",
       " '_i3',\n",
       " '_i30',\n",
       " '_i31',\n",
       " '_i32',\n",
       " '_i33',\n",
       " '_i34',\n",
       " '_i35',\n",
       " '_i36',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'a',\n",
       " 'all_token_offsets',\n",
       " 'bisect',\n",
       " 'block',\n",
       " 'blocks',\n",
       " 'create_blocks',\n",
       " 'create_lcp_array',\n",
       " 'create_suffix_array',\n",
       " 'create_token_array',\n",
       " 'dataclass',\n",
       " 'edge_label',\n",
       " 'exit',\n",
       " 'first',\n",
       " 'get_ipython',\n",
       " 'graphviz',\n",
       " 'key',\n",
       " 'lcp_array',\n",
       " 'matplotlib',\n",
       " 'node_id',\n",
       " 'prioritized_blocks',\n",
       " 'priority',\n",
       " 'process_block',\n",
       " 'process_token_instance',\n",
       " 'quit',\n",
       " 'second',\n",
       " 'sort_blocks_by_priority',\n",
       " 'suffix_array',\n",
       " 'tm',\n",
       " 'to_variant_graph_nodes',\n",
       " 'token_array',\n",
       " 'token_membership_array',\n",
       " 'token_string',\n",
       " 'token_to_node_membership',\n",
       " 'tokenize_witnesses',\n",
       " 'ts',\n",
       " 'value',\n",
       " 'vg_node',\n",
       " 'w',\n",
       " 'w0',\n",
       " 'w1',\n",
       " 'w2',\n",
       " 'witnesses']"
      ]
     },
     "execution_count": 36,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}