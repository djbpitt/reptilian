{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Store patterns efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Configure to show multiple value for development and debugging\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# two witnesses, with repetition and transposition\n",
    "\n",
    "# Original example, single leaf node\n",
    "w1 = '''the red and the black cat'''\n",
    "w2 = '''the black and the red cat'''\n",
    "\n",
    "# Adjacent transposition\n",
    "# w1 = '''the red striped cat'''\n",
    "# w2 = '''the striped red cat'''\n",
    "\n",
    "# Two leaf nodes\n",
    "# w1 = '''cat red black'''\n",
    "# w2 = '''cat black red'''\n",
    "\n",
    "# Branches meet in the middle at koala and then split again, with two leaf nodes\n",
    "# w1 = \"\"\"cat red black koala brown gray\"\"\"\n",
    "# w2 = \"\"\"cat black red koala gray brown\"\"\"\n",
    "\n",
    "# Two split and rejoin\n",
    "# w1 = '''the gray koala'''\n",
    "# w2 = '''the brown koala'''\n",
    "\n",
    "# medium example\n",
    "# w1 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ much more from each other, than do the individuals of any one\n",
    "# species or variety in a state of nature.'''\n",
    "# w2 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ more from each other than do the individuals of any one\n",
    "# species or variety in a state of nature.'''\n",
    "\n",
    "# Larger example\n",
    "# w1 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ much more from each other, than do the individuals of any one\n",
    "# species or variety in a state of nature. When we reflect on the vast diversity of the\n",
    "# plants and animals which have been cultivated, and which have varied during all ages\n",
    "# under the most different climates and treatment, I think we are driven to conclude that\n",
    "# this greater variability is simply due to our domestic productions having been raised\n",
    "# under conditions of life not so uniform as, and somewhat different from, those to which\n",
    "# the parent-species have been exposed under nature. There is, also, I think, some\n",
    "# probability in the view propounded by Andrew Knight, that this variability may be partly\n",
    "# connected with excess of food. It seems pretty clear that organic beings must be exposed\n",
    "# during several generations to the new conditions of life to cause any appreciable amount\n",
    "# of variation; and that when the organisation has once begun to vary, it generally\n",
    "# continues to vary for many generations. No case is on record of a variable being ceasing\n",
    "# to be variable under cultivation. Our oldest cultivated plants, such as wheat, still\n",
    "# often yield new varieties: our oldest domesticated animals are still capable of rapid\n",
    "# improvement or modification.'''\n",
    "# w2 = '''WHEN we look to the individuals of the same variety or sub-variety of\n",
    "# our older cultivated plants and animals, one of the first points which strikes us, is,\n",
    "# that they generally differ more from each other than do the individuals of any one\n",
    "# species or variety in a state of nature. When we reflect on the vast diversity of the\n",
    "# plants and animals which have been cultivated, and which have varied during all ages\n",
    "# under the most different climates and treatment, I think we are driven to conclude that\n",
    "# this great variability is simply due to our domestic productions having been raised\n",
    "# under conditions of life not so uniform as, and somewhat different from, those to which\n",
    "# the parent-species have been exposed under nature. There is also, I think, some\n",
    "# probability in the view propounded by Andrew Knight, that this variability may be partly\n",
    "# connected with excess of food. It seems pretty clear that organic beings must be exposed\n",
    "# during several generations to the new conditions of life to cause any appreciable amount\n",
    "# of variation; and that when the organisation has once begun to vary, it generally\n",
    "# continues to vary for many generations. No case is on record of a variable being ceasing\n",
    "# to be variable under cultivation. Our oldest cultivated plants, such as wheat, still\n",
    "# often yield new varieties: our oldest domesticated animals are still capable of rapid\n",
    "# improvement or modification'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Work plan\n",
    "\n",
    "1. Create token array (Python **list**)\n",
    "1. Create suffix array\n",
    "1. Create LCP (**longest common prefix**) array\n",
    "1. Calculate LCP intervals\n",
    "1. Create patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Construct list of ngrams shared by witnesses\n",
    "\n",
    "Find ngrams and positions in witnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tokenize witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def tokenize_witnesses(w1_string, w2_string):\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    w1_tokens = w1.split()\n",
    "    w2_tokens = w2.split()\n",
    "    witnesses = [w1_tokens, w2_tokens]\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "witnesses = tokenize_witnesses(w1, w2)\n",
    "# print(witnesses) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create token array\n",
    "# All tokens from both witnesses in a single list, with a separator (\" # \") between witnesses\n",
    "token_array = []\n",
    "token_array.extend(witnesses[0])\n",
    "token_array.append(\" # \")\n",
    "token_array.extend(witnesses[1])\n",
    "# [(index, value) for index,value in enumerate(token_array)] # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# determine suffixes of token array\n",
    "# output with print() is diagnostic\n",
    "# for index, token in enumerate(token_array):\n",
    "#     suffix = token_array[index:]\n",
    "#     print(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# suffix array is sorted alphabetically\n",
    "# tuples of list of tokens (suffix) and position in original list of tokens\n",
    "# TODO: less naive implementation\n",
    "suffixes = []\n",
    "for index, token in enumerate(token_array):\n",
    "    suffix = token_array[index:]\n",
    "    suffixes.append((suffix, index))\n",
    "suffixes.sort() # sort in place\n",
    "# suffixes # take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that suffixes that start at position 2 and 9 both start with \"and the\", which tells us that:\n",
    "\n",
    "1. There is a repeated suffix \"and the\"\n",
    "1. \"and\" appears without \"the\"\n",
    "\n",
    "Ergo, we don't need a unigram \"and\".\n",
    "\n",
    "Similarly, other ngrams occur repeatedly: \"the red\" twice, \"the\" four times, etc. Occurrences of \"and\" are easy because they are only \"and the\", while \"the\" occurs in different contexts, e.g., twice in \"the red\", twice in \"the black\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 9, 8, 4, 12, 5, 1, 11, 7, 3, 0, 10]"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suffix array is list of offsets of start positions of suffixes sorted alphabetically\n",
    "# Suffix array is economical because it is equal to the sum of the lengths of witnesses plus the number of witnesses - 1 (for the separators)\n",
    "suffix_array = []\n",
    "for suffix, index in suffixes:\n",
    "    suffix_array.append(index)\n",
    "suffix_array # take a look [80:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# compute LCP array\n",
    "# sequential pairs of values in suffix array, which are two offsets in the sorted suffixes\n",
    "# for i in range(0, len(suffix_array) - 1):\n",
    "#     print (suffix_array[i:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# compute LCP array, which is a sequence of integers representing the number of tokens shared by consecutive alphabetically sorted suffixes\n",
    "# sequential pairs of values in suffix array, which are two offsets in the sorted suffixes\n",
    "# TODO: we now need the longest prefix\n",
    "lcp_array = [0]\n",
    "for i in range(0, len(suffix_array) - 1): # for each pair of suffixes, retrieve list of tokens starting at that position\n",
    "    pair = suffix_array[i:i+2] # for each pair of suffixes\n",
    "    suffix_1 = token_array[pair[0]:] # tokens starting at first position\n",
    "    suffix_2 = token_array[pair[1]:] # tokens starting at second position\n",
    "    # print(suffix_1, suffix_2) # diagnostic: verify that they're paired correctly\n",
    "    lcp_value = next(filter(lambda t: t[1][0] != t[1][1], enumerate(zip(suffix_1, suffix_2))), min(len(suffix_1), len(suffix_2))) # pair the tokens up by position, return (number of matches, first non-match)\n",
    "    lcp_array.append(lcp_value[0] if type(lcp_value) == tuple else lcp_value) # most are tuples, but some are just an integer\n",
    "# lcp_array # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Use LCP array to calculate patterns (efficiently)\n",
    "# Values in LCP array represent lengths of matching ngrams\n",
    "#   e.g., the three values of 2 are \"and the\", \"the black\", \"the red\"\n",
    "#   \"the\" is harder: unigram appears four times, plus \"the black\" and \"the red\"\n",
    "#\n",
    "# Of interest:\n",
    "#   1. 0 means that whatever follows will have nothing in common with it\n",
    "#   2. Repetition of same number (doesn't happen here) means same pattern\n",
    "#   3. Consecutive non-zero values identify how much of the pattern they have in common, \n",
    "#      e.g., end goes from \"the black\" (2) to \"the\" (1) to \"the red\" (2)\n",
    "#         Counts are always +1, so there must be two instances of \"the red\", two of \"the black\", \n",
    "#      and four of \"the\" (two unigrams and two embedded in \"the red\" and \"the black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create Block dataclass\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not in token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    # witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (2, 2), (3, 0), (4, 1), (5, 0), (6, 1), (7, 0), (8, 1), (9, 0), (10, 2), (11, 1), (12, 2)]\n"
     ]
    }
   ],
   "source": [
    "# create blocks from the lcp array\n",
    "from collections import deque # faster append and pop than list\n",
    "blocks = []\n",
    "open_block_stack = deque()\n",
    "print(list(enumerate(lcp_array)))\n",
    "for offset, lcp in enumerate(lcp_array):\n",
    "    # three situations: next one is same value, higher that last, or lower than last\n",
    "    # if same value: same pattern\n",
    "    # if higher or lower, new pattern (may overlap with previous, unless one or the other value is 0)\n",
    "    if offset == 0: # skip the first one, which is a transition from a fake start value\n",
    "        continue # resume loop with next item in lcp array\n",
    "    elif lcp == lcp_array[offset - 1]:\n",
    "        pass # same pattern (happens with repetition), so do nothing\n",
    "    elif lcp > lcp_array[offset - 1]: # new prefix is longer than previous one, so start new pattern\n",
    "        # can fill in end_position and frequency only when we encounter a shorter value in the LCP array\n",
    "        # start_position is number of patterns that are the same \n",
    "        open_block_stack.append(Block(token_count = lcp, start_position = offset - 1, end_position = -1, all_start_positions = [], frequency = -1, how_created = 1))\n",
    "    else: # new prefix is shorter than previous one, so:\n",
    "            # 1. close open blocks with higher values\n",
    "            # 2. do something else\n",
    "        while open_block_stack and open_block_stack[-1].token_count > lcp: # if an open block is longer than the current length, pop and close it\n",
    "            block_being_modified = open_block_stack.pop()\n",
    "            block_being_modified.end_position = offset - 1\n",
    "            block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "            blocks.append(block_being_modified)\n",
    "        if lcp == 0: # if 0, stop after clearing the stack\n",
    "            continue\n",
    "        # not 0, and: either 1) stack is empty, or 2) top has an lcp value equal to current, or 3) an lcp value less than current\n",
    "        if not open_block_stack: # stack is empty, so hidden shorter block; create new block that starts at start position of last closed block\n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = blocks[-1].start_position, end_position = -1, all_start_positions = [], frequency = -1, how_created = 2))\n",
    "        elif open_block_stack[-1].token_count == lcp: # stack has value same length as current, so do nothing\n",
    "            pass\n",
    "        else: # stack has value less than current, so extends shorter block; create new block, but where?\n",
    "            # TODO: why does this work?\n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = blocks[-1].start_position, end_position = -1, all_start_positions = [], frequency = -1, how_created = 3))\n",
    "            # print(open_block_stack)\n",
    "            # print(blocks[-1])\n",
    "        # if equal to current length, do nothing; it's open, but we can't close it yet\n",
    "while open_block_stack: # pop anything left in open_block_stack\n",
    "    block_being_modified = open_block_stack.pop()\n",
    "    block_being_modified.end_position = len(lcp_array) - 1\n",
    "    block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "    blocks.append(block_being_modified)\n",
    "\n",
    "# for block in blocks: # diagnostic\n",
    "#     # suffix_array is offsets of start positions of suffixes (in alphabetical order) into token_array\n",
    "#     # block.start_position is offset of suffix into suffix_array, which is one less than offset into lcp_array\n",
    "#     # we slice token_array:\n",
    "#     #   start of slice is offset into token_array (by way of suffix_array by way of lcp_array) of first ngram token\n",
    "#     #   length of slice adds the length of the lcp interval (part of lcp array that represents pattern, which = length of ngram) to the start of the slice\n",
    "#     print(block)\n",
    "#     print(token_array[suffix_array[block.start_position]:suffix_array[block.start_position] + block.token_count])\n",
    "if open_block_stack: # diagnostic; should be empty\n",
    "    print('ERROR: open_block_stack should be empty')\n",
    "    print(open_block_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Block 1 starts at suffix_array position 1 and ends at 2. This means that tokens 2 (position 1) and 9 (position 2) are the (only) two start positions of the two instances of the bigram 'and the' in the token_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# If we enumerate the token array and look at tokens 2 and 9, we see the two instances of the bigram 'and the'.\n",
    "\n",
    "# [(index, value) for index,value in enumerate(token_array)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we iterate over the blocks, for each block we can get its start position(s) in the token array and the ngrams themselves (not in token-array order, but we can sort later). We can then sort them by token position to get a list of ngrams for both witnesses in token order (which we need because we do our alignment from left to right). The token array doesn't distinguish the witnesses explicitly, but if we subtract the length of w1 from a position, add 1, and get a positive number, we're in w2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# add all_start_tokens property value to blocks (initialized as empty list)\n",
    "for index, block in enumerate(blocks): # compute all start tokens just once for each block\n",
    "    _all_start_tokens = suffix_array[block.start_position: block.end_position + 1]\n",
    "    _all_start_tokens.sort()\n",
    "    block.all_start_positions = _all_start_tokens # save it to the block\n",
    "\n",
    "# blocks # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# use blocks to create separate list of matches\n",
    "#\n",
    "# block contain all_start_positions property, which lists ... wait for it ... all start positions of ngram\n",
    "# we can get the length of w1 from witnesses[0], so we can tell which start positions are in w1, and the others are in w2\n",
    "# we create all matches by pairing each w1 start position with each w2 start position in the same block\n",
    "#\n",
    "from itertools import product as cartesian_product # all combinations of members in two lists\n",
    "from typing import List\n",
    "w1_length = len(witnesses[0])\n",
    "#\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Match:\n",
    "    '''unique combination of ngram length and positions in the two witnesses\n",
    "\n",
    "    In case an ngram repeats in a witness, create a separate match for each combination of each instance\n",
    "    E.g., if an ngram occurs twice in each witness, create four matches\n",
    "    '''\n",
    "    __slots__ = ['ngram', 'token_count', 'w1_start_position', 'w1_end_position', 'w2_start_position', 'w2_end_position']\n",
    "    ngram: str # combine tokens into single string to make it hashable\n",
    "    token_count: int\n",
    "    w1_start_position: int\n",
    "    w1_end_position: int\n",
    "    w2_start_position: int\n",
    "    w2_end_position: int\n",
    "#\n",
    "matches = []\n",
    "for block in blocks:\n",
    "    _ngram = \" \".join(token_array[block.all_start_positions[0]:block.all_start_positions[0] + block.token_count])\n",
    "    start_positions = block.all_start_positions\n",
    "    w1_start_positions = list(filter(lambda x: x < w1_length, start_positions))\n",
    "    w2_start_positions = list(filter(lambda x: x not in w1_start_positions, start_positions))\n",
    "    match_pairs = cartesian_product(w1_start_positions, w2_start_positions)\n",
    "    matches.extend([Match(ngram = _ngram, \\\n",
    "                          token_count = block.token_count, \\\n",
    "                          w1_start_position = match_pair[0], \\\n",
    "                          w1_end_position = match_pair[0] + block.token_count - 1, \\\n",
    "                          w2_start_position = match_pair[1], \\\n",
    "                          w2_end_position = match_pair[1] + block.token_count - 1) for match_pair in match_pairs])\n",
    "# matches # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (cf cell #13 in decision_graph_3 notebook)\n",
    "# sort matches by witness A\n",
    "# column labels, ordered by w1 pos, then w2 pos, then ngram length (long to short)\n",
    "# patterns_A = [] # will need to be sorted after all values have been added\n",
    "# for k, v in alignments.items():\n",
    "#     for ngram in v: # add instance of dataclass Pattern for each ngram at A,B positions\n",
    "#         current_ngram_length = ngram_lengths[ngram]\n",
    "#         patterns_A.append(Pattern(k[0], k[1], ngram, current_ngram_length, k[0] + current_ngram_length - 1, k[1] + current_ngram_length - 1))\n",
    "patterns_A = sorted(matches, key=lambda x: (x.w1_start_position, x.w2_start_position, -x.token_count))\n",
    "patterns_A.insert(0, Match(w1_start_position = -1, \\\n",
    "                           w1_end_position = -1, \\\n",
    "                           w2_start_position = -1, \\\n",
    "                           w2_end_position = -1, \\\n",
    "                           ngram = 'ROOT', token_count = 1))\n",
    "\n",
    "# patterns_A # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# (cf cell #14 in decision_graph_3 notebook)\n",
    "# sort also by witness B\n",
    "patterns_B = sorted(patterns_A, key=lambda x: (x.w2_start_position, x.w1_start_position, -x.token_count))\n",
    "\n",
    "# patterns_B # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# find and save all diagonal patterns in list\n",
    "# TODO: can we use a copy constructor to avoid having to copy all of the properties individually?\n",
    "diagonal_patterns = []\n",
    "for table_offset_A, pattern in enumerate(patterns_A): # find table offset for each A pattern\n",
    "    if patterns_B[table_offset_A] is pattern: # is the pattern at the same offset in B the same object?\n",
    "        diagonal_patterns.append(pattern) # if so, it's a diagonal\n",
    "# diagonal_patterns # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Node objects contain unique id values plus pattern (not token) coordinates for both witnesses\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Decision_Graph_Node:\n",
    "    __slots__ = ['id', 'pattern_coordinate_witness_A', 'pattern_coordinate_witness_B']\n",
    "    id: int\n",
    "    pattern_coordinate_witness_A: int # offset into list of patterns (not the same as token coordinate)\n",
    "    pattern_coordinate_witness_B: int\n",
    "    # score: int # count of aligned tokens, or should that be on the edges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# First create root node\n",
    "root = Decision_Graph_Node(0, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def find_diagonal_pattern_for_node(current_node : Decision_Graph_Node):\n",
    "    # go over the diagional patterns and find the first pattern that begins after the end token of the current pattern.\n",
    "    current_pattern_A = patterns_A[current_node.pattern_coordinate_witness_A] # offset into list of patterns\n",
    "    current_pattern_B = patterns_B[current_node.pattern_coordinate_witness_B]\n",
    "\n",
    "    next_diagonal_pattern = next(filter(lambda dp: dp.w1_start_position > current_pattern_A.w1_end_position and dp.w2_start_position > current_pattern_B.w2_end_position, diagonal_patterns), None)\n",
    "    return next_diagonal_pattern\n",
    "\n",
    "# next_pattern_in_both = find_diagonal_pattern_for_node(root)\n",
    "# next_pattern_in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from typing import Set\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Decision_Graph_Edge:\n",
    "    __slots__ = ['source', 'target']\n",
    "    source: int\n",
    "    target: int\n",
    "\n",
    "@dataclass\n",
    "class Decision_Graph: # set of nodes\n",
    "    __slots__ = ['nodes', 'edges']\n",
    "    nodes: Set[Decision_Graph_Node]\n",
    "    edges: Set[Decision_Graph_Edge]\n",
    "\n",
    "decision_graph_nodes = {root} # set of nodes\n",
    "decision_graph_edges = set()\n",
    "decision_graph = Decision_Graph(decision_graph_nodes, decision_graph_edges)\n",
    "# decision_graph # take a look; so far just one node (root) and no edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# stable lookup for index position of pattern in witness-specific lists of patterns\n",
    "patterns_A_position_by_value = {val: index for index, val in enumerate(patterns_A)} # retrieve offset in witness-specific list of patterns by pattern\n",
    "patterns_B_position_by_value = {val: index for index, val in enumerate(patterns_B)} # retrieve offset in witness-specific list of patterns by pattern\n",
    "# patterns_A_position_by_value # WTF: take a look ... to know where to start computing diagonal ... perhaps???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Dictionary to check whether node has already been added for a pattern:\n",
    "#\n",
    "# If the node has already been added, don't add a new node, but retrieve the node id to add a new edge\n",
    "# If the node has not already been added, add it, along with an edge\n",
    "\n",
    "node_finder = {} # dictionary; key is pattern coordinate A, value is node id\n",
    "node_finder[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Recursive function to add all nodes to graph in depth-first-order\n",
    "# Starts from root, with has already been added manually\n",
    "from typing import List\n",
    "# Ror each node we create at most three children plus edges (or just edges if a child node already exists):\n",
    "#\n",
    "#   1. Next pattern in Witness A. Look for the same pattern in Witness B. Should not be a diagional, see below.\n",
    "#   2. Next pattern in Witness B. Look for the same pattern in Witness A. Should not be a diagional, see below.\n",
    "#   3. Next pattern on a diagional. Could result in a skip of the patterns mentioned above.\n",
    "\n",
    "# function to identify and add all children of specified node\n",
    "# TODO: do not create child if it already exists\n",
    "def add_children(parent: Decision_Graph_Node): # find and create children of input (parent), returns nothing\n",
    "    # BUG: we currently exclude diagonals from closest in A and B incorrectly; diagonals are from the parent at each step, and not invariably from the root\n",
    "    # find closest in A and B that are not also diagonals and create node plus edge (or just edge, if node has already been added)\n",
    "    # find closest diagonal and create node (or just edge, if node has already been added)\n",
    "    pattern_A = patterns_A[parent.pattern_coordinate_witness_A] # offset of parent Pattern in list of patterns ordered by witness A\n",
    "    token_coordinate_witness_A = pattern_A.w1_end_position # ending token position of parent in witness A\n",
    "    token_coordinate_witness_B = pattern_A.w2_end_position\n",
    "    next_A_pattern = next(filter(lambda x : x.w1_start_position > token_coordinate_witness_A and x.w2_start_position > token_coordinate_witness_B \\\n",
    "                               and x not in diagonal_patterns, patterns_A[parent.pattern_coordinate_witness_A:]), None) # first pattern after parent token position\n",
    "    next_B_pattern = next(filter(lambda x : x.w1_start_position > token_coordinate_witness_A and x.w2_start_position > token_coordinate_witness_B \\\n",
    "                               and x not in diagonal_patterns, patterns_B[parent.pattern_coordinate_witness_B:]), None) # first pattern after parent token position\n",
    "    next_diag_pattern = find_diagonal_pattern_for_node(parent)\n",
    "\n",
    "    if next_A_pattern:\n",
    "        next_A_pattern_index_A = patterns_A_position_by_value[next_A_pattern]\n",
    "        next_A_pattern_index_B = patterns_B_position_by_value[next_A_pattern]\n",
    "\n",
    "        if not next_A_pattern_index_A in node_finder: # add new node only if it hasn't already been added\n",
    "            new_node_id = len(decision_graph.nodes)\n",
    "            first_node = Decision_Graph_Node(new_node_id, next_A_pattern_index_A, next_A_pattern_index_B) # create new node\n",
    "            decision_graph_nodes.add(first_node)\n",
    "            node_finder[next_A_pattern_index_A] = new_node_id # and update node finder\n",
    "            edge1 = Decision_Graph_Edge(parent.id, first_node.id) # add edge to newly created node\n",
    "            decision_graph_edges.add(edge1)\n",
    "            add_children(first_node)\n",
    "        else:\n",
    "            edge1 = Decision_Graph_Edge(parent.id, node_finder[next_A_pattern_index_A]) # add new edge to pre-existing node\n",
    "            decision_graph_edges.add(edge1)\n",
    "\n",
    "    if next_B_pattern:\n",
    "        next_B_pattern_index_A = patterns_A_position_by_value[next_B_pattern]\n",
    "        next_B_pattern_index_B = patterns_B_position_by_value[next_B_pattern]\n",
    "\n",
    "        if not next_B_pattern_index_A in node_finder:\n",
    "            new_node_id = len(decision_graph.nodes)\n",
    "            second_node = Decision_Graph_Node(new_node_id, next_B_pattern_index_A, next_B_pattern_index_B)\n",
    "            decision_graph_nodes.add(second_node)\n",
    "            node_finder[next_B_pattern_index_A] = new_node_id\n",
    "            edge2 = Decision_Graph_Edge(parent.id, second_node.id)\n",
    "            decision_graph_edges.add(edge2)\n",
    "            add_children(second_node)\n",
    "        else:\n",
    "            edge2 = Decision_Graph_Edge(parent.id, node_finder[next_B_pattern_index_A])\n",
    "            decision_graph_edges.add(edge2)\n",
    "\n",
    "    if next_diag_pattern:\n",
    "        next_diag_pattern_index_A = patterns_A_position_by_value[next_diag_pattern]\n",
    "        next_diag_pattern_index_B = patterns_B_position_by_value[next_diag_pattern]\n",
    "\n",
    "        if not next_diag_pattern_index_A in node_finder:\n",
    "            new_node_id = len(decision_graph.nodes)\n",
    "            third_node = Decision_Graph_Node(new_node_id, next_diag_pattern_index_A, next_diag_pattern_index_B)\n",
    "            decision_graph_nodes.add(third_node)\n",
    "            node_finder[next_diag_pattern_index_A] = new_node_id\n",
    "            edge3 = Decision_Graph_Edge(parent.id, third_node.id)\n",
    "            decision_graph_edges.add(edge3)\n",
    "            add_children(third_node)\n",
    "        else:\n",
    "            edge3 = Decision_Graph_Edge(parent.id, node_finder[next_diag_pattern_index_A])\n",
    "            decision_graph_edges.add(edge3)\n",
    "\n",
    "# Populate the graph (all nodes and edges)\n",
    "add_children(root) # builds entire decision graph recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# decision_graph # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"412pt\" height=\"260pt\" viewBox=\"0.00 0.00 412.04 260.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-256 408.04,-256 408.04,4 -4,4\"/>\n<!-- 2 -->\n<g id=\"node1\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"200.3\" cy=\"-18\" rx=\"33.6\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"200.3\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2:cat</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"53.3\" cy=\"-162\" rx=\"53.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"53.3\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">1:the red</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M41.38,-144.34C29.28,-125.18 14.41,-93.6 30.3,-72 58.95,-33.06 116.03,-21.91 156.12,-19.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"156.38,-22.63 166.17,-18.59 156,-15.64 156.38,-22.63\"/>\n</g>\n<!-- 0 -->\n<g id=\"node3\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"200.3\" cy=\"-234\" rx=\"46.59\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"200.3\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">0:ROOT</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0-&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M171.95,-219.5C149.24,-208.69 117.14,-193.4 92.05,-181.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"93.34,-178.19 82.81,-177.06 90.33,-184.51 93.34,-178.19\"/>\n</g>\n<!-- 4 -->\n<g id=\"node4\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"200.3\" cy=\"-162\" rx=\"34.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"200.3\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">4:the</text>\n</g>\n<!-- 0&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>0-&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.3,-215.7C200.3,-207.98 200.3,-198.71 200.3,-190.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203.8,-190.1 200.3,-180.1 196.8,-190.1 203.8,-190.1\"/>\n</g>\n<!-- 3 -->\n<g id=\"node5\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"342.3\" cy=\"-162\" rx=\"61.99\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"342.3\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">3:the black</text>\n</g>\n<!-- 0&#45;&gt;3 -->\n<g id=\"edge5\" class=\"edge\">\n<title>0-&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M228.01,-219.34C249.39,-208.8 279.23,-194.09 303.06,-182.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"304.77,-185.4 312.19,-177.84 301.68,-179.12 304.77,-185.4\"/>\n</g>\n<!-- 6 -->\n<g id=\"node6\" class=\"node\">\n<title>6</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"83.3\" cy=\"-90\" rx=\"44.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"83.3\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">6:black</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4-&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M178.56,-148C161.03,-137.51 136.08,-122.58 116.11,-110.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"117.71,-107.51 107.33,-105.38 114.12,-113.52 117.71,-107.51\"/>\n</g>\n<!-- 7 -->\n<g id=\"node7\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"200.3\" cy=\"-90\" rx=\"54.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"200.3\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">7:and the</text>\n</g>\n<!-- 4&#45;&gt;7 -->\n<g id=\"edge10\" class=\"edge\">\n<title>4-&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.3,-143.7C200.3,-135.98 200.3,-126.71 200.3,-118.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203.8,-118.1 200.3,-108.1 196.8,-118.1 203.8,-118.1\"/>\n</g>\n<!-- 5 -->\n<g id=\"node8\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"308.3\" cy=\"-90\" rx=\"35.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"308.3\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">5:red</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge7\" class=\"edge\">\n<title>4-&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M221.12,-147.5C237.45,-136.92 260.39,-122.05 278.64,-110.22\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"280.68,-113.07 287.17,-104.69 276.87,-107.2 280.68,-113.07\"/>\n</g>\n<!-- 3&#45;&gt;2 -->\n<g id=\"edge9\" class=\"edge\">\n<title>3-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M350.53,-143.88C358.5,-124.59 367.33,-93.17 352.3,-72 327.99,-37.78 279.89,-25.47 244.28,-21.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.45,-17.64 234.14,-20.08 243.72,-24.6 244.45,-17.64\"/>\n</g>\n<!-- 6&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>6-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.25,-74.67C125.38,-63.82 150.39,-48.86 169.92,-37.18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"171.95,-40.04 178.73,-31.9 168.35,-34.03 171.95,-40.04\"/>\n</g>\n<!-- 7&#45;&gt;2 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M200.3,-71.7C200.3,-63.98 200.3,-54.71 200.3,-46.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"203.8,-46.1 200.3,-36.1 196.8,-46.1 203.8,-46.1\"/>\n</g>\n<!-- 5&#45;&gt;2 -->\n<g id=\"edge11\" class=\"edge\">\n<title>5-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.47,-75.5C271.04,-64.85 247.91,-49.86 229.6,-38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"231.35,-34.96 221.05,-32.46 227.54,-40.83 231.35,-34.96\"/>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 28,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize decision graph\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "# node id values must be strings for graphviz\n",
    "a = graphviz.Digraph(format=\"svg\")\n",
    "for item in decision_graph.nodes: # set of Decision_Graph_Node objects, call it \"item\" to avoid overusing the keyword \"node\"\n",
    "    a.node(str(item.id), label=str(item.id) + ':' + patterns_A[item.pattern_coordinate_witness_A].ngram)\n",
    "for item in decision_graph.edges:\n",
    "    a.edge(str(item.source), str(item.target))\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Non-OO Python implementation of Kahn's topological sort algorithm\n",
    "\n",
    "https://algocoding.wordpress.com/2015/04/05/topological-sorting-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# convert set of edges to adjacency list, preparatory to topological sorting\n",
    "from collections import defaultdict\n",
    "adjacency_list = defaultdict(list) # key is source, value is list of targets\n",
    "for edge in decision_graph.edges:\n",
    "    adjacency_list[edge.source].append(edge.target)\n",
    "    if edge.target not in adjacency_list: # adjacency list needs a key for every node, even if \n",
    "        adjacency_list[edge.target] = []\n",
    "# adjacency_list # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# copied from https://algocoding.wordpress.com/2015/04/05/topological-sorting-python/\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def kahn_topsort(graph):\n",
    "    in_degree = { u : 0 for u in graph }     # determine in-degree\n",
    "    for u in graph:                          # of each node\n",
    "        for v in graph[u]:\n",
    "            in_degree[v] += 1\n",
    "\n",
    "    Q = deque()                 # collect nodes with zero in-degree\n",
    "    for u in in_degree:\n",
    "        if in_degree[u] == 0:\n",
    "            Q.appendleft(u)\n",
    "\n",
    "    L = []     # list for order of nodes\n",
    "\n",
    "    while Q:\n",
    "        u = Q.pop()          # choose node of zero in-degree\n",
    "        L.append(u)          # and 'remove' it from graph\n",
    "        for v in graph[u]:\n",
    "            in_degree[v] -= 1\n",
    "            if in_degree[v] == 0:\n",
    "                Q.appendleft(v)\n",
    "\n",
    "    if len(L) == len(graph):\n",
    "        return L\n",
    "    else:                    # if there is a cycle,\n",
    "        return []            # then return an empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "order = kahn_topsort(adjacency_list)\n",
    "# print(order) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# dictionary to retrieve parent by child\n",
    "parent_by_child = defaultdict(list)\n",
    "for k, v in adjacency_list.items():\n",
    "    for node in v:\n",
    "        parent_by_child[node].append(k)\n",
    "# parent_by_child # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# traverse in topological order to find best path\n",
    "# for each node, track cumulative score (token score plus best token count of parents) and best parent\n",
    "# may be ties for best score / parent\n",
    "\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Score:\n",
    "    __slots__ = ['total', 'parent']\n",
    "    total: int # best cumulative total\n",
    "    parent: List[int] # node id\n",
    "\n",
    "scores = {}\n",
    "scores[0] = Score(total = 0, parent = []) # add root manually\n",
    "\n",
    "# scores # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Dictionary to retrieve node by id\n",
    "# Sigh ... another dictionary!\n",
    "node_by_id = {}\n",
    "for node in decision_graph.nodes:\n",
    "    node_by_id[node.id] = node\n",
    "# node_by_id # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# score all nodes, keep track of best cumulative score\n",
    "best_score = 0 # best cumulative score for all nodes\n",
    "for node in order[1:]: # skip root; already done\n",
    "    local_ngram_count = patterns_A[node_by_id[node].pattern_coordinate_witness_A].token_count # ngram token count for current node\n",
    "    parents = parent_by_child[node] # all parents\n",
    "    best_parent_score = 0\n",
    "    best_parents = [] # parents with highest score\n",
    "    for parent in parents:\n",
    "        current_parent_score = scores[parent].total\n",
    "        if current_parent_score > best_parent_score: # replace old best score and parent pointers\n",
    "            best_parent_score = current_parent_score\n",
    "            best_parents = [parent]\n",
    "        elif current_parent_score == best_parent_score: # tie for best parent, so add to list\n",
    "            best_parents.append(parent)\n",
    "    scores[node] = Score(total = best_parent_score + local_ngram_count, parent = best_parents)\n",
    "    if scores[node].total > best_score: # if new score is highest of all so far, update best_score\n",
    "        best_score = scores[node].total\n",
    "# print(scores) # take a look at all scores,\n",
    "# print(best_score) # and at best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# there will be exactly one or exactly two best leaves, return as list\n",
    "best_leaves = [k for k,v in scores.items() if v.total == best_score] # id(s) of leaves with best cumulative score\n",
    "# best_leaves # take a look at leaves with best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Work plan\n",
    "\n",
    "1. Traverse backward from best leaf (choose one) to find best path (choose one)\n",
    "1. Build and render variant graph (no transpositions yet)\n",
    "1. Find transpositions and add to variant graph; visualize variant graph with transpositions\n",
    "1. Visualize alignment as table\n",
    "1. Perhaps: Find all best paths, instead of just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# function to traverse backward from best leaf to find best path\n",
    "# arbitrarily chooses one path in case of ties\n",
    "def find_best_path(node: int) -> list: # start at leaf\n",
    "    path = find_next_path_step([node]) # beginning of path as list, end of list is most recent step\n",
    "    return path\n",
    "\n",
    "def find_next_path_step(_path:list) -> list:\n",
    "    if scores[_path[-1]].parent: # does last step (end of list) have a parent?\n",
    "        next_step = scores[_path[-1]].parent[0] # if so, add first of its parents to end list\n",
    "        _path.append(next_step)\n",
    "        return find_next_path_step(_path) # now look for the next step up the chain\n",
    "    else: # if no parent, we're at the root\n",
    "        return _path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "# find a best path from root to leaf\n",
    "best_path = find_best_path(best_leaves[0]) # arbitrarily choose first if there are two best leaves\n",
    "best_path.reverse() # built backward from leaf, so reverse in place (no return)\n",
    "print(best_path) # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create variant graph\n",
    "# we have a list of aligned ngrams, but not of aligned tokens, so we need to look up the tokens\n",
    "@dataclass\n",
    "class Variant_Graph_Node: # text can be retrieved from token, since no normalization (yet)\n",
    "    id: int\n",
    "    token_positions: defaultdict(int)\n",
    "\n",
    "@dataclass\n",
    "class Variant_Graph_Edge:\n",
    "    source: int\n",
    "    target: int\n",
    "\n",
    "@dataclass\n",
    "class Variant_Graph:\n",
    "    nodes: Set[Variant_Graph_Node]\n",
    "    edges: Set[Variant_Graph_Edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variant_Graph_Node(id=0, token_positions={'A': -1, 'B': -1}),\n",
       " Variant_Graph_Node(id=1, token_positions={'A': 6, 'B': 6}),\n",
       " Variant_Graph_Node(id=2, token_positions={'A': 0}),\n",
       " Variant_Graph_Node(id=3, token_positions={'A': 1}),\n",
       " Variant_Graph_Node(id=4, token_positions={'A': 2}),\n",
       " Variant_Graph_Node(id=5, token_positions={'A': 3}),\n",
       " Variant_Graph_Node(id=6, token_positions={'A': 4}),\n",
       " Variant_Graph_Node(id=7, token_positions={'A': 5})]"
      ]
     },
     "execution_count": 40,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the graph and the start and end nodes\n",
    "variant_graph = Variant_Graph(nodes=[], edges=[])\n",
    "max_token_pos = max({len(witness) for witness in witnesses}) # 1 greater than position of last token in longest witness\n",
    "variant_graph_start = Variant_Graph_Node(id=0, token_positions={'A': -1, 'B': -1})\n",
    "variant_graph_end = Variant_Graph_Node(id=1, token_positions={'A': max_token_pos, 'B': max_token_pos})\n",
    "variant_graph.nodes.append(variant_graph_start)\n",
    "variant_graph.nodes.append(variant_graph_end)\n",
    "\n",
    "# to add first witness, create a variant-graph node for each token in the witness\n",
    "# create pointer from token_position_A to node\n",
    "# create pointer from variant graph node id to the node itself\n",
    "# create edges from each token in A to the next\n",
    "variant_graph_node_by_token_position_A = {}\n",
    "variant_graph_node_by_id = {}\n",
    "variant_graph_node_by_id[0] = variant_graph_start\n",
    "variant_graph_node_by_id[1] = variant_graph_end\n",
    "most_recent_variant_graph_node_id = 0\n",
    "variant_graph_edge_target_by_source = {}\n",
    "for index, token in enumerate(witnesses[0]):\n",
    "    id = len(variant_graph.nodes)\n",
    "    new_variant_graph_node = Variant_Graph_Node(id=id, token_positions = {'A': index})\n",
    "    variant_graph.nodes.append(new_variant_graph_node)\n",
    "    variant_graph_node_by_token_position_A[index] = id\n",
    "    variant_graph_node_by_id[id] = new_variant_graph_node\n",
    "\n",
    "    variant_graph.edges.append(Variant_Graph_Edge(source=most_recent_variant_graph_node_id , target=id))\n",
    "    variant_graph_edge_target_by_source[most_recent_variant_graph_node_id] = id\n",
    "    most_recent_variant_graph_node_id = id\n",
    "\n",
    "variant_graph.edges.append(Variant_Graph_Edge(source=most_recent_variant_graph_node_id, target=variant_graph_end.id)) # add edge from last A node to end node\n",
    "variant_graph_edge_target_by_source[most_recent_variant_graph_node_id] = variant_graph_end.id\n",
    "variant_graph.nodes # take a look\n",
    "# variant_graph.edges\n",
    "# variant_graph_node_by_token_position_A\n",
    "# variant_graph_node_by_id\n",
    "# variant_graph_edge_target_by_source\n",
    "# print(len(variant_graph.nodes), 'nodes;', len(variant_graph.edges), 'edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: Decision_Graph_Node(id=2, pattern_coordinate_witness_A=10, pattern_coordinate_witness_B=10),\n",
       " 1: Decision_Graph_Node(id=1, pattern_coordinate_witness_A=2, pattern_coordinate_witness_B=6),\n",
       " 0: Decision_Graph_Node(id=0, pattern_coordinate_witness_A=0, pattern_coordinate_witness_B=0),\n",
       " 4: Decision_Graph_Node(id=4, pattern_coordinate_witness_A=1, pattern_coordinate_witness_B=1),\n",
       " 3: Decision_Graph_Node(id=3, pattern_coordinate_witness_A=6, pattern_coordinate_witness_B=2),\n",
       " 6: Decision_Graph_Node(id=6, pattern_coordinate_witness_A=9, pattern_coordinate_witness_B=4),\n",
       " 7: Decision_Graph_Node(id=7, pattern_coordinate_witness_A=5, pattern_coordinate_witness_B=5),\n",
       " 5: Decision_Graph_Node(id=5, pattern_coordinate_witness_A=4, pattern_coordinate_witness_B=9)}"
      ]
     },
     "execution_count": 41,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_by_id # diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(ngram='ROOT', token_count=1, w1_start_position=-1, w1_end_position=-1, w2_start_position=-1, w2_end_position=-1),\n",
       " Match(ngram='the', token_count=1, w1_start_position=0, w1_end_position=0, w2_start_position=7, w2_end_position=7),\n",
       " Match(ngram='the red', token_count=2, w1_start_position=0, w1_end_position=1, w2_start_position=10, w2_end_position=11),\n",
       " Match(ngram='the', token_count=1, w1_start_position=0, w1_end_position=0, w2_start_position=10, w2_end_position=10),\n",
       " Match(ngram='red', token_count=1, w1_start_position=1, w1_end_position=1, w2_start_position=11, w2_end_position=11),\n",
       " Match(ngram='and the', token_count=2, w1_start_position=2, w1_end_position=3, w2_start_position=9, w2_end_position=10),\n",
       " Match(ngram='the black', token_count=2, w1_start_position=3, w1_end_position=4, w2_start_position=7, w2_end_position=8),\n",
       " Match(ngram='the', token_count=1, w1_start_position=3, w1_end_position=3, w2_start_position=7, w2_end_position=7),\n",
       " Match(ngram='the', token_count=1, w1_start_position=3, w1_end_position=3, w2_start_position=10, w2_end_position=10),\n",
       " Match(ngram='black', token_count=1, w1_start_position=4, w1_end_position=4, w2_start_position=8, w2_end_position=8),\n",
       " Match(ngram='cat', token_count=1, w1_start_position=5, w1_end_position=5, w2_start_position=12, w2_end_position=12)]"
      ]
     },
     "execution_count": 42,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns_A # diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RESUME HERE\n",
    "\n",
    "1. Decision graph is correct. Error (well, *first* error) is in building variant graph, below\n",
    "2. Can we work collaboratively in PyCharm, and thus break the notebook into modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-8: -1, 0: 0, 2: 2, 3: 3, 5: 5}"
      ]
     },
     "execution_count": 43,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we want to have: is a mapping from witness B tokens to nodes in the variant graph\n",
    "# What we have (or can easily derive):\n",
    "# A mapping from tokens in witness B to tokens in withness A\n",
    "# A mapping from tokens in witness A to variant-graph nodes\n",
    "#\n",
    "# To get the mapping from tokens A to tokens B we have to go over the alignment path calculated above.\n",
    "# for each item in the path we have a Pattern with two offsets.\n",
    "# We need to convert that into a set of tokens from B mapped to tokens in A\n",
    "\n",
    "tokens_in_A_by_B = {} # use B token position to retrieve A token position\n",
    "\n",
    "for path_node_number in best_path: # best_path is list of node ids, which can be used to find patterns\n",
    "    current_node = node_by_id[path_node_number]\n",
    "    current_pattern = patterns_A[current_node.pattern_coordinate_witness_A] # stores start and end positions for both witnesses\n",
    "    token_position_range_A = range(current_pattern.w1_start_position, current_pattern.w1_end_position + 1)\n",
    "    token_position_range_B = range(current_pattern.w2_start_position - w1_length - 1, current_pattern.w2_end_position - w1_length - 1 + 1)\n",
    "    pairs = list(zip(token_position_range_A,token_position_range_B))\n",
    "    ngram = current_pattern.ngram\n",
    "    for pair in pairs:\n",
    "        tokens_in_A_by_B[pair[1]] = pair[0]\n",
    "\n",
    "tokens_in_A_by_B # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variant_Graph_Node(id=0, token_positions={'A': -1, 'B': -1}),\n",
       " Variant_Graph_Node(id=1, token_positions={'A': 6, 'B': 6}),\n",
       " Variant_Graph_Node(id=2, token_positions={'A': 0, 'B': 0}),\n",
       " Variant_Graph_Node(id=3, token_positions={'A': 1}),\n",
       " Variant_Graph_Node(id=4, token_positions={'A': 2, 'B': 2}),\n",
       " Variant_Graph_Node(id=5, token_positions={'A': 3, 'B': 3}),\n",
       " Variant_Graph_Node(id=6, token_positions={'A': 4}),\n",
       " Variant_Graph_Node(id=7, token_positions={'A': 5, 'B': 5}),\n",
       " Variant_Graph_Node(id=8, token_positions={'B': 1}),\n",
       " Variant_Graph_Node(id=9, token_positions={'B': 4})]"
      ]
     },
     "execution_count": 44,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Variant_Graph_Edge(source=0, target=2),\n",
       " Variant_Graph_Edge(source=2, target=3),\n",
       " Variant_Graph_Edge(source=3, target=4),\n",
       " Variant_Graph_Edge(source=4, target=5),\n",
       " Variant_Graph_Edge(source=5, target=6),\n",
       " Variant_Graph_Edge(source=6, target=7),\n",
       " Variant_Graph_Edge(source=7, target=1),\n",
       " Variant_Graph_Edge(source=2, target=8),\n",
       " Variant_Graph_Edge(source=8, target=4),\n",
       " Variant_Graph_Edge(source=5, target=9),\n",
       " Variant_Graph_Edge(source=9, target=7)]"
      ]
     },
     "execution_count": 44,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over tokens in B\n",
    "# if token is aligned and already present in A, update properties of variant graph node\n",
    "# otherwise create new node\n",
    "most_recent_variant_graph_node_id = 0 # source of first edge\n",
    "for index, token in enumerate(witnesses[1]): # each token in B\n",
    "    node_id_to_work_on = -1\n",
    "    if index in tokens_in_A_by_B: # token is aligned and already present for A\n",
    "        current_variant_graph_node_id = variant_graph_node_by_token_position_A[tokens_in_A_by_B[index]]\n",
    "        variant_graph_node_by_id[current_variant_graph_node_id].token_positions['B'] = index\n",
    "        node_id_to_work_on = current_variant_graph_node_id\n",
    "    else: # not in A, so add new node\n",
    "        id = len(variant_graph.nodes)\n",
    "        new_variant_graph_node = Variant_Graph_Node(id=id, token_positions = {'B': index})\n",
    "        variant_graph.nodes.append(new_variant_graph_node)\n",
    "        variant_graph_node_by_id[id] = new_variant_graph_node\n",
    "        node_id_to_work_on = id\n",
    "\n",
    "    # now we process the edge\n",
    "    if most_recent_variant_graph_node_id in variant_graph_edge_target_by_source and variant_graph_edge_target_by_source[most_recent_variant_graph_node_id] == node_id_to_work_on: # already exists\n",
    "        pass\n",
    "    else: #create it\n",
    "        variant_graph.edges.append(Variant_Graph_Edge(source=most_recent_variant_graph_node_id , target=node_id_to_work_on))\n",
    "        variant_graph_edge_target_by_source[most_recent_variant_graph_node_id] = node_id_to_work_on\n",
    "    most_recent_variant_graph_node_id = node_id_to_work_on\n",
    "\n",
    "if most_recent_variant_graph_node_id in variant_graph_edge_target_by_source and not variant_graph_edge_target_by_source[most_recent_variant_graph_node_id] == variant_graph_end.id:\n",
    "    variant_graph.edges.append(Variant_graph_edge(source=most_recent_variant_graph_node_id, target=variant_graph_end.id)) # add edge from last B node to end node\n",
    "\n",
    "variant_graph.nodes\n",
    "variant_graph.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# create adjacency list for variant graph\n",
    "variant_graph_adjacency_list = defaultdict(list) # key is source, value is list of targets\n",
    "for edge in variant_graph.edges:\n",
    "    variant_graph_adjacency_list[edge.source].append(edge.target)\n",
    "    if edge.target not in variant_graph_adjacency_list: # adjacency list needs a key for every node, even if \n",
    "        variant_graph_adjacency_list[edge.target] = []\n",
    "# variant_graph_adjacency_list # take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1007pt\" height=\"98pt\" viewBox=\"0.00 0.00 1006.54 98.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 1002.54,-94 1002.54,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"50.7\" cy=\"-45\" rx=\"50.89\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"50.7\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">0:START</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"192.84\" cy=\"-45\" rx=\"34.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"192.84\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">2:the</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0-&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M101.5,-45C116.57,-45 133,-45 147.67,-45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"148,-48.5 158,-45 148,-41.5 148,-48.5\"/>\n<text text-anchor=\"middle\" x=\"129.89\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">AB</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"956.95\" cy=\"-45\" rx=\"41.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"956.95\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">1:END</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"318.48\" cy=\"-72\" rx=\"35.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"318.48\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">3:red</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge2\" class=\"edge\">\n<title>2-&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.04,-51.81C240.44,-55.18 259.24,-59.28 275.85,-62.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"275.34,-66.38 285.86,-65.1 276.84,-59.54 275.34,-66.38\"/>\n<text text-anchor=\"middle\" x=\"250.78\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"318.48\" cy=\"-18\" rx=\"44.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"318.48\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">8:black</text>\n</g>\n<!-- 2&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>2-&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M225.04,-38.19C238.41,-35.27 254.33,-31.79 269.17,-28.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"270.19,-31.91 279.22,-26.36 268.7,-25.07 270.19,-31.91\"/>\n<text text-anchor=\"middle\" x=\"250.78\" y=\"-36.8\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"446.07\" cy=\"-45\" rx=\"36.29\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"446.07\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">4:and</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge3\" class=\"edge\">\n<title>3-&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M351.17,-65.19C366.7,-61.85 385.62,-57.78 402.4,-54.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"403.48,-57.52 412.52,-52 402,-50.68 403.48,-57.52\"/>\n<text text-anchor=\"middle\" x=\"386.18\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"573.92\" cy=\"-45\" rx=\"34.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"573.92\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">5:the</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4-&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M482.8,-45C497.2,-45 513.97,-45 529.1,-45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"529.27,-48.5 539.27,-45 529.27,-41.5 529.27,-48.5\"/>\n<text text-anchor=\"middle\" x=\"510.97\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">AB</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"699.56\" cy=\"-72\" rx=\"44.39\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"699.56\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">6:black</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge5\" class=\"edge\">\n<title>5-&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M606.12,-51.81C619.49,-54.73 635.41,-58.21 650.25,-61.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"649.78,-64.93 660.3,-63.64 651.27,-58.09 649.78,-64.93\"/>\n<text text-anchor=\"middle\" x=\"631.86\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"699.56\" cy=\"-18\" rx=\"35.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"699.56\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">9:red</text>\n</g>\n<!-- 5&#45;&gt;9 -->\n<g id=\"edge10\" class=\"edge\">\n<title>5-&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M606.12,-38.19C621.52,-34.82 640.32,-30.72 656.93,-27.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"657.92,-30.46 666.94,-24.9 656.42,-23.62 657.92,-30.46\"/>\n<text text-anchor=\"middle\" x=\"631.86\" y=\"-36.8\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"824.55\" cy=\"-45\" rx=\"33.6\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"824.55\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">7:cat</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge6\" class=\"edge\">\n<title>6-&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M738.8,-63.61C752.87,-60.52 768.85,-57.01 783.11,-53.88\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"784.12,-57.24 793.14,-51.68 782.62,-50.4 784.12,-57.24\"/>\n<text text-anchor=\"middle\" x=\"767.26\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n</g>\n<!-- 7&#45;&gt;1 -->\n<g id=\"edge7\" class=\"edge\">\n<title>7-&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M858.46,-45C872.57,-45 889.38,-45 905.03,-45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"905.11,-48.5 915.11,-45 905.11,-41.5 905.11,-48.5\"/>\n<text text-anchor=\"middle\" x=\"886.85\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">AB</text>\n</g>\n<!-- 8&#45;&gt;4 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8-&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.85,-26.25C371.89,-29.27 387.87,-32.7 402.28,-35.8\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"401.94,-39.31 412.45,-37.99 403.41,-32.46 401.94,-39.31\"/>\n<text text-anchor=\"middle\" x=\"386.18\" y=\"-36.8\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n</g>\n<!-- 9&#45;&gt;7 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9-&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M732.23,-24.95C747.68,-28.34 766.43,-32.46 782.92,-36.08\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"782.32,-39.53 792.84,-38.26 783.82,-32.69 782.32,-39.53\"/>\n<text text-anchor=\"middle\" x=\"767.26\" y=\"-36.8\" font-family=\"Times,serif\" font-size=\"14.00\">B</text>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 46,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = graphviz.Digraph(format=\"svg\")\n",
    "a.graph_attr['rankdir'] = 'LR'\n",
    "for item in variant_graph.nodes: # set of Node objects, call it \"item\" to avoid overusing the keyword \"node\"\n",
    "    if item.id == 0:\n",
    "        ngram = 'START'\n",
    "    elif item.id== 1:\n",
    "        ngram = 'END'\n",
    "    elif 'A' in item.token_positions:\n",
    "        ngram = witnesses[0][item.token_positions['A']]\n",
    "    elif 'B' in item.token_positions:\n",
    "        ngram = witnesses[1][item.token_positions['B']]\n",
    "    else:\n",
    "        ngram = 'ERROR'\n",
    "    a.node(str(item.id), label=str(item.id) + ':' + ngram)\n",
    "for item in variant_graph.edges:\n",
    "    outgoing_edge_targets = variant_graph_adjacency_list[item.source]\n",
    "    if len(outgoing_edge_targets) == 1: # if there is one edge, get its label from its source\n",
    "        label = \"\".join(variant_graph.nodes[item.source].token_positions.keys())\n",
    "    else: # there are two edges, so two possible cases\n",
    "        outgoing_edge_target_counts = sum([len(variant_graph_node_by_id[target_node].token_positions) for target_node in variant_graph_adjacency_list[item.source]]) # 2 in simple case, 3 in dificult one\n",
    "        if outgoing_edge_target_counts == 2: # easy case; each target has one witness\n",
    "            label = \"\".join([k for k, v in variant_graph_node_by_id[item.target].token_positions.items()]) # ugly way to retrieve a single siglum\n",
    "        else: # difficult case; one target has one witness and one has two; which is which?\n",
    "            node_with_two_witnesses = outgoing_edge_targets[0] if len(variant_graph_node_by_id[outgoing_edge_targets[0]].token_positions) == 2 else outgoing_edge_targets[1]\n",
    "            node_with_one_witness = outgoing_edge_targets[0] if len(variant_graph_node_by_id[outgoing_edge_targets[0]].token_positions) == 1 else outgoing_edge_targets[1]\n",
    "            single_witness_label = [k for k,v in variant_graph_node_by_id[node_with_one_witness].token_positions.items()][0] # ugly way to get only key as string value\n",
    "            difficult_label = ({'A', 'B'} - {single_witness_label})\n",
    "            if item.target == node_with_two_witnesses:\n",
    "                label = difficult_label.pop() # get the lone set member as a string\n",
    "            else:\n",
    "                label = single_witness_label\n",
    "    a.edge(str(item.source), str(item.target), label=label)\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}