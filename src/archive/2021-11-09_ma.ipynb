{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Alignment hypergraph to variant graph with networkx and masked arrays\n",
    "\n",
    "Convert alignment hypergraph to variant graph before visualization.\n",
    "\n",
    "1. Create alignment hypergraph as a list of hyperedge objects\n",
    "1. Convert alignment hypergraph to numpy masked array\n",
    "1. Create variant graph from masked array using networkx\n",
    "1. Create graphviz SVG output from variant graph\n",
    "1. Other visualizations will be created from the variant graph (as in current CollateX)\n",
    "\n",
    "## Changelog\n",
    "\n",
    "### 2021-11-02\n",
    "\n",
    "1. Do not visualize original alignment graph (too large).\n",
    "1. Instead, visualize variant graph without node-joining (\"segmentation\").\n",
    "1. Perform node-joining of variant graph and visualize again.\n",
    "\n",
    "### 2021-11-09\n",
    "\n",
    "Alignment should be performed on vector space, not on alignment graph.\n",
    "\n",
    "But: the alignment graph can merge hyperedges, so we need a function that operates on the vector space to merge vectors and delete unneeded old ones.\n",
    "\n",
    "But: we can't delete a vector without forcing a copy, so we should zero it out instead. But then we'll have a lot of nulled rows that will complicate our processing, so …\n",
    "\n",
    "So: Create a potential vector by itself in memory, determine whether it can be merged into an existing vector, and either merge or add, as needed. If we keep track of the tail position, we can allocate a large space initially, know how much of it is in use, and operate over a slice that includes only the vectors that contain real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext memory_profiler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc # garbage collection\n",
    "import operator # comparison operators lt and gt\n",
    "from collections import defaultdict, deque\n",
    "from typing import Set, List\n",
    "from dataclasses import dataclass\n",
    "import networkx as nx\n",
    "import re\n",
    "import time\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "debug = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 paragraphs from 6 witnesses\n",
      "aligning 1 paragraphs\n"
     ]
    }
   ],
   "source": [
    "%run create-blocks.ipynb # data will be in prioritized_blocks\n",
    "print('aligning', how_many_paragraphs, 'paragraphs') # confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create data structures\n",
    "\n",
    "1. Create witness_node (dataclass) for witness nodes. Properties (not all may be needed):\n",
    "    1. token string: str\n",
    "    1. witness: str\n",
    "    1. offset in witness: int\n",
    "    1. hyperedge: hyperedge instance (initially null)\n",
    "    1. first_block: int (used for debugging, defaults to None)\n",
    "1. Create alignment_hyperedge (dataclass) for … wait for it! … alignment hyperedges! Properties:\n",
    "    1. witness_nodes: set of witness_node objects\n",
    "1. Create lists for witness_node and alignment_hyperedge instances\n",
    "1. Create START and END nodes and associated hyperedges\n",
    "\n",
    "**TODO:** Can we abolish the alignment_hyperedge objects entirely, keeping all information in the vector space model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "@dataclass(unsafe_hash=True)\n",
    "class WitnessNode:\n",
    "    \"\"\"TODO: Write a docstring some day\"\"\"\n",
    "    token_string: str\n",
    "    witness: str\n",
    "    witness_offset: int\n",
    "    hyperedge: hyperedge = None\n",
    "    # first_block: int = None # TODO: Remove this?\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.token_string\n",
    "\n",
    "# Subclass of WitnessNodeEnd so that our constructed end node will be able to identify itself\n",
    "class WitnessNodeEnd(WitnessNode):\n",
    "    \"\"\"TODO: Write a docstring some day\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Store all witness nodes in dictionary of lists\n",
    "witness_node_lists = defaultdict(list) # one per witness (keys will be sigla)\n",
    "# create, for each witness:\n",
    "#   nodes for real tokens\n",
    "#   END node\n",
    "# A start node is not created. That is on purpose.\n",
    "for index, witness_siglum in enumerate(witness_sigla):\n",
    "    # witness_sigla is a global set when the input data is read\n",
    "    for witness_token_offset, witness_token in enumerate(witnesses[index]): # list of tokens in a single witness\n",
    "        witness_node_lists[witness_siglum].append(WitnessNode(witness_token, witness_siglum, witness_token_offset))\n",
    "    witness_node_lists[witness_siglum].append(WitnessNodeEnd('END', witness_siglum, len(token_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_candidate_vectors(block):\n",
    "    \"\"\"Returns list of hyperedge data for block, one numpy array per hyperedge\"\"\"\n",
    "    candidate_vectors = [] # list of individual vectors, not a matrix\n",
    "    for token_offset_in_block in range(block.token_count):\n",
    "        # first add the token_offset_in_block to the block start positions\n",
    "        offset_start_positions = [value + token_offset_in_block for value in block.all_start_positions]\n",
    "        tokens_to_place = [ # list of all token positions, not just first in each witness\n",
    "            (\n",
    "                token_membership_array[token_array_index],\n",
    "                token_witness_offset_array[token_array_index]\n",
    "            )\n",
    "            for token_array_index in offset_start_positions\n",
    "        ]\n",
    "\n",
    "        data_for_new_vector = [0] * len(witness_sigla) # initialize to meaningless values\n",
    "        mask_for_new_vector = [True] * len(witness_sigla) # we'll unmask individual values as needed\n",
    "        for witness_number, witness_offset in tokens_to_place:\n",
    "            data_for_new_vector[witness_number] = witness_offset\n",
    "            mask_for_new_vector[witness_number] = False\n",
    "        candidate_vectors.append(ma.MaskedArray(data=data_for_new_vector, mask=mask_for_new_vector))\n",
    "\n",
    "    return candidate_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "if debug:\n",
    "    a = create_candidate_vectors(prioritized_blocks[0]) # no masked values\n",
    "    print(a)\n",
    "    b = create_candidate_vectors(prioritized_blocks[800]) # perhaps some masked values; too lazy to check\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def check_whether_okay_to_place(current_vectors, potential_vector) -> bool:\n",
    "    \"\"\"Return True iff we can add row without creating transpositions\n",
    "\n",
    "    current_vectors: vector space before new addition\n",
    "    potential_vector: we check whether this can be added\n",
    "\n",
    "    If subtracting a potential vector from any existing vector would return\n",
    "    values that diverge in sign, the potential would cross the existing one\n",
    "\n",
    "    If it's okay to place, we need to call merge_vector() to see whether we\n",
    "    should merge\"\"\"\n",
    "    subtractionResult = current_vectors - potential_vector\n",
    "    signs = np.sign(subtractionResult)\n",
    "    okayToPlace = (signs.min(axis=1) == signs.max(axis=1)).all()\n",
    "    if debug:\n",
    "        print(f\"{current_vectors=}\")\n",
    "        print(f\"{potential_vector=}\")\n",
    "        print(f\"{subtractionResult=}\")\n",
    "        print(f\"{signs=}\")\n",
    "        print(f\"{signs.min(axis = 1)}\")\n",
    "        print(f\"{signs.max(axis = 1)}\")\n",
    "        print(f\"{okayToPlace=}\")\n",
    "        print(f\"{type(okayToPlace)=}\")\n",
    "    return True if okayToPlace is ma.masked else okayToPlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def merge_vector(current_vectors, potential_vector):\n",
    "    # a potential vector must contain two or more witnesses\n",
    "    # vectors know their witness tokens, but witness tokens do not know their vector\n",
    "    #\n",
    "    # We've already determined that the vector is okay to place with check_whether_okay_to_place(),\n",
    "    # and we now need to check for whether we should merge\n",
    "    #\n",
    "    # How many existing vectors already contain tokens on the potential vector?\n",
    "    # There may be zero, one, or more existing vectors that contain tokens from the potential\n",
    "    #\n",
    "    found = [] # row numbers of vectors that match any witness in potential\n",
    "    for row_number, row_vector in enumerate(current_vectors):\n",
    "        comparison = row_vector - potential_vector\n",
    "        if len(np.where(comparison == 0)).size > 0 : # check whether comparison (a vector) contains any 0 values\n",
    "            found.append(row_number)\n",
    "\n",
    "    if len(found) > 1:\n",
    "        # found two or more existing vectors ...\n",
    "        # remove them from the hyperedges set\n",
    "        for index, row_number in enumerate(found):\n",
    "            existing_vector = current_vectors[row_number]\n",
    "            if debug==True:\n",
    "                print(\"We are trying to the vector at row: \" + str(row_number))\n",
    "                print(\"Existing vector has witness offsets: \" + str(existing_vector))\n",
    "                print(\"Potential vector has witness offsets: \" + str(potential_vector))\n",
    "            # to mask out a row (assuming six columns):\n",
    "            #   t = ma.MaskedArray(data = [0] * 6, mask=[True] * 6)\n",
    "            # We keep first existing vector, merge others into it, and mask out others\n",
    "            # np.logical_xor(t0.mask, t1.mask) returns True if one is masked, which are the ones we need to process\n",
    "            # ma.MaskedArray(data=t0.filled(1) * t1.filled(1), mask=~np.logical_xor(t0.mask, t1.mask))\n",
    "            #   keeps as unmasked only those where one value is masked and the other isn't\n",
    "            if index == 0:\n",
    "                pass # merge potential into first\n",
    "            else:\n",
    "                pass # merge current into first and mask current\n",
    "\n",
    "            # NB: Siglum may be in both existing and potential; we assume (!) always\n",
    "            #   with the same value\n",
    "            # TODO (maybe): Should we override the keys method in Alignment_Hyperedge instead of checking _sigla?\n",
    "            for siglum in existing_hyperedge._sigla: # don't get all keys, since that would include _sigla\n",
    "                if siglum not in potential_hyperedge:\n",
    "                    potential_hyperedge[siglum] = existing_hyperedge[siglum]\n",
    "        # replace them by a new hyperedge\n",
    "        hyperedges.append(potential_hyperedge)\n",
    "        # update hyperedge property on the witness nodes involved in the hyperedge\n",
    "        new_hyperedge = potential_hyperedge\n",
    "        for witness_node in new_hyperedge.values():\n",
    "            witness_node.hyperedge = new_hyperedge\n",
    "\n",
    "    elif len(found) == 0:\n",
    "        # we checked for transpositions\n",
    "        # TODO: Can we filter out hyperedges we've already placed, so that we don't have to process them\n",
    "        #   fully each time\n",
    "        hyperedges.append(potential_hyperedge)\n",
    "        # update hyperedge property on the witness nodes involved in the hyperedge\n",
    "        new_hyperedge = potential_hyperedge\n",
    "        for witness_node in new_hyperedge.values():\n",
    "            witness_node.hyperedge = new_hyperedge\n",
    "    else: # update single existing hyperedge\n",
    "        for siglum in potential_hyperedge._sigla:\n",
    "            list(found)[0][siglum] = potential_hyperedge[siglum] # add node to hyperedge (possibly redundantly)\n",
    "            potential_hyperedge[siglum].hyperedge = list(found)[0] # and update hyperedge property of node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def process_blocks(selected_blocks):\n",
    "    for index, selected_block in enumerate(selected_blocks):\n",
    "        # Create hyperedges for entire block\n",
    "        new_hyperedges = create_hyperedges(selected_block)\n",
    "        # Check only first and last, merge is both are okay\n",
    "        # NB: Checking only first and last to test entire block will break with discontinuous blocks\n",
    "        #   (should we ever switch to working with discontinuous blocks)\n",
    "        if index == 0:\n",
    "            # add the first block without checking for transposition since matrix will be empty\n",
    "            for potential_hyperedge in new_hyperedges:\n",
    "                merge_hyperedge(potential_hyperedge)\n",
    "        else:\n",
    "            debug_flag = None\n",
    "            no_transposition = check_whether_okay_to_place(hyperedges, new_hyperedges[0], debug_flag) and check_whether_okay_to_place(hyperedges, new_hyperedges[-1])\n",
    "            if no_transposition: # all-or-nothing\n",
    "                if debug:\n",
    "                    print(\"Block #\" + str(index) + \" has no transposition\")\n",
    "                for potential_hyperedge in new_hyperedges:\n",
    "                    merge_hyperedge(potential_hyperedge, debug=False)\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(\"Block #\"+ str(index) + \" (allegedly) has a transposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_hyperedges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1559/1543275265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# %prun profiles; see https://towardsdatascience.com/magic-commands-for-profiling-in-jupyter-notebook-d2ef00e29a63\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# %prun process_blocks(selected_blocks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprocess_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1559/3248783151.py\u001b[0m in \u001b[0;36mprocess_blocks\u001b[0;34m(selected_blocks)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Create hyperedges for entire block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnew_hyperedges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hyperedges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Check only first and last, merge is both are okay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# NB: Checking only first and last to test entire block will break with discontinuous blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_hyperedges' is not defined"
     ]
    }
   ],
   "source": [
    "# create all hyperedges (gingerly)\n",
    "# We exclude blocks with repetition (temporarily?)\n",
    "selected_blocks = filter(lambda x: len(x.all_start_positions) == x.witness_count, prioritized_blocks)\n",
    "\n",
    "# store all hyperedges in list\n",
    "hyperedges = []\n",
    "\n",
    "# %prun profiles; see https://towardsdatascience.com/magic-commands-for-profiling-in-jupyter-notebook-d2ef00e29a63\n",
    "# %prun process_blocks(selected_blocks)\n",
    "process_blocks(selected_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Convert alignment graph to numpy masked array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_alignment_graph_to_numpy_masked_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1559/2573502282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector_space_as_ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_alignment_graph_to_numpy_masked_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperedges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwitness_sigla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_alignment_graph_to_numpy_masked_array' is not defined"
     ]
    }
   ],
   "source": [
    "vector_space_as_ma = convert_alignment_graph_to_numpy_masked_array(hyperedges, witness_sigla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create variant graph from alignment hypergraph\n",
    "\n",
    "First create variant graph nodes. Walk over witness nodes in `witness_node_lists`. Keep track of which hyperedges have been added to variant graph. For each witness node:\n",
    "\n",
    "1. If node has no hyperedge, create variant graph node with selected information from witness node on it.\n",
    "1. If node has new hyperedge, create variant graph node with selected information from all witness nodes from hyperedge on it.\n",
    "1. If node has hyperedge that is already on variant graph, do nothing.\n",
    "\n",
    "Witness nodes contain token string, a witness siglum, a witness offset, and an optional hyperedge. The hyperedge contains, as dictionary items, all witness sigla as keys with witness nodes as values. From this information, we put onto the variant graph node the token string plus the hyperedge dictionary with sigla as keys and witness offsets as values.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The variant graph does not need to contain the token string, since we can look it up later, but for ease of visualization we include it here.\n",
    "* In Real Life CollateX a variant graph node contains properties other than the token string. We ignore that structure in our experiment as we concentrate on visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class VG_node():\n",
    "    \"\"\"Variant graph node\"\"\"\n",
    "    def __init__(self, token_string, data): # dictionary of siglum:witness_offset\n",
    "        self.token_string = token_string\n",
    "        self.__dict__.update(**data)\n",
    "        self._sigla = [key for key in data.keys()]\n",
    "    def __repr__(self):\n",
    "        return self.token_string + \"~\" + \"|\".join([\":\".join([str(key), str(getattr(self, key))]) for key in self.sigla()])\n",
    "    def __setitem__(self, key, value):\n",
    "        self._sigla.append(key)\n",
    "        self.__dict__[key] = value\n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "    def __contains__(self, key):\n",
    "        return key in self.__dict__\n",
    "    def sigla(self):\n",
    "        return self._sigla\n",
    "    def values(self):\n",
    "        return [self[key] for key in self._sigla]\n",
    "    def items(self):\n",
    "        return [(key, self.__dict__[key]) for key in self._sigla]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create networkx variant graph from our (not networkx) alignment graph\n",
    "# Create variant graph edges for start and end nodes (only)\n",
    "# We've treated VG as a global and we refer to it in the earlier functions; should we pass it instead?\n",
    "def augment_or_add_edge_without_conversion(siglum, source_VG_node, target_VG_node):\n",
    "    if VG.has_edge(source_VG_node, target_VG_node):\n",
    "        VG[source_VG_node][target_VG_node][\"siglum\"].append(siglum)\n",
    "    else:\n",
    "        VG.add_edge(\n",
    "            source_VG_node,\n",
    "            target_VG_node,\n",
    "            siglum = [siglum]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Use alignment graph node to look up variant graph node\n",
    "def alignment_node_to_VG_node(alignment_node: WitnessNode) -> VG_node:\n",
    "    global witness_offset_to_VG_node\n",
    "    return witness_offset_to_VG_node[alignment_node.witness][alignment_node.witness_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Create variant graph data edges (except start and end nodes)\n",
    "def augment_or_add_edge(siglum, source, target):\n",
    "    source_VG_node = alignment_node_to_VG_node(source)\n",
    "    target_VG_node = alignment_node_to_VG_node(target)\n",
    "    if VG.has_edge(source_VG_node, target_VG_node):\n",
    "        VG[source_VG_node][target_VG_node][\"siglum\"].append(siglum)\n",
    "    else:\n",
    "        VG.add_edge(\n",
    "            source_VG_node,\n",
    "            target_VG_node,\n",
    "            siglum = [siglum]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_variant_graph_from_vector_space():\n",
    "    # create variant graph and add start and end nodes\n",
    "\n",
    "    start_node = VG_node(\"START\", {})\n",
    "    end_node = VG_node(\"END\", {})\n",
    "    global VG # does this have to be global; we return it at the end, but we also use it in functions above\n",
    "    VG = nx.DiGraph(start = start_node, end = end_node) # create start and end properties to find terminal nodes\n",
    "    VG.add_node(start_node)\n",
    "    VG.add_node(end_node)\n",
    "\n",
    "    # keep track of which witness nodes (and therefore also hyperedges)\n",
    "    # have been added to VG\n",
    "    # does this duplicate information available from the following structure, since\n",
    "    #   if we assign a value below to replace the None, this doesn't seem to add anything\n",
    "    #   Can we get rid of this and use witness_offset_to_VG_node for this purpose instead?\n",
    "    # NB: Includes START and END, which we will later ignore\n",
    "    from bitarray import bitarray\n",
    "    global VG_tracking # keep this, but should we return it instead of making it global?\n",
    "    VG_tracking = {}\n",
    "    for siglum, witness_node_list in witness_node_lists.items():\n",
    "        VG_tracking[siglum] = bitarray(len(witness_node_list))\n",
    "        VG_tracking[siglum].setall(0)\n",
    "\n",
    "    # map from witness token node to variant graph node (needed to construct edges)\n",
    "    # values will be added as we create variant graph nodes\n",
    "    # NB: Includes START and END, which we will later ignore\n",
    "    global witness_offset_to_VG_node # does this have to be global?\n",
    "    witness_offset_to_VG_node = {}\n",
    "    for siglum in witness_node_lists.keys():\n",
    "        witness_offset_to_VG_node[siglum] = [None] * (len(witness_node_lists[siglum]))\n",
    "\n",
    "    # Replaces old code in following cell to build variant graph,\n",
    "    #   this time using vector space as main data source\n",
    "\n",
    "    # 1. Convert vectors to variant graph nodes\n",
    "\n",
    "    for row in vector_space_as_ma:\n",
    "        if debug:\n",
    "            print(' '.join(('Processing', str(row))))\n",
    "        # create dictionary of siglum:value for node (variable name: data) and update globals\n",
    "        data = {}\n",
    "        for index, value in enumerate(row):\n",
    "            if value != vector_space_as_ma.fill_value:\n",
    "                siglum = 'w' + str(index)\n",
    "                value = int(value)\n",
    "                data[siglum] = value # add to eventually data for new VG node\n",
    "                VG_tracking[siglum][value] = 1 # update global; do we need to adjust by 1?\n",
    "        # get token string for node (variable name: token_string)\n",
    "        siglum, offset = next(iter(data.items()))\n",
    "        token_string = witness_node_lists[siglum][int(offset)].token_string\n",
    "        # create and add new VG_node\n",
    "        new_VG_node = VG_node(token_string, data)\n",
    "        VG.add_node(new_VG_node)\n",
    "        for siglum, offset in data.items(): # Eek! Another for loop! How embarrassing!\n",
    "            witness_offset_to_VG_node[siglum][offset] = new_VG_node # update other global;\n",
    "\n",
    "    # 2. Traverse variant graph (in arbitrary order), each of which contains witness nodes.\n",
    "    #    Draw outgoing edges, which point to nodes with next tokens in each witness present\n",
    "    #      on the variant graph node.\n",
    "    #    Creating new variant graph nodes for witness tokens not in a hyperedge.\n",
    "    #    Use queue because we're iterating over a dynamic structure (inventory of variant graph nodes).\n",
    "\n",
    "    VG_node_queue = queue.Queue()\n",
    "    #\n",
    "    # Temporarily skipping START and END, which are the first two nodes\n",
    "    #\n",
    "    for node in VG.nodes(): # add all initial VG nodes to queue except START and END\n",
    "        VG_node_queue.put(node)\n",
    "        if debug:\n",
    "            print(f\"Adding node to VG_node_queue; length is {VG_node_queue.qsize()=}\")\n",
    "    ignore_start = VG_node_queue.get() # temporarily ignore START node when creating edges\n",
    "    ignore_end = VG_node_queue.get() # temporarily ignore END node when creating edges\n",
    "    if debug:\n",
    "        print(f\"Removed START and END; length is {VG_node_queue.qsize()=}\")\n",
    "    while not VG_node_queue.empty():\n",
    "        if debug:\n",
    "            print(f\"Processing source node from VG_node_queue; length is {VG_node_queue.qsize()=}\")\n",
    "        source_node = VG_node_queue.get()\n",
    "        targets = set()\n",
    "        edge_labels = defaultdict(list) # key is target VG node, value is list of sigla\n",
    "        sigla = source_node._sigla # all sigla on source node\n",
    "        for siglum in sigla:\n",
    "            # target may or may not already exist as node in VG\n",
    "            source_offset = source_node[siglum]\n",
    "            target_offset = source_offset + 1\n",
    "            target = witness_offset_to_VG_node[siglum][target_offset]\n",
    "            if target: # does the target already exist?:\n",
    "                targets.add(target)\n",
    "                edge_labels[target].append(siglum)\n",
    "            else: # 1) create target, 2) add to targets, 3) add to queue of nodes, and 4 + 5) update globals\n",
    "                # TODO: Do we need to update globals? We shouldn't need to return to nodes we add here.\n",
    "                # NB: If real target object is of type WitnessNodeEnd, don't add it to the queue or create an edge\n",
    "                # print(f\"{type(witness_node_lists[siglum][target_offset])=}\")\n",
    "                witness_node_target = witness_node_lists[siglum][target_offset]\n",
    "                if type(witness_node_target) != WitnessNodeEnd:\n",
    "                    target_token_string = witness_node_target.token_string\n",
    "                    new_VG_node = VG_node(target_token_string, {siglum: target_offset}) # 1 create target\n",
    "                    targets.add(new_VG_node) # 2 add to targets\n",
    "                    edge_labels[new_VG_node].append(siglum)\n",
    "                    VG_node_queue.put(new_VG_node) # 3 add to queue of nodes\n",
    "                    VG_tracking[siglum][target_offset] = 1 # 4 update first global\n",
    "                    witness_offset_to_VG_node[siglum][target_offset] = new_VG_node # 5. update second global\n",
    "        for target in targets:\n",
    "            VG.add_edge(source_node, target, label=\",\".join(edge_labels[target]))\n",
    "        #\n",
    "        # Add edges for start VG nodes\n",
    "        #\n",
    "        all_first_data_nodes = defaultdict(list)\n",
    "        for siglum in witness_sigla:\n",
    "            key =  witness_offset_to_VG_node[siglum][0]\n",
    "            all_first_data_nodes[key].append(siglum)\n",
    "        for key,value in all_first_data_nodes.items():\n",
    "            VG.add_edge(start_node, key, label=\",\".join(value))\n",
    "\n",
    "        #\n",
    "        # Add edges for end VG node\n",
    "        #\n",
    "        all_end_data_nodes = defaultdict(list)\n",
    "        for siglum in witness_sigla:\n",
    "            # Figure out why -2 works and clean up as needed\n",
    "            key = witness_offset_to_VG_node[siglum][-2]\n",
    "            all_end_data_nodes[key].append(siglum)\n",
    "        for key,value in all_end_data_nodes.items():\n",
    "            VG.add_edge(key, end_node, label=\",\".join(value))\n",
    "\n",
    "    # we're done! return the result\n",
    "    return VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'vector_space_as_ma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1559/3492852946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# del VG_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mVG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_variant_graph_from_vector_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1559/1225889532.py\u001b[0m in \u001b[0;36mcreate_variant_graph_from_vector_space\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# 1. Convert vectors to variant graph nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvector_space_as_ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector_space_as_ma' is not defined"
     ]
    }
   ],
   "source": [
    "# # recover some memory\n",
    "# del witness_node_list\n",
    "# del witness_offset_to_VG_node\n",
    "# del VG_tracking\n",
    "gc.collect()\n",
    "VG = create_variant_graph_from_vector_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualize variant graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"124pt\" height=\"98pt\" viewBox=\"0.00 0.00 123.69 98.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>variant_graph_unjoined</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 119.69,-94 119.69,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"57.84\" cy=\"-18\" rx=\"57.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"57.84\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">START (0)</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"57.84\" cy=\"-72\" rx=\"48.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"57.84\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">END (1)</text>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 18,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # node id values must be strings for graphviz\n",
    "a = graphviz.Digraph(format=\"svg\", name=\"variant_graph_unjoined\")\n",
    "a.attr(rankdir = \"LR\")\n",
    "a.attr(rank = 'same')\n",
    "a.attr(compound='true')\n",
    "\n",
    "# plot nodes, building {node: id} for lookup\n",
    "node_to_id = {}\n",
    "for index, node in enumerate(VG.nodes()):\n",
    "    node_id = str(index)\n",
    "    node_to_id[node] = node_id\n",
    "    node_text = node.token_string + \" (\" + node_id + \")\"\n",
    "    a.node(node_id, label=node_text)\n",
    "\n",
    "# plot edges\n",
    "for edge in VG.edges(data=True):\n",
    "    # edge is a three-item tuple: source, target, dictionary of properties\n",
    "    a.edge(node_to_id[edge[0]], node_to_id[edge[1]], label=edge[2][\"label\"])\n",
    "\n",
    "# print('aligning', how_many_paragraphs, 'paragraphs') # confirm\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#  This function joins the variant graph in place.\n",
    "#  This function is a straight port of the Java version of CollateX.\n",
    "\n",
    "def join(graph):\n",
    "    processed = set()\n",
    "    end = graph.graph[\"end\"]\n",
    "    queue = deque()\n",
    "    for (_, neighbor) in graph.out_edges(graph.graph[\"start\"]):\n",
    "        queue.appendleft(neighbor)\n",
    "    while queue:\n",
    "        vertex = queue.popleft()\n",
    "        out_edges = graph.out_edges(vertex)\n",
    "        if len(out_edges) == 1:\n",
    "            (_, join_candidate) = next(iter(out_edges))\n",
    "            can_join = join_candidate != end and len(graph.in_edges(join_candidate)) == 1\n",
    "            if can_join:\n",
    "                join_vertex_and_join_candidate(graph, join_candidate, vertex)\n",
    "                # we have merged join_candidate (tokens originally to the right) into vertex (tokens originally to the left)\n",
    "                # (for now, join_candidate node and all of its edges are still there, and we need to remove edges before\n",
    "                #   we can remove node, which we do in a for loop)\n",
    "                #\n",
    "                # RESUME HERE: both of the following branches are wrong, the first cryptically (it shows sigla, but not\n",
    "                #   all sigla) and the second conspicuously\n",
    "                #\n",
    "                for (_, neighbor, data) in list(graph.out_edges(join_candidate, data=True)):\n",
    "                    graph.remove_edge(join_candidate, neighbor)\n",
    "                    if graph.has_edge(vertex, neighbor):\n",
    "                        continue # TODO: this looks wrong\n",
    "#                         graph[source][target][\"siglum\"].append(data[\"siglum\"])\n",
    "                    else:\n",
    "#                         graph.add_edge(vertex, neighbor, siglum=data[\"siglum\"])\n",
    "                         graph.add_edge(vertex, neighbor,label=data[\"label\"])\n",
    "                graph.remove_edge(vertex, join_candidate)\n",
    "                graph.remove_node(join_candidate)\n",
    "                queue.appendleft(vertex)\n",
    "                continue\n",
    "        processed.add(vertex)\n",
    "        for (_, neighbor) in out_edges:\n",
    "            # FIXME: Why do we run out of memory in some cases here, if this is not checked?\n",
    "            if neighbor not in processed:\n",
    "                queue.appendleft(neighbor)\n",
    "\n",
    "\n",
    "def join_vertex_and_join_candidate(graph, join_candidate, vertex):\n",
    "    # Note: since there is no normalized/non normalized content in the graph\n",
    "    # a space character is added here for non punctuation tokens\n",
    "\n",
    "    if re.match(r'^\\W', join_candidate.token_string):\n",
    "        vertex.token_string += join_candidate.token_string\n",
    "    else:\n",
    "        vertex.token_string += (\" \" + join_candidate.token_string)\n",
    "    # join_candidate must have exactly one token (inside a list); left item may have more\n",
    "#     for siglum, token in join_candidate.tokens.items():\n",
    "#         vertex.add_token(siglum, token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# len(nx.algorithms.cycles.find_cycle(VG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "# nx.algorithms.cycles.find_cycle(VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "join(VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# pp.pprint([edge for edge in VG.edges()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligning 1 paragraphs\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"124pt\" height=\"98pt\" viewBox=\"0.00 0.00 123.69 98.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n<title>variant_graph_joined</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 119.69,-94 119.69,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"57.84\" cy=\"-18\" rx=\"57.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"57.84\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">START (0)</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"57.84\" cy=\"-72\" rx=\"48.19\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"57.84\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">END (1)</text>\n</g>\n</g>\n</svg>",
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 24,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## node id values must be strings for graphviz\n",
    "a = graphviz.Digraph(format=\"svg\", name=\"variant_graph_joined\")\n",
    "a.attr(rankdir = \"LR\")\n",
    "a.attr(rank = 'same')\n",
    "a.attr(compound='true')\n",
    "\n",
    "# plot nodes, building {node: id} for lookup\n",
    "node_to_id = {}\n",
    "for index, node in enumerate(VG.nodes()):\n",
    "    node_id = str(index)\n",
    "    node_to_id[node] = node_id\n",
    "    node_text = node.token_string + \" (\" + node_id + \")\"\n",
    "    a.node(node_id, label=node_text)\n",
    "\n",
    "# plot edges\n",
    "for edge in VG.edges(data=True):\n",
    "    # edge is a three-item tuple: source, target, dictionary of properties\n",
    "#     label = \"(all)\" if len(edge[2][\"siglum\"]) == len(witness_sigla) else \",\".join(sorted(edge[2][\"siglum\"]))\n",
    "    label = edge[2][\"label\"]\n",
    "    a.edge(node_to_id[edge[0]], node_to_id[edge[1]], label=label)\n",
    "\n",
    "print('aligning', how_many_paragraphs, 'paragraphs') # confirm\n",
    "SVG(a.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# for edge in VG.edges(data=True):\n",
    "#     print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# nx.algorithms.cycles.find_cycle(VG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# To do next\n",
    "\n",
    "1. Move SVG/Graphviz code into function\n",
    "1. Create alignment table visualization without joining\n",
    "1. Add joining to the alignment table (to check accuracy of joining results)\n",
    "1. Edge labels are wrong, which may be a problem with the join() function\n",
    "1. Test intermediate data sets, larger than a paragraph and smaller than a chapter\n",
    "1. Reassess method of prioritizing blocks\n",
    "1. Implement decision tree / graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# hyperedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to init server: Could not connect: Connection refused\n"
     ]
    }
   ],
   "source": [
    "# len(hyperedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:1680): Gtk-WARNING **: 20:04:28.937: cannot open display: :0\n"
     ]
    }
   ],
   "source": [
    "# vector_space_as_ma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# vector_space_as_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# for node in VG.nodes:\n",
    "#     pp.pprint(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# pp.pprint(witness_offset_to_VG_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}