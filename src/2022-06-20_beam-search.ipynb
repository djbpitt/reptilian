{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Traverse LCP, then MFS\n",
    "\n",
    "* Replace earlier masked-array strategy with regular arrays, using 0 to represent a null.\n",
    "* Real data (offset of token within witness) is one-based.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/pfzwp1hd7sv9jqxqw7tytff40000gn/T/ipykernel_85685/1182565917.py:11: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from linsuffarr import SuffixArray\n",
    "from linsuffarr import UNIT_BYTE\n",
    "import pprint\n",
    "import numpy as np\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from heapq import * # priority heap, https://docs.python.org/3/library/heapq.html\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "from bisect import bisect_right\n",
    "from IPython.core.display import display, HTML\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sigla = ['w0', 'w1', 'w2', 'w3', 'w4', 'w5']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt', 'darwin1861.txt', 'darwin1866.txt', 'darwin1869.txt', 'darwin1872.txt']\n",
    "# sigla = ['w0', 'w1', 'w2', 'w3']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt', 'darwin1861.txt', 'darwin1866.txt']\n",
    "# sigla = ['w0', 'w1']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt']\n",
    "sigla = ['w0', 'w1', 'w2', 'w3', 'w4']\n",
    "filenames = ['abc/abcd.txt', 'abc/abcda.txt', 'abc/abcdb.txt', 'abc/abcdc.txt', 'abc/abcdd.txt']\n",
    "first_paragraph = 0\n",
    "last_paragraph = 10\n",
    "how_many_paragraphs = last_paragraph - first_paragraph\n",
    "raw_data_dict = {}\n",
    "for siglum, filename in zip(sigla, filenames):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line for line in lines if line != '\\n']\n",
    "        raw_data_dict[siglum] = \" \".join(lines[first_paragraph : last_paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_witnesses(witness_strings: List[str]): # one string per witness\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    witnesses = []\n",
    "    for witness_string in witness_strings:\n",
    "        # witness_tokens = witness_string.split()\n",
    "        witness_tokens = re.findall(r'\\w+\\s*|\\W+', witness_string)\n",
    "        witness_tokens = [token.strip() for token in witness_tokens]\n",
    "        witnesses.append(witness_tokens)\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_token_array(witness_token_lists): # list of token lists per witness\n",
    "    '''Create token array (single list, with separator \" # \" between witnesses'''\n",
    "    token_array = [] # strings\n",
    "    token_membership_array = [] # witness identifiers, same offsets as in token_array\n",
    "    token_witness_offset_array = [] # one-based offset of token in witness\n",
    "    last_witness_offset = len(witness_token_lists) - 1\n",
    "    for index, witness_token_list in enumerate(witness_token_lists):\n",
    "        token_array.extend(witness_token_list)\n",
    "        for token_offset, token in enumerate(witness_token_list): # don't need enumerate, just len()\n",
    "            token_witness_offset_array.append(token_offset)\n",
    "        token_membership_array.extend([index for token in witness_token_list])\n",
    "        if index < last_witness_offset:\n",
    "            separator = \" #\" + str(index + 1) + \" \"\n",
    "            token_array.append(separator)\n",
    "            token_membership_array.append(separator)\n",
    "            token_witness_offset_array.append(-1)\n",
    "    return token_array, token_membership_array, token_witness_offset_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "witness_sigla = [key for key in raw_data_dict.keys()]\n",
    "witnesses = tokenize_witnesses([value for value in raw_data_dict.values()]) # strings\n",
    "# token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_array=['Darwin', '1859', 'WHEN', 'we', 'look', 'to', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub', '-', 'variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals', ',', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us', ',', 'is', ',', 'that', 'they', 'generally', 'differ', 'much', 'more', 'from', 'each', 'other', ',', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature', '.', 'When', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated', ',', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment', ',', 'I', 'think', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'greater', 'variability', 'is', 'simply', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as', ',', 'and', 'somewhat', 'different', 'from', ',', 'those', 'to', 'which', 'the', 'parent', '-', 'species', 'have', 'been', 'exposed', 'under', 'nature', '.', 'There', 'is', ',', 'also', ',', 'I', 'think', ',', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight', ',', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food', '.', 'It', 'seems', 'pretty', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'the', 'new', 'conditions', 'of', 'life', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation', ';', 'and', 'that', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary', ',', 'it', 'generally', 'continues', 'to', 'vary', 'for', 'many', 'generations', '.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'being', 'ceasing', 'to', 'be', 'variable', 'under', 'cultivation', '.', 'Our', 'oldest', 'cultivated', 'plants', ',', 'such', 'as', 'wheat', ',', 'still', 'often', 'yield', 'new', 'varieties', ':', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification', '.', ' #1 ', 'Darwin', '1860', 'WHEN', 'we', 'look', 'to', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub', '-', 'variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals', ',', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us', ',', 'is', ',', 'that', 'they', 'generally', 'differ', 'more', 'from', 'each', 'other', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature', '.', 'When', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated', ',', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment', ',', 'I', 'think', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'great', 'variability', 'is', 'simply', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as', ',', 'and', 'somewhat', 'different', 'from', ',', 'those', 'to', 'which', 'the', 'parent', '-', 'species', 'have', 'been', 'exposed', 'under', 'nature', '.', 'There', 'is', 'also', ',', 'I', 'think', ',', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight', ',', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food', '.', 'It', 'seems', 'pretty', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'the', 'new', 'conditions', 'of', 'life', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation', ';', 'and', 'that', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary', ',', 'it', 'generally', 'continues', 'to', 'vary', 'for', 'many', 'generations', '.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'being', 'ceasing', 'to', 'be', 'variable', 'under', 'cultivation', '.', 'Our', 'oldest', 'cultivated', 'plants', ',', 'such', 'as', 'wheat', ',', 'still', 'often', 'yield', 'new', 'varieties', ':', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification', '.', ' #2 ', 'Darwin', '1861', 'WHEN', 'we', 'look', 'to', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub', '-', 'variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals', ',', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us', ',', 'is', ',', 'that', 'they', 'generally', 'differ', 'more', 'from', 'each', 'other', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature', '.', 'When', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated', ',', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment', ',', 'I', 'think', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'great', 'variability', 'is', 'simply', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as', ',', 'and', 'somewhat', 'different', 'from', ',', 'those', 'to', 'which', 'the', 'parent', '-', 'species', 'have', 'been', 'exposed', 'under', 'nature', '.', 'There', 'is', 'also', ',', 'I', 'think', ',', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight', ',', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food', '.', 'It', 'seems', 'pretty', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'the', 'new', 'conditions', 'of', 'life', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation', ';', 'and', 'that', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary', ',', 'it', 'generally', 'continues', 'to', 'vary', 'for', 'many', 'generations', '.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'being', 'ceasing', 'to', 'be', 'variable', 'under', 'cultivation', '.', 'Our', 'oldest', 'cultivated', 'plants', ',', 'such', 'as', 'wheat', ',', 'still', 'often', 'yield', 'new', 'varieties', ':', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification', '.', ' #3 ', 'Darwin', '1866', 'Causes', 'of', 'Variability', '.', 'WHEN', 'we', 'look', 'to', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub', '-', 'variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals', ',', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us', ',', 'is', ',', 'that', 'they', 'generally', 'differ', 'more', 'from', 'each', 'other', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature', '.', 'When', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated', ',', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment', ',', 'I', 'think', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'great', 'variability', 'is', 'simply', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as', ',', 'and', 'somewhat', 'different', 'from', ',', 'those', 'to', 'which', 'the', 'parent', '-', 'species', 'have', 'been', 'exposed', 'under', 'nature', '.', 'There', 'is', 'also', ',', 'I', 'think', ',', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight', ',', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food', '.', 'It', 'seems', 'pretty', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'the', 'new', 'conditions', 'of', 'life', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation', ';', 'and', 'that', ',', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary', ',', 'it', 'generally', 'continues', 'to', 'vary', 'for', 'many', 'generations', '.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'being', 'ceasing', 'to', 'be', 'variable', 'under', 'cultivation', '.', 'Our', 'oldest', 'cultivated', 'plants', ',', 'such', 'as', 'wheat', ',', 'still', 'often', 'yield', 'new', 'varieties', ':', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification', '.', ' #4 ', 'Darwin', '1869', 'Causes', 'of', 'Variability', '.', 'WHEN', 'we', 'compare', 'the', 'individuals', 'of', 'the', 'same', 'variety', 'or', 'sub', '-', 'variety', 'of', 'our', 'older', 'cultivated', 'plants', 'and', 'animals', ',', 'one', 'of', 'the', 'first', 'points', 'which', 'strikes', 'us', 'is', ',', 'that', 'they', 'generally', 'differ', 'from', 'each', 'other', 'more', 'than', 'do', 'the', 'individuals', 'of', 'any', 'one', 'species', 'or', 'variety', 'in', 'a', 'state', 'of', 'nature', '.', 'And', 'if', 'we', 'reflect', 'on', 'the', 'vast', 'diversity', 'of', 'the', 'plants', 'and', 'animals', 'which', 'have', 'been', 'cultivated', ',', 'and', 'which', 'have', 'varied', 'during', 'all', 'ages', 'under', 'the', 'most', 'different', 'climates', 'and', 'treatment', ',', 'we', 'are', 'driven', 'to', 'conclude', 'that', 'this', 'great', 'variability', 'is', 'due', 'to', 'our', 'domestic', 'productions', 'having', 'been', 'raised', 'under', 'conditions', 'of', 'life', 'not', 'so', 'uniform', 'as', ',', 'and', 'somewhat', 'different', 'from', ',', 'those', 'to', 'which', 'the', 'parent', '-', 'species', 'had', 'been', 'exposed', 'under', 'nature', '.', 'There', 'is', 'also', ',', 'I', 'think', ',', 'some', 'probability', 'in', 'the', 'view', 'propounded', 'by', 'Andrew', 'Knight', ',', 'that', 'this', 'variability', 'may', 'be', 'partly', 'connected', 'with', 'excess', 'of', 'food', '.', 'It', 'seems', 'clear', 'that', 'organic', 'beings', 'must', 'be', 'exposed', 'during', 'several', 'generations', 'to', 'new', 'conditions', 'to', 'cause', 'any', 'appreciable', 'amount', 'of', 'variation', ';', 'and', 'that', ',', 'when', 'the', 'organisation', 'has', 'once', 'begun', 'to', 'vary', ',', 'it', 'generally', 'continues', 'varying', 'for', 'many', 'generations', '.', 'No', 'case', 'is', 'on', 'record', 'of', 'a', 'variable', 'organism', 'ceasing', 'to', 'vary', 'under', 'cultivation', '.', 'Our', 'oldest', 'cultivated', 'plants', ',', 'such', 'as', 'wheat', ',', 'still', 'yield', 'new', 'varieties', ':', 'our', 'oldest', 'domesticated', 'animals', 'are', 'still', 'capable', 'of', 'rapid', 'improvement', 'or', 'modification', '.']\n",
      "token_membership_array=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ' #1 ', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ' #2 ', 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ' #3 ', 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ' #4 ', 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "token_witness_offset_array=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252]\n"
     ]
    }
   ],
   "source": [
    "token_array, token_membership_array, token_witness_offset_array = create_token_array(witnesses)\n",
    "print(f\"{token_array=}\")\n",
    "print(f\"{token_membership_array=}\")\n",
    "print(f\"{token_witness_offset_array=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "suffix_array = SuffixArray(token_array, unit=UNIT_BYTE)\n",
    "# print(suffix_array)\n",
    "# LCP=0 means that the block has nothing in common with the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array('i', [0, 0, 0, 0, 0])"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcp_array = suffix_array._LCP_values\n",
    "lcp_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create Block dataclass\n",
    "from dataclasses import dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not into token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    witnesses: set\n",
    "    witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    # how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_blocks_old (_lcp_array):\n",
    "    '''Create blocks from lcp array\n",
    "\n",
    "    Skip first lcp value, which is a fake; otherwise compare lcp value to length of block at top of stack.\n",
    "    Four possibilities:\n",
    "\n",
    "        stack is empty\n",
    "            * if lcp value == 0, proceed to next lcp value (continue)\n",
    "            * if lcp value > 0, create block and push onto stack, then proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) matches block length at top of stack\n",
    "            * proceed to next lcp value (continue)\n",
    "\n",
    "        lcp value (cannot equal 0) is longer than block length at top of stack\n",
    "            * create and push new block\n",
    "\n",
    "        lcp value is shorter than block length at top of stack\n",
    "            * (recursive) if block at top of stack is longer than current lcp value, pop and append to _blocks\n",
    "            * if block at top of stack is equal to lcp value, proceed to next lcp value (continue)\n",
    "            * if block at top of stack is shorter than current lcp value ...\n",
    "            *   create and push new block starting at start position of most recently closed block, then proceed to next lcp value (continue)\n",
    "\n",
    "    In other words:\n",
    "\n",
    "        We proceed to next lcp value if:\n",
    "            * stack is empty and lcp value == 0\n",
    "            * lcp value matches block length at top of stack (can we combine this with the preceding, since an empty stack effectively has a zero-length block on top?)\n",
    "\n",
    "        We push a new value on stack and then proceed to next lcp value if:\n",
    "            * stack is empty and lcp value > 0\n",
    "            * lcp value is longer than block length at top of stack (where is the start position?)\n",
    "\n",
    "        We pop from the stack to _blocks and then check the next stack value (stick with same lcp) if:\n",
    "            * lcp value is shorter than current block value\n",
    "\n",
    "cases (occurrences are always one more than number of repetitions):\n",
    "    5 5 2     --> 1 block of 5 occurs 3 times, 1 block of 2 occures 4 times\n",
    "    2 5 5 2   --> 1 block of 2 occurs 5 times, 1 block of 5 occures 3 times\n",
    "    5 5 0 2   --> 1 block of 5 occurs 3 times, 1 block of 2 occures 2 times\n",
    "    2 5 5 2 3 --> \n",
    "\n",
    "\n",
    "Nested while structures:\n",
    "\n",
    "(Create blocks in two places because they have different start positions)\n",
    "(Nested while loops because we traverse two things: lcp array and, sometimes, stack)\n",
    "\n",
    "while next-lcp-value: # traverse lcp array\n",
    "    if something\n",
    "    elif something else\n",
    "    elif perhaps yet another something else\n",
    "    else: # possible hidden block (or possibly not)\n",
    "        while something-on-the-stack: # traverse stack for some lcp value situations\n",
    "            pop larger values\n",
    "        if hidden-block:\n",
    "            create and push\n",
    "clean-up-stack-after-last-lcp-value # or tack a 0 onto the end of the lcp to avoid extra clean-up code\n",
    "'''\n",
    "    from collections import deque # deque has faster append and pop than list\n",
    "    _blocks = []\n",
    "    open_block_stack = deque()\n",
    "    for offset, lcp in enumerate(lcp_array):\n",
    "        # three situations: next one is same value, higher that last, or lower than last\n",
    "        # if same value: same pattern\n",
    "        # if higher or lower, new pattern (may overlap with previous, unless one or the other value is 0)\n",
    "        peek = open_block_stack[-1] if open_block_stack else None\n",
    "        peek_token_count = peek.token_count if peek else 0\n",
    "        if offset == 0: # skip the first one, which is a transition from a fake start value\n",
    "            continue # resume loop with next item in lcp array\n",
    "        elif lcp == peek_token_count:\n",
    "            pass # same pattern (happens with repetition), so do nothing\n",
    "        elif lcp > peek_token_count: # new prefix is longer than previous one, so start new pattern\n",
    "            # can fill in end_position and frequency only when we encounter a shorter value in the LCP array\n",
    "            # start_position is number of patterns that are the same \n",
    "            open_block_stack.append(Block(token_count = lcp, start_position = offset - 1, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "        else: # new prefix is shorter than previous one, so:\n",
    "                # 1. close open blocks with higher values\n",
    "                # 2. do something else\n",
    "            while open_block_stack and open_block_stack[-1].token_count > lcp: # if an open block is longer than the current length, pop and close it\n",
    "                block_being_modified = open_block_stack.pop()\n",
    "                block_being_modified.end_position = offset - 1\n",
    "                block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "                _blocks.append(block_being_modified)\n",
    "            if lcp > 0 and (not open_block_stack or open_block_stack[-1].token_count < lcp):\n",
    "                open_block_stack.append(Block(token_count = lcp, start_position = _blocks[-1].start_position, end_position = -1, all_start_positions = [], witnesses = (), witness_count = -1, frequency = -1))\n",
    "\n",
    "    while open_block_stack: # pop anything left in open_block_stack\n",
    "        block_being_modified = open_block_stack.pop()\n",
    "        block_being_modified.end_position = len(lcp_array) - 1\n",
    "        block_being_modified.frequency = block_being_modified.end_position - block_being_modified.start_position + 1\n",
    "        _blocks.append(block_being_modified)\n",
    "\n",
    "    # add all_start_positions and then witness_count properties to blocks\n",
    "    for _index, _block in enumerate(_blocks):\n",
    "        # block_start_position through block_end_position gives offsets of all start positions in suffix_array\n",
    "        _block.all_start_positions = sorted([suffix_array.SA[x] for x in range(_block.start_position,_block.end_position + 1)])\n",
    "        # use all start positions to find witness count\n",
    "        _block.witnesses = set(token_membership_array[offset] for offset in _block.all_start_positions)\n",
    "        _block.witness_count = len(_block.witnesses)\n",
    "    return _blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Lcp_interval_candidate:\n",
    "    lcp_start_offset: int\n",
    "    lcp_interval_token_count: int\n",
    "    lcp_end_offset: int = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def expand_prefix(prefix_to_expand:Lcp_interval_candidate):\n",
    "    token_start_position = suffix_array.SA[prefix_to_expand.lcp_start_offset]\n",
    "    token_count = prefix_to_expand.lcp_interval_token_count\n",
    "    tokens = token_array[token_start_position: token_start_position + token_count]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_for_depth_and_repetition(_lcp_interval:Lcp_interval_candidate, _witness_count: int) -> bool:\n",
    "    \"\"\"Write a docstring someday\n",
    "\n",
    "    Number of prefixes >= total number of witnesses\n",
    "    Accumulate set of witness sigla for prefixes\n",
    "    if:\n",
    "        no witness occurs more than once, return True to keep this block\n",
    "    else:\n",
    "        return False\n",
    "    \"\"\"\n",
    "#     print(f\"Checking for depth and repetition for: {_lcp_interval=}\")\n",
    "#     print(f\"Occurs {_lcp_interval.lcp_end_offset - _lcp_interval.lcp_start_offset + 1} times in witness set\")\n",
    "#     print(expand_prefix(_lcp_interval))\n",
    "#     print()\n",
    "    block_instance_count = _lcp_interval.lcp_end_offset - _lcp_interval.lcp_start_offset + 1\n",
    "    if block_instance_count != _witness_count:\n",
    "        return False\n",
    "    else:\n",
    "        witnesses_found = []\n",
    "        for lcp_interval_item_offset in range(_lcp_interval.lcp_start_offset, _lcp_interval.lcp_end_offset + 1):\n",
    "            token_position = suffix_array.SA[lcp_interval_item_offset] # point from prefix to suffix array position\n",
    "            witness_siglum = token_membership_array[token_position] # point from token array position to witness identifier\n",
    "            if witness_siglum in witnesses_found:\n",
    "                return False\n",
    "            else:\n",
    "                witnesses_found.append(witness_siglum)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_blocks(_lcp_array: list):\n",
    "    \"\"\"Write a docstring someday\n",
    "\n",
    "    Look at changes in length of LCP array\n",
    "    Initial value is 0 or -1 because it's a comparison with previous, and first has no previous\n",
    "    Next value is number of tokens shared with previous\n",
    "    Exact length doesn't matter, but if it changes, new pattern:\n",
    "        If it stays the same, take note but do nothing yet; it means that the pattern repeats\n",
    "        No change for a while, then goes to 0:\n",
    "            Number of repetitions plus 1, e.g., 5 5 5 0 = 4 instances of 5\n",
    "            Once it changes to 0, we've seen complete pattern\n",
    "        Changer to smaller means hidden, deeper block\n",
    "        Changes to longer means ???\n",
    "    \"\"\"\n",
    "    accumulator = [] # lcp positions (not values) since most recent 0\n",
    "    frequent_sequences = [] # lcp intervals to be considered for mfs\n",
    "    #\n",
    "    # lcp value\n",
    "    # if == 0 it's a new interval, so:\n",
    "    #   1. if there is already an accumulation, commit (process) it\n",
    "    #      \"committing the buffer\" means checking for repetition and depth\n",
    "    #          if it passes check: store in mfs list\n",
    "    #          otherwise throw it away\n",
    "    #   2. clear buffer (accumulator) and begin accumulating new buffer with the new offset with 0 value\n",
    "    # otherwise it isn't zero, so there must be a buffer in place, so add to it (for now)\n",
    "    for offset, value in enumerate(_lcp_array):\n",
    "        if not accumulator and value == 0: # if accumulator is empty and new value is 0, do nothing\n",
    "            continue\n",
    "        elif not accumulator: # accumulator is empty and new value is non-zero, so begin new accumulator\n",
    "            accumulator.append(Lcp_interval_candidate(lcp_start_offset = offset - 1, lcp_interval_token_count = value))\n",
    "        elif value > accumulator[-1].lcp_interval_token_count: # new interval, so add to accumulator and continue\n",
    "            accumulator.append(Lcp_interval_candidate(lcp_start_offset = offset - 1, lcp_interval_token_count = value))\n",
    "        elif value == accumulator[-1].lcp_interval_token_count: # same block as before, so do nothing\n",
    "            continue\n",
    "        else: # new value is less than top of accumulator, so pop everything that is higher\n",
    "            # Positions in lcp array and suffix array coincide:\n",
    "            #   The lcp array value is the length of the sequence\n",
    "            #   The suffix array value is the start position of the sequence\n",
    "            # Assume accumulator values (offsets into lcp array) point to [3, 6] and new value is 4, so:\n",
    "            #   First: Pop pointer to 6 (length value in lcp array), store in frequent_sequences\n",
    "            #   Second: Push new pointer to same position in lcp array, but change value in lcp array to 4\n",
    "            while accumulator and accumulator[-1].lcp_interval_token_count > value:\n",
    "                # Create pointer to last closed block that is not filtered (like frequent_sequences)\n",
    "                newly_closed_block = accumulator.pop()\n",
    "                newly_closed_block.lcp_end_offset = offset - 1\n",
    "                if check_for_depth_and_repetition(newly_closed_block, len(witnesses)):\n",
    "                    frequent_sequences.append([newly_closed_block.lcp_start_offset, newly_closed_block.lcp_end_offset, newly_closed_block.lcp_interval_token_count])\n",
    "            # There are three options:\n",
    "            #   1. there is content in the accumulator and latest value is not 0\n",
    "            #   2. accumulator is empty and latest value is 0\n",
    "            #   3. accumulator is empty and latest value is not 0\n",
    "            # (the fourth logical combination, content in the accumulator and 0 value, cannot occur\n",
    "            #     because a 0 value will empty the accumulator)\n",
    "            if value > 0 and (not accumulator or accumulator[-1].lcp_interval_token_count != value):\n",
    "                accumulator.append(Lcp_interval_candidate(lcp_start_offset = newly_closed_block.lcp_start_offset, lcp_interval_token_count = value))\n",
    "    # End of lcp array; run through any residual accumulator values\n",
    "    while accumulator:\n",
    "        newly_closed_block = accumulator.pop()\n",
    "        newly_closed_block.lcp_end_offset = len(_lcp_array) - 1\n",
    "        if check_for_depth_and_repetition(newly_closed_block, len(witnesses)):\n",
    "            frequent_sequences.append([newly_closed_block.lcp_start_offset, len(_lcp_array)-1, newly_closed_block.lcp_interval_token_count])\n",
    "    return frequent_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# frequent_sequences is a list of lists\n",
    "# the embedded lists contain LCP indices\n",
    "#   LCP indices point into LCP array, but same index also points into suffix array\n",
    "#   value in LCP array points to prefix length (compared to previous one)\n",
    "#   value in suffix array points into token array\n",
    "frequent_sequences = create_blocks(lcp_array)\n",
    "# print(len(frequent_sequences))\n",
    "# pp.pprint(frequent_sequences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # A sequence like [40, 41, 42, 43] represents the same prefix. Each of these is a position in the LCP array that represents the length of the prefix.\n",
    "# for i in range(40, 44):\n",
    "#     print(f\"{lcp_array[i]=}\")\n",
    "#     print(f\"{token_array[suffix_array.SA[i]]=}\")\n",
    "# # Returns: 193, 30, 78, 0\n",
    "# # The length of a block is the lowest value higher than 0, so in this case 30.\n",
    "# # The suffix_array is in suffix_array.SA. Each of the four values is for a specific witness, so choose the first one arbitrarily, so 193.\n",
    "# # Examine 193rd value in suffix array:\n",
    "# print(f\"{suffix_array.SA[193]=}\")\n",
    "# # This returns 3378. The suffix array value is a pointer into the token array. So:\n",
    "# print(f\"{token_array[3378]=}\")\n",
    "# # The blocks are in alphabetical order.\n",
    "# # Look at part of token string\n",
    "# \" \".join(token_array[3368:3388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look at results\n",
    "# NB: This is debug output only\n",
    "# print(f\"{suffix_array._LCP_values=}\")\n",
    "# print(f\"{suffix_array.SA=}\")\n",
    "# pp.pprint(witnesses)\n",
    "#\n",
    "# print(\"Values are lcp_start_offset, lcp_end_offset, and lcp_interval_token_count\")\n",
    "# print()\n",
    "# largest_blocks = {} # key is token end position, value is (length, [witness-start-positions])\n",
    "# for frequent_sequence in frequent_sequences:\n",
    "# #     print(f\"Before filtering: examining frequent sequence {frequent_sequence}\")\n",
    "#     length = frequent_sequence[2]\n",
    "#     suffix_array_values = [suffix_array.SA[i] for i in range(frequent_sequence[0], frequent_sequence[1] + 1)]\n",
    "#     tokens = [token_array[i] for i in range(suffix_array_values[0], suffix_array_values[0] + length)]\n",
    "#     token_end_position = min(suffix_array_values) + length # token end position for first witness\n",
    "#     if token_end_position not in largest_blocks: # first block with this end position, so create new key\n",
    "#         largest_blocks[token_end_position] = (length, suffix_array_values)\n",
    "#     else: # if new block is longer, replace old one with same key\n",
    "#         if length > largest_blocks[token_end_position][0]:\n",
    "#             largest_blocks[token_end_position] = (length, suffix_array_values)\n",
    "# for b in frequent_sequences:\n",
    "#     lcp_start_value = b[0]\n",
    "#     token_start_position = suffix_array.SA[lcp_start_value]\n",
    "#     token_count = b[2]\n",
    "#     tokens = token_array[token_start_position: token_start_position + token_count]\n",
    "#     print(b, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(suffix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest_blocks={173: (29, [144, 404, 664, 928, 1189]), 135: (29, [106, 367, 627, 891, 1152]), 93: (31, [62, 323, 583, 847, 1111]), 212: (12, [200, 460, 720, 985, 1242]), 32: (26, [6, 269, 529, 793, 1057]), 244: (13, [231, 491, 751, 1016, 1271]), 39: (6, [33, 296, 556, 820, 1083]), 226: (12, [214, 474, 734, 999, 1255]), 143: (7, [136, 397, 657, 921, 1182]), 262: (17, [245, 505, 765, 1030, 1284]), 200: (10, [190, 450, 710, 974, 1231]), 1: (1, [0, 263, 523, 783, 1048]), 4: (2, [2, 265, 525, 789, 1054]), 61: (16, [45, 306, 566, 830, 1093]), 102: (7, [95, 356, 616, 880, 1142]), 185: (11, [174, 434, 694, 958, 1218]), 229: (2, [227, 487, 747, 1012, 1268]), 44: (3, [41, 303, 563, 827, 1089]), 41: (1, [40, 302, 562, 826, 1092]), 188: (2, [186, 446, 706, 970, 1229]), 105: (2, [103, 364, 624, 888, 1150])}\n"
     ]
    }
   ],
   "source": [
    "# To remove embedded prefixes:\n",
    "#\n",
    "# 1. Create dictionary with end position in witness 0 (arbitrarily) as key\n",
    "# 2. Set value of key to longest sequence with that end position\n",
    "# 3. Dictionary values will contain only longest frequent sequences, removing embedded ones,\n",
    "#    as tuples if (length, [token start positions for all witnesses])\n",
    "\n",
    "@dataclass\n",
    "class LongestSequence:\n",
    "    length: int\n",
    "    witness_start_and_end: List[int]\n",
    "\n",
    "def find_longest_sequences(_frequent_sequences, _suffix_array):\n",
    "    _largest_blocks = {} # key is token end position, value is (length, [witness-start-positions])\n",
    "    for frequent_sequence in _frequent_sequences:\n",
    "        length = frequent_sequence[2]\n",
    "        suffix_array_values = [_suffix_array.SA[i] for i in range(frequent_sequence[0], frequent_sequence[1] + 1)]\n",
    "        token_end_position = min(suffix_array_values) + length # token end position for first witness\n",
    "        if token_end_position not in _largest_blocks: # first block with this end position, so create new key\n",
    "            _largest_blocks[token_end_position] = (length, sorted(suffix_array_values))\n",
    "        else: # if new block is longer, replace old one with same key\n",
    "            if length > _largest_blocks[token_end_position][0]:\n",
    "                _largest_blocks[token_end_position] = (length, sorted(suffix_array_values))\n",
    "    return _largest_blocks\n",
    "\n",
    "largest_blocks = find_longest_sequences(frequent_sequences, suffix_array)\n",
    "print(f\"{largest_blocks=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144, 141, 141, 145, 141]\n",
      "[106, 104, 104, 108, 104]\n",
      "[62, 60, 60, 64, 63]\n",
      "[200, 197, 197, 202, 194]\n",
      "[6, 6, 6, 10, 9]\n",
      "[231, 228, 228, 233, 223]\n",
      "[33, 33, 33, 37, 35]\n",
      "[214, 211, 211, 216, 207]\n",
      "[136, 134, 134, 138, 134]\n",
      "[245, 242, 242, 247, 236]\n",
      "[190, 187, 187, 191, 183]\n",
      "[0, 0, 0, 0, 0]\n",
      "[2, 2, 2, 6, 6]\n",
      "[45, 43, 43, 47, 45]\n",
      "[95, 93, 93, 97, 94]\n",
      "[174, 171, 171, 175, 170]\n",
      "[227, 224, 224, 229, 220]\n",
      "[41, 40, 40, 44, 41]\n",
      "[40, 39, 39, 43, 44]\n",
      "[186, 183, 183, 187, 181]\n",
      "[103, 101, 101, 105, 102]\n",
      "block_offsets_by_witness=[[0, 2, 6, 33, 40, 41, 45, 62, 95, 103, 106, 136, 144, 174, 186, 190, 200, 214, 227, 231, 245], [263, 265, 269, 296, 302, 303, 306, 323, 356, 364, 367, 397, 404, 434, 446, 450, 460, 474, 487, 491, 505], [523, 525, 529, 556, 562, 563, 566, 583, 616, 624, 627, 657, 664, 694, 706, 710, 720, 734, 747, 751, 765], [783, 789, 793, 820, 826, 827, 830, 847, 880, 888, 891, 921, 928, 958, 970, 974, 985, 999, 1012, 1016, 1030], [1048, 1054, 1057, 1083, 1089, 1092, 1093, 1111, 1142, 1150, 1152, 1182, 1189, 1218, 1229, 1231, 1242, 1255, 1268, 1271, 1284], [0, 2, 6, 33, 40, 41, 45, 62, 95, 103, 106, 136, 144, 174, 186, 190, 200, 214, 227, 231, 245], [263, 265, 269, 296, 302, 303, 306, 323, 356, 364, 367, 397, 404, 434, 446, 450, 460, 474, 487, 491, 505], [523, 525, 529, 556, 562, 563, 566, 583, 616, 624, 627, 657, 664, 694, 706, 710, 720, 734, 747, 751, 765], [783, 789, 793, 820, 826, 827, 830, 847, 880, 888, 891, 921, 928, 958, 970, 974, 985, 999, 1012, 1016, 1030], [1048, 1054, 1057, 1083, 1089, 1092, 1093, 1111, 1142, 1150, 1152, 1182, 1189, 1218, 1229, 1231, 1242, 1255, 1268, 1271, 1284], [0, 2, 6, 33, 40, 41, 45, 62, 95, 103, 106, 136, 144, 174, 186, 190, 200, 214, 227, 231, 245], [263, 265, 269, 296, 302, 303, 306, 323, 356, 364, 367, 397, 404, 434, 446, 450, 460, 474, 487, 491, 505], [523, 525, 529, 556, 562, 563, 566, 583, 616, 624, 627, 657, 664, 694, 706, 710, 720, 734, 747, 751, 765], [783, 789, 793, 820, 826, 827, 830, 847, 880, 888, 891, 921, 928, 958, 970, 974, 985, 999, 1012, 1016, 1030], [1048, 1054, 1057, 1083, 1089, 1092, 1093, 1111, 1142, 1150, 1152, 1182, 1189, 1218, 1229, 1231, 1242, 1255, 1268, 1271, 1284], [0, 2, 6, 33, 40, 41, 45, 62, 95, 103, 106, 136, 144, 174, 186, 190, 200, 214, 227, 231, 245], [263, 265, 269, 296, 302, 303, 306, 323, 356, 364, 367, 397, 404, 434, 446, 450, 460, 474, 487, 491, 505], [523, 525, 529, 556, 562, 563, 566, 583, 616, 624, 627, 657, 664, 694, 706, 710, 720, 734, 747, 751, 765], [783, 789, 793, 820, 826, 827, 830, 847, 880, 888, 891, 921, 928, 958, 970, 974, 985, 999, 1012, 1016, 1030], [1048, 1054, 1057, 1083, 1089, 1092, 1093, 1111, 1142, 1150, 1152, 1182, 1189, 1218, 1229, 1231, 1242, 1255, 1268, 1271, 1284], [0, 2, 6, 33, 40, 41, 45, 62, 95, 103, 106, 136, 144, 174, 186, 190, 200, 214, 227, 231, 245], [263, 265, 269, 296, 302, 303, 306, 323, 356, 364, 367, 397, 404, 434, 446, 450, 460, 474, 487, 491, 505], [523, 525, 529, 556, 562, 563, 566, 583, 616, 624, 627, 657, 664, 694, 706, 710, 720, 734, 747, 751, 765], [783, 789, 793, 820, 826, 827, 830, 847, 880, 888, 891, 921, 928, 958, 970, 974, 985, 999, 1012, 1016, 1030], [1048, 1054, 1057, 1083, 1089, 1092, 1093, 1111, 1142, 1150, 1152, 1182, 1189, 1218, 1229, 1231, 1242, 1255, 1268, 1271, 1284]]\n",
      "witness_offsets_to_blocks={0: 1, 2: 4, 6: 32, 33: 39, 40: 41, 41: 44, 45: 61, 62: 93, 95: 102, 103: 105, 106: 135, 136: 143, 144: 173, 174: 185, 186: 188, 190: 200, 200: 212, 214: 226, 227: 229, 231: 244, 245: 262, 263: 1, 265: 4, 269: 32, 296: 39, 302: 41, 303: 44, 306: 61, 323: 93, 356: 102, 364: 105, 367: 135, 397: 143, 404: 173, 434: 185, 446: 188, 450: 200, 460: 212, 474: 226, 487: 229, 491: 244, 505: 262, 523: 1, 525: 4, 529: 32, 556: 39, 562: 41, 563: 44, 566: 61, 583: 93, 616: 102, 624: 105, 627: 135, 657: 143, 664: 173, 694: 185, 706: 188, 710: 200, 720: 212, 734: 226, 747: 229, 751: 244, 765: 262, 783: 1, 789: 4, 793: 32, 820: 39, 826: 41, 827: 44, 830: 61, 847: 93, 880: 102, 888: 105, 891: 135, 921: 143, 928: 173, 958: 185, 970: 188, 974: 200, 985: 212, 999: 226, 1012: 229, 1016: 244, 1030: 262, 1048: 1, 1054: 4, 1057: 32, 1083: 39, 1089: 44, 1092: 41, 1093: 61, 1111: 93, 1142: 102, 1150: 105, 1152: 135, 1182: 143, 1189: 173, 1218: 185, 1229: 188, 1231: 200, 1242: 212, 1255: 226, 1268: 229, 1271: 244, 1284: 262}\n",
      "first_token_offset_in_block_by_witness=[0, 263, 523, 783, 1048]\n",
      "first_absolute_token_by_witness=[0, 263, 523, 783, 1048]\n",
      "score_by_block={173: 857, 135: 671, 93: 464, 212: 1050, 32: 167, 244: 1208, 39: 201, 226: 1119, 143: 711, 262: 1297, 200: 988, 1: 5, 4: 28, 61: 303, 102: 507, 185: 916, 229: 1134, 44: 221, 41: 210, 188: 930, 105: 522}\n"
     ]
    }
   ],
   "source": [
    "# block_offsets_by_witness: list of lists holds sorted start offsets per witness (offsets are into global token array)\n",
    "# witness_offsets_to_blocks: dictionary points from start offsets to blocks\n",
    "# score_by_block: number of tokens placed or skipped if block is placed\n",
    "# Beam search requires us, given an offset in a witness, to find the next block. We do\n",
    "#   that by looking up the value in block_offsets_by_witness and then using that value\n",
    "#   to retrieve the block key from witness_offsets_to_blocks\n",
    "# Lookup in the list of lists is:\n",
    "#   block_offsets_by_witness[witness_number][bisect_right(block_offsets_by_witness[witness_number], most_recent_offset_in_witness)]\n",
    "# (See: https://www.geeksforgeeks.org/python-find-smallest-element-greater-than-k/)\n",
    "# FIXME: traverse largest_blocks only once and add values for all witnesses in same pass\n",
    "witness_count = len(witnesses)\n",
    "block_offsets_by_witness = []\n",
    "witness_offsets_to_blocks = {}\n",
    "first_token_offset_in_block_by_witness = [] # only tokens in blocks\n",
    "first_absolute_token_by_witness = [] # all tokens, whether in block or not\n",
    "for i in range(witness_count):\n",
    "    first_token_offset_in_block_by_witness.append(token_membership_array.index(i))\n",
    "    # Score = number of tokens either placed or skipped (we don't care which)\n",
    "    # Low score is best because it leaves the highest potential\n",
    "    # NB: The name \"score\" seems to imply that higher is better, and the\n",
    "    #   opposite is the case here. Rename the variable?\n",
    "    # NB: High potential is paramount during beam search, but should the\n",
    "    #   difference between placed and skip matter at a later stage? Or\n",
    "    #   does placing more blocks (more tiers) take care of that?\n",
    "    score_by_block = {}\n",
    "    for i in range(witness_count):\n",
    "        witness_offset_list = []\n",
    "        for key, value in largest_blocks.items():\n",
    "            witness_offset_list.append(value[1][i])\n",
    "            witness_offsets_to_blocks[value[1][i]] = key\n",
    "        witness_offset_list.sort()\n",
    "        block_offsets_by_witness.append(witness_offset_list)\n",
    "for i in range(witness_count):\n",
    "    first_absolute_token_by_witness.append(token_membership_array.index(i))\n",
    "for key, value in largest_blocks.items():\n",
    "    # to determine number of tokens that will have been placed or skipped\n",
    "    #   after placing block:\n",
    "    #       matrix-subtract first_token_offset_by_witness from value[1]\n",
    "    #       add witness_count * value[0] (to account for block length)\n",
    "    #   key by block key, value is score\n",
    "    differences = [x - y for x, y in zip(value[1], first_token_offset_in_block_by_witness)]\n",
    "    if debug:\n",
    "        print(differences)\n",
    "    score = sum(differences) + witness_count * value[0]\n",
    "    score_by_block[key] = score\n",
    "if debug:\n",
    "    print(f\"{block_offsets_by_witness=}\")\n",
    "    witness_offsets_to_blocks = { key: witness_offsets_to_blocks[key] for key in sorted(witness_offsets_to_blocks.keys())}\n",
    "    print(f\"{witness_offsets_to_blocks=}\")\n",
    "    print(f\"{first_token_offset_in_block_by_witness=}\")\n",
    "    print(f\"{first_absolute_token_by_witness=}\")\n",
    "    print(f\"{score_by_block=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "outputs": [],
   "source": [
    "# To perform beam search\n",
    "#   Create single start option (at Start node, which is a fiction [there is no Start block]\n",
    "#       created for the beam search)\n",
    "#   Loop: for each BeamOption on current tier\n",
    "#       Evaluate score for advancing in each witness and bringing others into alignment with it\n",
    "#       For  lowest (!) scores create new BeamOption (this advances to next tier)\n",
    "#           Score is count of tokens placed or skipped (!)\n",
    "#           Favor lowest score because that has the greatest potential"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(order=True, frozen=True, eq=True) # heapqueue is priority queue, so requires comparison\n",
    "class BeamOption:\n",
    "    score: int\n",
    "    path: tuple # path through sequence of blocks leading to current BeamOption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "outputs": [],
   "source": [
    "# Create initial BeamOption\n",
    "initial = [BeamOption(score=0, path=())] # tier 0, one-item list\n",
    "def perform_beam_search_step(beam_options=initial, beta=3):\n",
    "    new_options = [] # candidates for next tier\n",
    "    finished_options = []\n",
    "    for beam_option in beam_options:\n",
    "        for i in range(witness_count): # advance for each witness in turn\n",
    "            if not beam_option.path: # path is empty only for initial state at tier 0\n",
    "                last_offset = -1 # NB: same for all witnesses, and not 0, which will break for witness 0\n",
    "            else:\n",
    "                last_offset = largest_blocks[beam_option.path[0]][1][i]\n",
    "            try:\n",
    "                next_offset = block_offsets_by_witness[i][bisect_right(block_offsets_by_witness[i], last_offset)]\n",
    "                next_block = witness_offsets_to_blocks[next_offset] # find that next block to get its length\n",
    "                # would any witness pointer move backwards?\n",
    "                # perform matrix subtraction; if signs differ, there are items that move in opposite directions\n",
    "                # first option cannot be transposed, so accept it automatically\n",
    "                if (not beam_option.path) or (len(set([np.sign(x - y) for x, y in zip(largest_blocks[next_block][1], largest_blocks[beam_option.path[0]][1])])) == 1):\n",
    "                    new_score = score_by_block[next_block] # accounts for all witnesses\n",
    "                    # concatenate tuples with a +;  most recent first (for priority heap)\n",
    "                    new_options.append(BeamOption(score=new_score, path=((next_block,) + beam_option.path)))\n",
    "            except IndexError: # we've gone as far as we can with this path\n",
    "                finished_options.append(beam_option)\n",
    "                continue\n",
    "    new_options = list(set(new_options)) #deduplicate\n",
    "    heapify(new_options) # sort from low score to high (low score is best)\n",
    "    return new_options[:3], finished_options"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BeamOption(score=28, path=(4, 1))] [] 0 0\n",
      "[BeamOption(score=167, path=(32, 4, 1))] [] 1 0\n",
      "[BeamOption(score=201, path=(39, 32, 4, 1))] [] 2 0\n",
      "[BeamOption(score=210, path=(41, 39, 32, 4, 1)), BeamOption(score=221, path=(44, 39, 32, 4, 1))] [] 3 0\n",
      "[BeamOption(score=303, path=(61, 41, 39, 32, 4, 1)), BeamOption(score=303, path=(61, 44, 39, 32, 4, 1))] [] 4 0\n",
      "[BeamOption(score=464, path=(93, 61, 41, 39, 32, 4, 1)), BeamOption(score=464, path=(93, 61, 44, 39, 32, 4, 1))] [] 5 0\n",
      "[BeamOption(score=507, path=(102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=507, path=(102, 93, 61, 44, 39, 32, 4, 1))] [] 6 0\n",
      "[BeamOption(score=522, path=(105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=522, path=(105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 7 0\n",
      "[BeamOption(score=671, path=(135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=671, path=(135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 8 0\n",
      "[BeamOption(score=711, path=(143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=711, path=(143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 9 0\n",
      "[BeamOption(score=857, path=(173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=857, path=(173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 10 0\n",
      "[BeamOption(score=916, path=(185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=916, path=(185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 11 0\n",
      "[BeamOption(score=930, path=(188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=930, path=(188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 12 0\n",
      "[BeamOption(score=988, path=(200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=988, path=(200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 13 0\n",
      "[BeamOption(score=1050, path=(212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1050, path=(212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 14 0\n",
      "[BeamOption(score=1119, path=(226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1119, path=(226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 15 0\n",
      "[BeamOption(score=1134, path=(229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1134, path=(229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 16 0\n",
      "[BeamOption(score=1208, path=(244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1208, path=(244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 17 0\n",
      "[BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] [] 18 0\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 19 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 20 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 21 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 22 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 23 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 24 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 25 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 26 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 27 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 28 10\n",
      "[] [BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 41, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1)), BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))] 29 10\n"
     ]
    }
   ],
   "source": [
    "options, _ = perform_beam_search_step()\n",
    "finished = [] # options that cannot go further\n",
    "for i in range(30):\n",
    "    options, end_of_life = perform_beam_search_step(options)\n",
    "    finished.extend(end_of_life) # add any options that cannot go further\n",
    "    print(options, finished, i, len(finished))\n",
    "finished = list(set(finished))\n",
    "# TODO:\n",
    "#   1. Verify the scores\n",
    "#   2. Hold on to beam options when they cannot be extended (currently we throw them away). Hold results in global?\n",
    "# When new_options is empty, all options have overrun the end of the blocks, and best results will be in new global"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finished)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 238\n",
      "20 236\n",
      "BeamOption(score=1297, path=(262, 244, 229, 226, 212, 200, 188, 185, 173, 143, 135, 105, 102, 93, 61, 44, 39, 32, 4, 1))\n"
     ]
    }
   ],
   "source": [
    "# finished holds beam options that cannot go further, with duplicates removed\n",
    "# BeamOption.score counts tokens placed or skipped, which is correct for traversing, but\n",
    "#   for evaluation we count only most tokens placed and sub-sort by fewest blocks\n",
    "# Blocks know their length, so we sum the lengths of the finalists and keep only the highest\n",
    "# NB: There could be more than one\n",
    "finished.sort(reverse = True, key = lambda f: (sum([largest_blocks[b][0] for b in f.path])))\n",
    "for f in finished: #diagnostic\n",
    "    print(len(f.path), sum([largest_blocks[b][0] for b in f.path]))\n",
    "print(finished[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <html>\n        <head>\n            <style type=\"text/css\">\n                table, tr, th, td {border: 1px solid black; border-collapse: collapse;}\n                th, td {padding: 3px;}\n                td:first-child {text-align: right;}\n            </style></head><body><table><tr style=\"background-color: pink;\"><th>Row</th>\n    <th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w0</th>\n<th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w1</th>\n<th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w2</th>\n<th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w3</th>\n<th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w4</th></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">0</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">Darwin</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">1859</td><td style=\"border: 1px black solid; border-collapse: collapse;\">1860</td><td style=\"border: 1px black solid; border-collapse: collapse;\">1861</td><td style=\"border: 1px black solid; border-collapse: collapse;\">1866 Causes of Variability .</td><td style=\"border: 1px black solid; border-collapse: collapse;\">1869 Causes of Variability .</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">1</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">WHEN we</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">look to</td><td style=\"border: 1px black solid; border-collapse: collapse;\">look to</td><td style=\"border: 1px black solid; border-collapse: collapse;\">look to</td><td style=\"border: 1px black solid; border-collapse: collapse;\">look to</td><td style=\"border: 1px black solid; border-collapse: collapse;\">compare</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">2</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">the individuals of the same variety or sub - variety of our older cultivated plants and animals , one of the first points which strikes us</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">3</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">is , that they generally differ</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">much more</td><td style=\"border: 1px black solid; border-collapse: collapse;\">more</td><td style=\"border: 1px black solid; border-collapse: collapse;\">more</td><td style=\"border: 1px black solid; border-collapse: collapse;\">more</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">4</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">from each other</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\">more</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">5</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">than do the individuals of any one species or variety in a state of nature .</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">When</td><td style=\"border: 1px black solid; border-collapse: collapse;\">When</td><td style=\"border: 1px black solid; border-collapse: collapse;\">When</td><td style=\"border: 1px black solid; border-collapse: collapse;\">When</td><td style=\"border: 1px black solid; border-collapse: collapse;\">And if</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">6</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">we reflect on the vast diversity of the plants and animals which have been cultivated , and which have varied during all ages under the most different climates and treatment ,</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">I think</td><td style=\"border: 1px black solid; border-collapse: collapse;\">I think</td><td style=\"border: 1px black solid; border-collapse: collapse;\">I think</td><td style=\"border: 1px black solid; border-collapse: collapse;\">I think</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">7</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">we are driven to conclude that this</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">greater</td><td style=\"border: 1px black solid; border-collapse: collapse;\">great</td><td style=\"border: 1px black solid; border-collapse: collapse;\">great</td><td style=\"border: 1px black solid; border-collapse: collapse;\">great</td><td style=\"border: 1px black solid; border-collapse: collapse;\">great</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">8</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">variability is</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">simply</td><td style=\"border: 1px black solid; border-collapse: collapse;\">simply</td><td style=\"border: 1px black solid; border-collapse: collapse;\">simply</td><td style=\"border: 1px black solid; border-collapse: collapse;\">simply</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">9</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">due to our domestic productions having been raised under conditions of life not so uniform as , and somewhat different from , those to which the parent - species</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">have</td><td style=\"border: 1px black solid; border-collapse: collapse;\">have</td><td style=\"border: 1px black solid; border-collapse: collapse;\">have</td><td style=\"border: 1px black solid; border-collapse: collapse;\">have</td><td style=\"border: 1px black solid; border-collapse: collapse;\">had</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">10</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">been exposed under nature . There is</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">11</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">also , I think , some probability in the view propounded by Andrew Knight , that this variability may be partly connected with excess of food . It seems</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">pretty</td><td style=\"border: 1px black solid; border-collapse: collapse;\">pretty</td><td style=\"border: 1px black solid; border-collapse: collapse;\">pretty</td><td style=\"border: 1px black solid; border-collapse: collapse;\">pretty</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">12</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">clear that organic beings must be exposed during several generations to</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">the</td><td style=\"border: 1px black solid; border-collapse: collapse;\">the</td><td style=\"border: 1px black solid; border-collapse: collapse;\">the</td><td style=\"border: 1px black solid; border-collapse: collapse;\">the</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">13</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">new conditions</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">of life</td><td style=\"border: 1px black solid; border-collapse: collapse;\">of life</td><td style=\"border: 1px black solid; border-collapse: collapse;\">of life</td><td style=\"border: 1px black solid; border-collapse: collapse;\">of life</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">14</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">to cause any appreciable amount of variation ; and that</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td><td style=\"border: 1px black solid; border-collapse: collapse;\">,</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">15</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">when the organisation has once begun to vary , it generally continues</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">to vary</td><td style=\"border: 1px black solid; border-collapse: collapse;\">to vary</td><td style=\"border: 1px black solid; border-collapse: collapse;\">to vary</td><td style=\"border: 1px black solid; border-collapse: collapse;\">to vary</td><td style=\"border: 1px black solid; border-collapse: collapse;\">varying</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">16</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">for many generations . No case is on record of a variable</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">being</td><td style=\"border: 1px black solid; border-collapse: collapse;\">being</td><td style=\"border: 1px black solid; border-collapse: collapse;\">being</td><td style=\"border: 1px black solid; border-collapse: collapse;\">being</td><td style=\"border: 1px black solid; border-collapse: collapse;\">organism</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">17</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">ceasing to</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">be variable</td><td style=\"border: 1px black solid; border-collapse: collapse;\">be variable</td><td style=\"border: 1px black solid; border-collapse: collapse;\">be variable</td><td style=\"border: 1px black solid; border-collapse: collapse;\">be variable</td><td style=\"border: 1px black solid; border-collapse: collapse;\">vary</td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">18</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">under cultivation . Our oldest cultivated plants , such as wheat , still</td></tr><tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td><td style=\"border: 1px black solid; border-collapse: collapse;\">often</td><td style=\"border: 1px black solid; border-collapse: collapse;\">often</td><td style=\"border: 1px black solid; border-collapse: collapse;\">often</td><td style=\"border: 1px black solid; border-collapse: collapse;\">often</td><td style=\"border: 1px black solid; border-collapse: collapse;\"></td></tr><tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">19</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"5\">yield new varieties : our oldest domesticated animals are still capable of rapid improvement or modification .</td></tr></table></body></html>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table_top = \"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <style type=\"text/css\">\n",
    "                table, tr, th, td {border: 1px solid black; border-collapse: collapse;}\n",
    "                th, td {padding: 3px;}\n",
    "                td:first-child {text-align: right;}\n",
    "            </style></head><body><table><tr style=\"background-color: pink;\"><th>Row</th>\n",
    "    \"\"\" + '\\n'.join(['<th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w' + str(i) + '</th>' for i in range(len(witnesses))]) + '</tr>'\n",
    "table_bottom = '</table></body></html>'\n",
    "rows = []\n",
    "# Rows with aligned tokens are the same in all witness by definition\n",
    "# The path contains largest_blocks keys, which represent the last token of\n",
    "#   a block in witness 0\n",
    "# The value of a block is a tuple, the first member of which is the length\n",
    "# We can retrieve the aligned tokens by slicing them from the token_array\n",
    "for index, end_token_offset in enumerate(finished[0].path[::-1]): # path is ordered from last to first\n",
    "    # ###\n",
    "    # Information for aligned block\n",
    "    # This is the same for all witnesses, taken from witness 0\n",
    "    # ###\n",
    "    block_length = largest_blocks[end_token_offset][0]\n",
    "    start_token_offset = end_token_offset - block_length\n",
    "    tokens = token_array[start_token_offset: end_token_offset]\n",
    "    # ###\n",
    "    # Information for preceding non-aligned block\n",
    "    # This is different for each witness\n",
    "    #\n",
    "    # Loop over witnesses using range(len(witnesses))\n",
    "    # Get start token offset for aligned block for current witness\n",
    "    # Get end token offset for preceding aligned block for current witness\n",
    "    # Get tokens by slicing token array\n",
    "    # ###\n",
    "    if index > 0:\n",
    "        current_block = largest_blocks[end_token_offset]\n",
    "        preceding_block = largest_blocks[finished[0].path[::-1][index - 1]]\n",
    "        unaligned_row = []\n",
    "        unaligned_row.append('<tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td>')\n",
    "        for i in range(len(witnesses)):\n",
    "            unaligned_start_token_offset = preceding_block[1][i] + preceding_block[0]\n",
    "            unaligned_end_token_offset = current_block[1][i] - 1\n",
    "            unaligned_tokens = token_array[unaligned_start_token_offset: unaligned_end_token_offset + 1]\n",
    "            unaligned_row.append('<td style=\"border: 1px black solid; border-collapse: collapse;\">' + \" \".join(unaligned_tokens) + '</td>')\n",
    "        unaligned_row.append('</tr>')\n",
    "        rows.append(\"\".join(unaligned_row))\n",
    "    # ###\n",
    "    # Create aligned block\n",
    "    # ###\n",
    "    rows.append('<tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">' + str(index) + '</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"' + str(len(witnesses)) + '\">' + \" \".join(tokens) + '</td></tr>')\n",
    "display(HTML(table_top + \"\".join(rows) + table_bottom))\n",
    "# print(\"\".join(rows))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.core.display import HTML\n",
    "# table_top = \"\"\"\n",
    "#     <html>\n",
    "#         <head>\n",
    "#             <style type=\"text/css\">\n",
    "#                 table, tr, th, td {border: 1px solid black; border-collapse: collapse;}\n",
    "#                 th, td {padding: 3px;}\n",
    "#                 td:first-child {text-align: right;}\n",
    "#                 .aligned { background-color: beige;}\n",
    "#                 .nonaligned { background-color: lightgray;}\n",
    "#             </style></head><body><table><tr><th>Row</th>\n",
    "#     \"\"\" + '\\n'.join(['<th>w' + str(i) + '</th>' for i in range(len(witnesses))]) + '</tr>'\n",
    "# table_bottom = '</table></body></html>'\n",
    "# table_contents = ''\n",
    "# rows = []\n",
    "# sorted_keys = sorted(largest_blocks.keys())\n",
    "# #\n",
    "# # Check for leading nonaligned tokens\n",
    "# #\n",
    "# first_block_offsets = [token_witness_offset_array[i] for i in sorted(largest_blocks[sorted_keys[0]][1])]\n",
    "# leading_nonaligned_block = []\n",
    "# leading_nonaligned_witness_count = 0\n",
    "# leading_tokens = ''\n",
    "# for witness_number in range(len(first_block_offsets)):\n",
    "#     if first_block_offsets[witness_number] == 0:\n",
    "#         content = '[None]'\n",
    "#     else:\n",
    "#         content = \" \".join(witnesses[witness_number][0:first_block_offsets[witness_number]])\n",
    "#         leading_nonaligned_witness_count += 1\n",
    "#     leading_nonaligned_block.append('<td>' + content + '</td>')\n",
    "# if leading_nonaligned_witness_count:\n",
    "#     leading_tokens = '<tr class=\"nonaligned\"><td>Nonaligned</td>' + \"\".join(leading_nonaligned_block) + '</tr>'\n",
    "# #\n",
    "# # Creates aligned rows (preceded by any non-aligned tokens)\n",
    "# #\n",
    "# row_number = -1\n",
    "# for key_position, sorted_key in enumerate(sorted_keys):\n",
    "#     block_data = largest_blocks[sorted_key]\n",
    "#     block_length = block_data[0]\n",
    "#     token_start_positions = sorted(block_data[1])\n",
    "#     #\n",
    "#     # Check for preceding non-aligned row\n",
    "#     #\n",
    "#     preceding_nonaligned_block = ''\n",
    "#     if key_position != 0:\n",
    "#         end_positions_of_previous_block = []\n",
    "#         for start_position in largest_blocks[sorted_keys[key_position - 1]][1]:\n",
    "#             end_positions_of_previous_block.append(start_position + largest_blocks[sorted_keys[key_position - 1]][0])\n",
    "#         end_positions_of_previous_block.sort()\n",
    "#         content = ['<tr class=\"nonaligned\"><td>Nonaligned</td>']\n",
    "#         for nonaligned_segment_group in zip(end_positions_of_previous_block, token_start_positions):\n",
    "#             content.append('<td>' + \" \".join(token_array[nonaligned_segment_group[0]:nonaligned_segment_group[1]]) + '</td>')\n",
    "#         content.append('</td>')\n",
    "#         preceding_nonaligned_block = \"\".join(content)\n",
    "#     #\n",
    "#     # Create aligned row\n",
    "#     #\n",
    "#     row = []\n",
    "#     row_number += 1\n",
    "#     row_start = '<tr class=\"aligned\"><td>' + str(row_number) + '</td>'\n",
    "#     row_end = '</tr>'\n",
    "#     row.append(row_start)\n",
    "#     contents = token_array[token_start_positions[0]:token_start_positions[0] + block_length]\n",
    "#     row.append('<td colspan=\"' + str(len(token_start_positions)) + '\">' + ' '.join(contents) + '</td>')\n",
    "#     row.append(row_end)\n",
    "#     rows.append(preceding_nonaligned_block)\n",
    "#     rows.append(''.join(row))\n",
    "# table_contents = '\\n'.join(rows)\n",
    "# #\n",
    "# # Check for trailing nonaligned tokens, create row if needed\n",
    "# #\n",
    "# last_aligned_block = largest_blocks[sorted_keys[-1]]\n",
    "# last_aligned_block_length = last_aligned_block[0]\n",
    "# last_aligned_block_end_positions = [start_position + last_aligned_block_length - 1 for start_position in sorted(last_aligned_block[1])]\n",
    "# witness_lengths = [len(witness) for witness in witnesses]\n",
    "# last_aligned_token_pos = [token_witness_offset_array[i] for i in last_aligned_block_end_positions]\n",
    "# trailing_unaligned_token_counts = [witness_lengths[i] - last_aligned_token_pos[i] - 1 for i in range(len(witnesses))]\n",
    "#\n",
    "# trailing_nonaligned_block = []\n",
    "# trailing_nonaligned_witness_count = 0\n",
    "# trailing_tokens = ''\n",
    "# for witness_number, token_count in enumerate(trailing_unaligned_token_counts):\n",
    "#     if token_count == 0:\n",
    "#         content = \"[None]\"\n",
    "#     else:\n",
    "#         content = \" \".join(witnesses[witness_number][-token_count])\n",
    "#         trailing_nonaligned_witness_count += 1\n",
    "#     trailing_nonaligned_block.append('<td>' + content + '</td>')\n",
    "# if trailing_nonaligned_witness_count:\n",
    "#     trailing_tokens = '<tr class=\"nonaligned\"><td>Nonaligned</td>' + \"\".join(trailing_nonaligned_block) + '</tr>'\n",
    "# #\n",
    "# # Create and render table\n",
    "# #\n",
    "# # print(witnesses)\n",
    "# HTML(table_top + leading_tokens + table_contents + trailing_tokens + table_bottom)\n",
    "#\n",
    "# # 2022-06-14\n",
    "# #\n",
    "# # Where we are today\n",
    "# #\n",
    "# # We are not checking for transpositions (using the beam search);\n",
    "# #   instead we assume no transpositions (correct for our current test data)\n",
    "# # We number and output aligned blocks correctly\n",
    "# # We output nonaligned blocks correctly\n",
    "# #\n",
    "# # TODO\n",
    "# # Reimplement beam search to check for transpositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RESUME HERE\n",
    "\n",
    "1. We are not yet looking at a skip option, and we should.\n",
    "1. We have duplicate states and we shouldn't. If moving more than one witness takes us to the same state, we should keep only one (arbitrarily).\n",
    "1. We are not yet keeping track of our paths, so we can't reconstruct the best search result from start to finish\n",
    "\n",
    "Ad 2: With our red and black cat example, moving from the initial \"the\" to red and to black produces the same score but different states, so keep both. With our current non-diverging Darwin example, all successors have not only the same score, but also the same state, so we should simplify (in this case, our beam would contain only one item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dependency graph, with lots of simplifying assumptions\n",
    "#\n",
    "# Relations are from larger to smaller\n",
    "# Each value in LCP array corresponds to position in suffix array,\n",
    "#     which corresponds to position in token array,\n",
    "#     which corresponds to position in witness arrays\n",
    "# If there is already a path (chain) from A to B, do not create a direct edge\n",
    "# Requires that the length of the next block be shorter than the current one\n",
    "#   and that the start position in the next block be one less than that in the current block\n",
    "# The witnesses for a dependent block must be a subset (possibly equivalent) of the source of the dependency\n",
    "# dependencies = {}\n",
    "# for block_position in range(len(sorted_blocks) - 1):\n",
    "#     current_block = sorted_blocks[block_position]\n",
    "#     next_block = sorted_blocks[block_position + 1]\n",
    "#     if current_block.token_count > next_block.token_count \\\n",
    "#     and current_block.all_start_positions[0] == next_block.all_start_positions[0] - 1 \\\n",
    "#     and current_block.witnesses.issuperset(next_block.witnesses):\n",
    "#         dependencies[block_position] = block_position + 1\n",
    "# # print(dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# What to do next (in order)\n",
    "\n",
    "- [x] Create dataclass for beam option \\(initially score, witness, state\\)\n",
    "- [x] Remove witness identifier from beam option object; keep only score and state\n",
    "- [x] Save state as tuple instead of list\n",
    "- [x] Save history as tuple of tuples \\(stack\\), where most recent value is the current state\n",
    "- [x] Deduplicate options where score plus most recent state are the same. Earlier history may be different, but subsequent history can't be different if the most recent state is the same. We keep **a** \\(potential\\) best path, but not **all** potential best paths of the same score.\n",
    "- [x] Create visualization (vertical alignment table)\n",
    "- [ ] Process skip-one options for each witness\n",
    "- [ ] Deduplicate skip-one options before processing\n",
    "- [ ] Make beam size variable, depending on nature and extent of options (currently keep  best results, choosing arbitrarily in case of ties; perhaps keep  best scores, which may have more than  options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array('i', [0, 0, 0, 0, 0, 28, 55, 117, 117, 3, 107, 169, 10, 1, 1, 13, 21, 77, 139, 2, 122, 184, 25, 16, 1, 165, 227, 7, 1, 54, 54, 54, 4, 1, 174, 236, 16, 9, 1, 25, 52, 114, 114, 1, 20, 20, 20, 2, 1, 24, 24, 24, 6, 1, 1, 5, 163, 225, 5, 2, 15, 42, 104, 104, 1, 8, 16, 72, 134, 1, 1, 13, 0, 2, 10, 66, 128, 1, 183, 245, 25, 18, 0, 1, 1, 1, 1, 1, 1, 3, 30, 92, 92, 1, 45, 45, 45, 9, 1, 29, 29, 29, 11, 1, 3, 32, 59, 121, 1, 3, 1, 139, 201, 42, 0, 0, 0, 0, 0, 0, 14, 14, 14, 14, 0, 16, 3, 65, 65, 0, 0, 17, 44, 106, 106, 0, 6, 0, 1, 1, 1, 1, 0, 27, 54, 116, 116, 2, 106, 168, 9, 0, 2, 29, 91, 91, 0, 16, 43, 105, 105, 0, 44, 44, 44, 8, 0, 28, 28, 28, 10, 0, 2, 31, 58, 120, 0, 4, 0, 2, 195, 257, 37, 0, 138, 200, 41, 0, 5, 143, 205, 46, 1, 38, 38, 38, 2, 0, 115, 177, 18, 9, 0, 116, 178, 19, 10, 0, 29, 56, 118, 118, 0, 19, 6, 68, 68, 0, 176, 238, 18, 11, 2, 128, 190, 31, 22, 1, 12, 20, 76, 138, 1, 15, 2, 64, 64, 1, 109, 171, 12, 3, 1, 121, 183, 24, 15, 0, 175, 237, 17, 10, 1, 10, 10, 10, 10, 1, 127, 189, 30, 21, 0, 21, 8, 70, 70, 1, 11, 149, 211, 52, 0, 20, 7, 69, 69, 0, 9, 103, 165, 6, 1, 9, 9, 9, 9, 0, 14, 22, 78, 140, 1, 22, 22, 22, 4, 0, 6, 21, 83, 83, 1, 10, 37, 99, 99, 1, 33, 33, 33, 0, 124, 186, 27, 18, 1, 7, 36, 63, 125, 1, 23, 31, 87, 149, 0, 57, 57, 57, 7, 0, 36, 36, 36, 0, 8, 23, 85, 85, 0, 18, 45, 107, 107, 0, 7, 7, 7, 7, 0, 43, 43, 43, 7, 0, 22, 9, 71, 71, 0, 35, 35, 35, 2, 0, 11, 26, 88, 88, 0, 110, 172, 13, 4, 0, 0, 6, 100, 162, 3, 0, 20, 28, 84, 146, 3, 13, 75, 75, 1, 0, 8, 35, 97, 97, 0, 51, 51, 51, 1, 0, 123, 185, 26, 17, 1, 26, 26, 26, 8, 2, 178, 240, 20, 13, 0, 30, 30, 30, 12, 0, 1, 159, 221, 1, 0, 111, 173, 14, 5, 1, 10, 18, 74, 136, 0, 132, 194, 35, 26, 0, 15, 153, 215, 56, 0, 26, 34, 90, 152, 0, 11, 11, 11, 11, 0, 8, 102, 164, 5, 0, 29, 37, 93, 155, 0, 117, 179, 20, 11, 1, 4, 19, 81, 81, 0, 2, 2, 156, 218, 0, 6, 33, 95, 95, 0, 5, 20, 82, 82, 1, 6, 35, 62, 124, 0, 170, 232, 12, 5, 0, 4, 31, 93, 93, 0, 48, 48, 48, 12, 0, 9, 17, 73, 135, 1, 3, 3, 157, 219, 0, 52, 52, 52, 2, 1, 2, 160, 222, 2, 0, 46, 46, 46, 10, 1, 2, 17, 79, 79, 0, 3, 97, 159, 0, 0, 0, 59, 59, 59, 9, 0, 125, 187, 28, 19, 2, 8, 64, 126, 1, 119, 181, 22, 13, 0, 24, 32, 88, 150, 0, 0, 4, 4, 4, 4, 0, 6, 144, 206, 47, 1, 22, 49, 111, 111, 0, 13, 151, 213, 54, 2, 190, 252, 32, 25, 0, 2, 6, 164, 226, 6, 1, 30, 57, 119, 1, 1, 42, 42, 42, 6, 1, 39, 95, 157, 0, 53, 53, 53, 3, 0, 18, 26, 82, 144, 1, 11, 73, 73, 0, 193, 255, 35, 0, 47, 47, 47, 11, 0, 11, 38, 100, 100, 0, 2, 2, 2, 2, 0, 4, 158, 220, 1, 0, 112, 174, 15, 6, 0, 0, 7, 22, 84, 84, 0, 2, 4, 33, 60, 122, 2, 140, 202, 43, 0, 14, 76, 76, 2, 1, 16, 16, 16, 16, 0, 17, 25, 81, 143, 0, 5, 1, 39, 39, 39, 3, 1, 12, 150, 212, 53, 1, 5, 32, 94, 94, 1, 19, 27, 83, 145, 2, 12, 74, 74, 1, 3, 141, 203, 44, 1, 181, 243, 23, 16, 1, 6, 6, 6, 6, 1, 172, 234, 14, 7, 2, 131, 193, 34, 25, 2, 189, 251, 31, 24, 1, 18, 5, 67, 67, 0, 18, 18, 18, 0, 179, 241, 21, 14, 0, 27, 27, 27, 9, 1, 12, 12, 12, 12, 0, 41, 41, 41, 5, 1, 135, 197, 38, 29, 0, 58, 58, 58, 8, 0, 173, 235, 15, 8, 1, 10, 148, 210, 51, 0, 3, 3, 3, 3, 1, 185, 247, 27, 20, 1, 8, 146, 208, 49, 0, 9, 24, 86, 86, 0, 60, 60, 60, 10, 0, 0, 1, 1, 155, 217, 0, 27, 35, 91, 153, 1, 180, 242, 22, 15, 1, 13, 13, 13, 13, 0, 3, 11, 67, 129, 0, 9, 36, 98, 98, 0, 25, 25, 25, 7, 1, 177, 239, 19, 12, 3, 129, 191, 32, 23, 0, 169, 231, 11, 4, 0, 27, 89, 89, 0, 23, 50, 112, 112, 0, 25, 33, 89, 151, 0, 19, 46, 108, 108, 0, 22, 30, 86, 148, 0, 5, 5, 5, 5, 0, 40, 40, 40, 4, 0, 136, 198, 39, 30, 0, 187, 249, 29, 22, 0, 1, 28, 90, 90, 0, 3, 18, 80, 80, 0, 38, 94, 156, 0, 16, 24, 80, 142, 0, 24, 51, 113, 113, 0, 11, 19, 75, 137, 0, 1, 9, 65, 127, 1, 9, 147, 209, 50, 0, 4, 142, 204, 45, 0, 8, 8, 8, 8, 1, 19, 19, 19, 1, 0, 167, 229, 9, 2, 0, 184, 246, 26, 19, 0, 23, 23, 23, 5, 0, 16, 154, 216, 57, 0, 14, 1, 10, 25, 87, 87, 1, 4, 162, 224, 4, 1, 5, 99, 161, 2, 2, 14, 41, 103, 103, 1, 63, 63, 0, 171, 233, 13, 6, 1, 14, 152, 214, 55, 3, 191, 253, 33, 26, 1, 113, 175, 16, 7, 1, 15, 77, 77, 1, 61, 61, 61, 11, 1, 4, 12, 68, 130, 1, 130, 192, 33, 24, 1, 188, 250, 30, 23, 1, 134, 196, 37, 28, 1, 21, 48, 110, 110, 0, 3, 161, 223, 3, 0, 26, 53, 115, 115, 1, 105, 167, 8, 0, 4, 98, 160, 1, 1, 13, 40, 102, 102, 0, 7, 15, 71, 133, 0, 34, 34, 34, 1, 23, 10, 72, 72, 1, 7, 101, 163, 4, 1, 1, 28, 36, 92, 154, 1, 192, 254, 34, 2, 16, 78, 78, 1, 56, 56, 56, 6, 2, 50, 50, 50, 2, 1, 6, 14, 70, 132, 0, 108, 170, 11, 2, 0, 21, 29, 85, 147, 1, 31, 31, 31, 13, 1, 5, 34, 61, 123, 1, 114, 176, 17, 8, 0, 15, 23, 79, 141, 0, 166, 228, 8, 1, 0, 2, 40, 96, 158, 1, 12, 39, 101, 101, 0, 37, 37, 37, 1, 1, 32, 32, 32, 0, 17, 4, 66, 66, 0, 118, 180, 21, 12, 0, 15, 15, 15, 15, 0, 7, 145, 207, 48, 1, 182, 244, 24, 17, 1, 186, 248, 28, 21, 0, 55, 55, 55, 5, 1, 49, 49, 49, 1, 0, 0, 133, 195, 36, 27, 0, 20, 47, 109, 109, 0, 10, 104, 166, 7, 1, 1, 194, 256, 36, 1, 137, 199, 40, 31, 0, 21, 21, 21, 3, 0, 62, 62, 62, 12, 0, 126, 188, 29, 20, 2, 120, 182, 23, 14, 1, 168, 230, 10, 3, 1, 5, 13, 69, 131, 0, 7, 34, 96, 96, 0, 17, 17, 17, 17])"
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix_array._LCP_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values are:\n",
      "  offset into suffix array,\n",
      "  value in suffix array (pointer to first token in token_array),\n",
      "  value in LCP array (length of prefix),\n",
      "  tokens (if any)\n",
      "\n",
      "87 1108 1 .\n",
      "\n",
      "88 1215 1 .\n",
      "\n",
      "89 954 3 . It seems\n",
      "\n",
      "90 170 30 . It seems pretty clear that organic beings must be exposed during several generations to the new conditions of life to cause any appreciable amount of variation ; and that\n",
      "\n",
      "91 430 92 . It seems pretty clear that organic beings must be exposed during several generations to the new conditions of life to cause any appreciable amount of variation ; and that when the organisation has once begun to vary , it generally continues to vary for many generations . No case is on record of a variable being ceasing to be variable under cultivation . Our oldest cultivated plants , such as wheat , still often yield new varieties : our oldest domesticated animals are still capable of rapid improvement or modification .\n",
      "\n",
      "92 690 92 . It seems pretty clear that organic beings must be exposed during several generations to the new conditions of life to cause any appreciable amount of variation ; and that when the organisation has once begun to vary , it generally continues to vary for many generations . No case is on record of a variable being ceasing to be variable under cultivation . Our oldest cultivated plants , such as wheat , still often yield new varieties : our oldest domesticated animals are still capable of rapid improvement or modification .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1072\t'also,Ithin'\tLCP=0 \n",
    "# 129\t'also,Ithin'\tLCP=25 \n",
    "# 364\t'also,Ithin'\tLCP=106 \n",
    "# 599\t'also,Ithin'\tLCP=106 \n",
    "# 837\t'also,Ithin'\tLCP=50 \n",
    "\n",
    "print(\"Values are:\\n  offset into suffix array,\\n  value in suffix array (pointer to first token in token_array),\\n  value in LCP array (length of prefix),\\n  tokens (if any)\")\n",
    "print()\n",
    "for offset, sa_item in enumerate(suffix_array.SA):\n",
    "    lcp_value = suffix_array._LCP_values[offset]\n",
    "    tokens = token_array[sa_item: sa_item + lcp_value]\n",
    "    if offset in range(87, 93):\n",
    "        print(offset, sa_item, lcp_value, \" \".join(tokens))\n",
    "        print()\n",
    "\n",
    "# 2022-06-17 RESUME HERE\n",
    "# For each step in this interval check accumulator and frequent sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def priority(block: Block) -> float:\n",
    "#     '''Priority ranges from 0 to \n",
    "\n",
    "#     depth (number of witnesses) / (frequency * length)\n",
    "#         modified (by trial and error) to weight the components\n",
    "#     scale: # TODO: how can we set these in a generally meaningful way?\n",
    "#         high depth (more witnesses) is most important\n",
    "#         low frequency (less repetition) is next most important\n",
    "#         high length (token count) is least important\n",
    "#     higher numbers are better\n",
    "#         distance between neighboring values is irrelevant; all that matters is order\n",
    "#     '''\n",
    "#     # score = pow(block.witness_count,4) / (pow(block.frequency,3) * block.token_count)\n",
    "#     score = pow(block.witness_count,6)  * block.token_count / pow(block.frequency,3)\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def sort_blocks_by_priority (_blocks: List[Block]) -> List[tuple]:\n",
    "#     blocks_to_tuples = [(_block, index) for index, _block in enumerate(_blocks)]\n",
    "#     return sorted(blocks_to_tuples, key=lambda x: priority(x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prioritized_blocks = sort_blocks_by_priority(sorted_blocks) # sorted_blocks has been sorted for dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# witness_sigla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prioritized_blocks[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}