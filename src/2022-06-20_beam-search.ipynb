{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Traverse LCP, then MFS\n",
    "\n",
    "* Replace earlier masked-array strategy with regular arrays, using 0 to represent a null.\n",
    "* Real data (offset of token within witness) is one-based.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from linsuffarr import SuffixArray\n",
    "from linsuffarr import UNIT_BYTE\n",
    "import pprint\n",
    "import numpy as np\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from heapq import * # priority heap, https://docs.python.org/3/library/heapq.html\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "from bisect import bisect_right\n",
    "from IPython.display import display, HTML, SVG\n",
    "import graphviz\n",
    "from collections import deque\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigla = ['w0', 'w1', 'w2', 'w3', 'w4', 'w5']\n",
    "filenames = ['darwin/darwin1859.txt', 'darwin/darwin1860.txt', 'darwin/darwin1861.txt', 'darwin/darwin1866.txt', 'darwin/darwin1869.txt', 'darwin/darwin1872.txt']\n",
    "# sigla = ['w0', 'w1', 'w2', 'w3']\n",
    "# filenames = ['darwin/darwin1859.txt', 'darwin/darwin1860.txt', 'darwin/darwin1861.txt', 'darwin/darwin1866.txt']\n",
    "# sigla = ['w0', 'w1']\n",
    "# filenames = ['darwin1859.txt', 'darwin1860.txt']\n",
    "# sigla = ['w0', 'w1', 'w2', 'w3', 'w4', 'w5']\n",
    "# filenames = ['abc/abcd.txt', 'abc/abcda.txt', 'abc/abcdb.txt', 'abc/abcdc.txt', 'abc/abcdd.txt', 'abc/abcde.txt']\n",
    "first_paragraph = 0\n",
    "last_paragraph = 50\n",
    "how_many_paragraphs = last_paragraph - first_paragraph\n",
    "raw_data_dict = {}\n",
    "for siglum, filename in zip(sigla, filenames):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line for line in lines if line != '\\n']\n",
    "        raw_data_dict[siglum] = \" \".join(lines[first_paragraph : last_paragraph])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_witnesses(witness_strings: List[str]): # one string per witness\n",
    "    '''Return list of witnesses, each represented by a list of tokens'''\n",
    "    # TODO: handle punctuation, upper- vs lowercase\n",
    "    witnesses = []\n",
    "    for witness_string in witness_strings:\n",
    "        # witness_tokens = witness_string.split()\n",
    "        witness_tokens = re.findall(r'\\w+\\s*|\\W+', witness_string)\n",
    "        witness_tokens = [token.strip().lower() for token in witness_tokens]\n",
    "        witnesses.append(witness_tokens)\n",
    "    return witnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_token_array(_witness_token_lists): # list of token lists per witness\n",
    "    '''Create token array (single list, with separator \" # \" between witnesses'''\n",
    "    _token_array = [] # strings\n",
    "    _token_membership_array = [] # witness identifiers, same offsets as in token_array\n",
    "    _token_witness_offset_array = [] # one-based offset of token in witness\n",
    "    _last_witness_offset = len(_witness_token_lists) - 1\n",
    "    for _index, _witness_token_list in enumerate(_witness_token_lists):\n",
    "        print(\"Witness starts at =\", len(_token_array))\n",
    "        print(\"Witness ends at =\", len(_token_array) + len(_witness_token_list) - 1)\n",
    "        _token_array.extend(_witness_token_list)\n",
    "        for _token_offset, _token in enumerate(_witness_token_list): # don't need enumerate, just len()\n",
    "            _token_witness_offset_array.append(_token_offset)\n",
    "        _token_membership_array.extend([_index for _token in _witness_token_list])\n",
    "        if _index < _last_witness_offset:\n",
    "            _separator = \" #\" + str(_index + 1) + \" \"\n",
    "            _token_array.append(_separator)\n",
    "            _token_membership_array.append(_separator)\n",
    "            _token_witness_offset_array.append(-1)\n",
    "    return _token_array, _token_membership_array, _token_witness_offset_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "witness_sigla = [key for key in raw_data_dict.keys()]\n",
    "witnesses = tokenize_witnesses([value for value in raw_data_dict.values()]) # strings\n",
    "# token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Witness starts at = 0\n",
      "Witness ends at = 12789\n",
      "Witness starts at = 12791\n",
      "Witness ends at = 25640\n",
      "Witness starts at = 25642\n",
      "Witness ends at = 38698\n",
      "Witness starts at = 38700\n",
      "Witness ends at = 52017\n",
      "Witness starts at = 52019\n",
      "Witness ends at = 66248\n",
      "Witness starts at = 66250\n",
      "Witness ends at = 80425\n"
     ]
    }
   ],
   "source": [
    "token_array, token_membership_array, token_witness_offset_array = create_token_array(witnesses)\n",
    "# print(f\"{token_array=}\")\n",
    "# print(f\"{token_membership_array=}\")\n",
    "# print(f\"{token_witness_offset_array=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "when\n",
      "when\n",
      "when\n",
      "causes\n",
      "causes\n",
      "causes\n"
     ]
    }
   ],
   "source": [
    "# START HERE 2022-11-05\n",
    "# Verify that we are finding the start and end offsets of\n",
    "#   each witness in the global token array. We need these\n",
    "#   to specify the children of root when we expand it.\n",
    "# FOR SAT 2022-11-12\n",
    "#   Move finished (or, at least, temporarily finished) parts\n",
    "#   of the code out of the notebook\n",
    "for i in (12789, 25640, 38698, 52017, 66248, 80425):\n",
    "    print(token_array[i])\n",
    "for i in (0, 12791, 25642, 38700, 52019, 66250):\n",
    "    print(token_array[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suffix_array = SuffixArray(token_array, unit=UNIT_BYTE)\n",
    "# print(suffix_array)\n",
    "# LCP=0 means that the block has nothing in common with the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lcp_array = suffix_array._LCP_values\n",
    "if debug:\n",
    "    print(lcp_array[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create Block dataclass\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Block:\n",
    "    token_count: int\n",
    "    start_position: int # offset into suffix array (not into token array!)\n",
    "    end_position: int # start and end position give number of occurrences\n",
    "    all_start_positions: [] # compute after blocks have been completed\n",
    "    witnesses: set\n",
    "    witness_count: int # number of witnesses in which pattern occurs, omitted temporarily because requires further computation\n",
    "    frequency: int # number of times pattern occurs in whole witness set (may be more than once in a witness), end_position - start_position + 1\n",
    "    # how_created: int # debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Lcp_interval_candidate:\n",
    "    lcp_start_offset: int\n",
    "    lcp_interval_token_count: int\n",
    "    lcp_end_offset: int = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_for_depth_and_repetition(_suffix_array, _token_membership_array, _lcp_interval:Lcp_interval_candidate, _witness_count: int) -> bool:\n",
    "    \"\"\"Write a docstring someday\n",
    "\n",
    "    Number of prefixes >= total number of witnesses\n",
    "    Accumulate set of witness sigla for prefixes\n",
    "    if:\n",
    "        no witness occurs more than once, return True to keep this block\n",
    "    else:\n",
    "        return False\n",
    "    \"\"\"\n",
    "    block_instance_count = _lcp_interval.lcp_end_offset - _lcp_interval.lcp_start_offset + 1\n",
    "    if block_instance_count != _witness_count:\n",
    "        return False\n",
    "    else:\n",
    "        _witnesses_found = []\n",
    "        for _lcp_interval_item_offset in range(_lcp_interval.lcp_start_offset, _lcp_interval.lcp_end_offset + 1):\n",
    "            _token_position = _suffix_array.SA[_lcp_interval_item_offset] # point from prefix to suffix array position\n",
    "            _witness_siglum = _token_membership_array[_token_position] # point from token array position to witness identifier\n",
    "            if _witness_siglum in _witnesses_found:\n",
    "                return False\n",
    "            else:\n",
    "                _witnesses_found.append(_witness_siglum)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_blocks(_suffix_array, _token_membership_array, _witnesses, _lcp_array: list):\n",
    "    \"\"\"Write a docstring someday\n",
    "\n",
    "    Look at changes in length of LCP array\n",
    "    Initial value is 0 or -1 because it's a comparison with previous, and first has no previous\n",
    "    Next value is number of tokens shared with previous\n",
    "    Exact length doesn't matter, but if it changes, new pattern:\n",
    "        If it stays the same, take note but do nothing yet; it means that the pattern repeats\n",
    "        No change for a while, then goes to 0:\n",
    "            Number of repetitions plus 1, e.g., 5 5 5 0 = 4 instances of 5\n",
    "            Once it changes to 0, we've seen complete pattern\n",
    "        Changer to smaller means hidden, deeper block\n",
    "        Changes to longer means ???\n",
    "    \"\"\"\n",
    "    _accumulator = [] # lcp positions (not values) since most recent 0\n",
    "    _frequent_sequences = [] # lcp intervals to be considered for mfs\n",
    "    #\n",
    "    # lcp value\n",
    "    # if == 0 it's a new interval, so:\n",
    "    #   1. if there is already an accumulation, commit (process) it\n",
    "    #      \"committing the buffer\" means checking for repetition and depth\n",
    "    #          if it passes check: store in mfs list\n",
    "    #          otherwise throw it away\n",
    "    #   2. clear buffer (accumulator) and begin accumulating new buffer with the new offset with 0 value\n",
    "    # otherwise it isn't zero, so there must be a buffer in place, so add to it (for now)\n",
    "    for _offset, _value in enumerate(_lcp_array):\n",
    "        if not _accumulator and _value == 0: # if accumulator is empty and new value is 0, do nothing\n",
    "            continue\n",
    "        elif not _accumulator: # accumulator is empty and new value is non-zero, so begin new accumulator\n",
    "            _accumulator.append(Lcp_interval_candidate(lcp_start_offset = _offset - 1, lcp_interval_token_count = _value))\n",
    "        elif _value > _accumulator[-1].lcp_interval_token_count: # new interval, so add to accumulator and continue\n",
    "            _accumulator.append(Lcp_interval_candidate(lcp_start_offset = _offset - 1, lcp_interval_token_count = _value))\n",
    "        elif _value == _accumulator[-1].lcp_interval_token_count: # same block as before, so do nothing\n",
    "            continue\n",
    "        else: # new value is less than top of accumulator, so pop everything that is higher\n",
    "            # Positions in lcp array and suffix array coincide:\n",
    "            #   The lcp array value is the length of the sequence\n",
    "            #   The suffix array value is the start position of the sequence\n",
    "            # Assume accumulator values (offsets into lcp array) point to [3, 6] and new value is 4, so:\n",
    "            #   First: Pop pointer to 6 (length value in lcp array), store in frequent_sequences\n",
    "            #   Second: Push new pointer to same position in lcp array, but change value in lcp array to 4\n",
    "            while _accumulator and _accumulator[-1].lcp_interval_token_count > _value:\n",
    "                # Create pointer to last closed block that is not filtered (like frequent_sequences)\n",
    "                _newly_closed_block = _accumulator.pop()\n",
    "                _newly_closed_block.lcp_end_offset = _offset - 1\n",
    "                if check_for_depth_and_repetition(_suffix_array, _token_membership_array, _newly_closed_block, len(_witnesses)):\n",
    "                    _frequent_sequences.append([_newly_closed_block.lcp_start_offset, _newly_closed_block.lcp_end_offset, _newly_closed_block.lcp_interval_token_count])\n",
    "            # There are three options:\n",
    "            #   1. there is content in the accumulator and latest value is not 0\n",
    "            #   2. accumulator is empty and latest value is 0\n",
    "            #   3. accumulator is empty and latest value is not 0\n",
    "            # (the fourth logical combination, content in the accumulator and 0 value, cannot occur\n",
    "            #     because a 0 value will empty the accumulator)\n",
    "            if _value > 0 and (not _accumulator or _accumulator[-1].lcp_interval_token_count != _value):\n",
    "                _accumulator.append(Lcp_interval_candidate(lcp_start_offset = _newly_closed_block.lcp_start_offset, lcp_interval_token_count = _value))\n",
    "    # End of lcp array; run through any residual accumulator values\n",
    "    while _accumulator:\n",
    "        _newly_closed_block = _accumulator.pop()\n",
    "        _newly_closed_block.lcp_end_offset = len(_lcp_array) - 1\n",
    "        if check_for_depth_and_repetition(_suffix_array, _token_membership_array, _newly_closed_block, len(witnesses)):\n",
    "            _frequent_sequences.append([_newly_closed_block.lcp_start_offset, len(_lcp_array)-1, _newly_closed_block.lcp_interval_token_count])\n",
    "    return _frequent_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frequent_sequences is a list of lists\n",
    "# the embedded lists contain LCP indices\n",
    "#   LCP indices point into LCP array, but same index also points into suffix array\n",
    "#   value in LCP array points to prefix length (compared to previous one)\n",
    "#   value in suffix array points into token array\n",
    "frequent_sequences = create_blocks(suffix_array, token_membership_array, witnesses, lcp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To remove embedded prefixes:\n",
    "#\n",
    "# 1. Create dictionary with end position in witness 0 (arbitrarily) as key\n",
    "# 2. Set value of key to longest sequence with that end position\n",
    "# 3. Dictionary values will contain only longest frequent sequences, removing embedded ones,\n",
    "#    as tuples if (length, [token start positions for all witnesses])\n",
    "\n",
    "@dataclass\n",
    "class LongestSequence:\n",
    "    length: int\n",
    "    witness_start_and_end: List[int]\n",
    "\n",
    "def find_longest_sequences(_frequent_sequences, _suffix_array):\n",
    "    _largest_blocks = {} # key is token end position, value is (length, [witness-start-positions])\n",
    "    for _frequent_sequence in _frequent_sequences:\n",
    "        _length = _frequent_sequence[2]\n",
    "        _suffix_array_values = [_suffix_array.SA[i] for i in range(_frequent_sequence[0], _frequent_sequence[1] + 1)]\n",
    "        _token_end_position = min(_suffix_array_values) + _length # token end position for first witness\n",
    "        if _token_end_position not in _largest_blocks: # first block with this end position, so create new key\n",
    "            _largest_blocks[_token_end_position] = (_length, sorted(_suffix_array_values))\n",
    "        else: # if new block is longer, replace old one with same key\n",
    "            if _length > _largest_blocks[_token_end_position][0]:\n",
    "                _largest_blocks[_token_end_position] = (_length, sorted(_suffix_array_values))\n",
    "    return _largest_blocks\n",
    "\n",
    "largest_blocks = find_longest_sequences(frequent_sequences, suffix_array)\n",
    "if debug:\n",
    "    print(f\"{largest_blocks=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_for_beam_search(_witnesses, _token_membership_array, _largest_blocks):\n",
    "    # block_offsets_by_witness: list of lists holds sorted start offsets per witness (offsets are into global token array)\n",
    "    # witness_offsets_to_blocks: dictionary points from start offsets to blocks\n",
    "    # score_by_block: number of tokens placed or skipped if block is placed\n",
    "    # Beam search requires us, given an offset in a witness, to find the next block. We do\n",
    "    #   that by looking up the value in block_offsets_by_witness and then using that value\n",
    "    #   to retrieve the block key from witness_offsets_to_blocks\n",
    "    # Lookup in the list of lists is:\n",
    "    #   block_offsets_by_witness[witness_number][bisect_right(block_offsets_by_witness[witness_number], most_recent_offset_in_witness)]\n",
    "    # (See: https://www.geeksforgeeks.org/python-find-smallest-element-greater-than-k/)\n",
    "    # FIXME: traverse largest_blocks only once and add values for all witnesses in same pass\n",
    "    _witness_count = len(_witnesses)\n",
    "    _block_offsets_by_witness = []\n",
    "    _witness_offsets_to_blocks = {}\n",
    "    _first_token_offset_in_block_by_witness = [] # only tokens in blocks\n",
    "    _first_absolute_token_by_witness = [] # all tokens, whether in block or not\n",
    "    for i in range(_witness_count):\n",
    "        _first_token_offset_in_block_by_witness.append(_token_membership_array.index(i))\n",
    "        # Score = number of tokens either placed or skipped (we don't care which)\n",
    "        # Low score is best because it leaves the highest potential\n",
    "        # NB: The name \"score\" seems to imply that higher is better, and the\n",
    "        #   opposite is the case here. Rename the variable?\n",
    "        # NB: High potential is paramount during beam search, but should the\n",
    "        #   difference between placed and skip matter at a later stage? Or\n",
    "        #   does placing more blocks (more tiers) take care of that?\n",
    "        _score_by_block = {}\n",
    "        for i in range(_witness_count):\n",
    "            _witness_offset_list = []\n",
    "            for _key, _value in _largest_blocks.items():\n",
    "                _witness_offset_list.append(_value[1][i])\n",
    "                _witness_offsets_to_blocks[_value[1][i]] = _key\n",
    "            _witness_offset_list.sort()\n",
    "            _block_offsets_by_witness.append(_witness_offset_list)\n",
    "    for i in range(_witness_count):\n",
    "        _first_absolute_token_by_witness.append(_token_membership_array.index(i))\n",
    "    for _key, _value in _largest_blocks.items():\n",
    "        # to determine number of tokens that will have been placed or skipped\n",
    "        #   after placing block:\n",
    "        #       matrix-subtract first_token_offset_by_witness from value[1]\n",
    "        #       add witness_count * value[0] (to account for block length)\n",
    "        #   key by block key, value is score\n",
    "        _differences = [x - y for x, y in zip(_value[1], _first_token_offset_in_block_by_witness)]\n",
    "        if debug:\n",
    "            print(_differences)\n",
    "        _score = sum(_differences) + _witness_count * _value[0]\n",
    "        _score_by_block[_key] = _score\n",
    "    if debug:\n",
    "        print(f\"{_block_offsets_by_witness=}\")\n",
    "        print()\n",
    "        _witness_offsets_to_blocks = { key: _witness_offsets_to_blocks[_key] for _key in sorted(_witness_offsets_to_blocks.keys())}\n",
    "        print(f\"{_witness_offsets_to_blocks=}\")\n",
    "        print()\n",
    "        print(f\"{_first_token_offset_in_block_by_witness=}\")\n",
    "        print()\n",
    "        print(f\"{_first_absolute_token_by_witness=}\")\n",
    "        print()\n",
    "        print(f\"{_score_by_block=}\")\n",
    "    return _block_offsets_by_witness, _witness_offsets_to_blocks, _first_token_offset_in_block_by_witness, _first_absolute_token_by_witness, _score_by_block\n",
    "\n",
    "block_offsets_by_witness, witness_offsets_to_blocks, first_token_offset_in_block_by_witness, first_absolute_token_by_witness, score_by_block \\\n",
    "    = prepare_for_beam_search(witnesses, token_membership_array, largest_blocks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To perform beam search\n",
    "#   Create single start option (at Start node, which is a fiction [there is no Start block]\n",
    "#       created for the beam search)\n",
    "#   Loop: for each BeamOption on current tier\n",
    "#       Evaluate score for advancing in each witness and bringing others into alignment with it\n",
    "#       For β lowest (!) scores create new BeamOption (this advances to next tier)\n",
    "#           Score is count of tokens placed or skipped (!)\n",
    "#           Favor lowest score because that has the greatest potential"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass(order=True, frozen=True, eq=True) # heapqueue is priority queue, so requires comparison\n",
    "class BeamOption:\n",
    "    score: int\n",
    "    path: tuple # path through sequence of blocks leading to current BeamOption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create initial BeamOption\n",
    "initial = [BeamOption(score=0, path=())] # tier 0, one-item list\n",
    "def perform_beam_search_step(_witnesses, _largest_blocks, _block_offsets_by_witness, _witness_offsets_to_blocks, _score_by_block, _beam_options=initial, _beta=3):\n",
    "    # TODO: The witness count should be a global constant\n",
    "    # print(\"New tier with \" + str(len(beam_options)) + \" beam options\")\n",
    "    _new_options = [] # candidates for next tier\n",
    "    _finished_options = []\n",
    "    for _beam_option in _beam_options:\n",
    "        # ###\n",
    "        # 2022-09-06\n",
    "        # Three possibilities for an individual beam option:\n",
    "        # 1. Option leads to new option\n",
    "        # 2. Option is finished\n",
    "        # 3. No new option but option isn't finished (transposition)\n",
    "        # NB: We check each witness in the beam option, and if any witness\n",
    "        #     raises an IndexError, the whole block cannot be advanced and\n",
    "        #     is finished. (This is true because of our constraints: every\n",
    "        #     block is a) full-depth and b) no repetition.)\n",
    "        #\n",
    "        # What to do:\n",
    "        #\n",
    "        # 1. Perform the bisect for each witness based on the head of the\n",
    "        #    path of the current beam option. This returns an offset into\n",
    "        #    the witness-specific list of block offsets. Initialize a\n",
    "        #    counter to 0.\n",
    "        # 2. Using the initial offsets returned by the bisect operation that\n",
    "        #    we performed in step #1 (and never perform again) for each witness\n",
    "        #    plus the counter (which we will increment if needed in the inner\n",
    "        #    loop), check that next option for each witness. There are three\n",
    "        #    possibilities for each counter value (over the entire witness group):\n",
    "        #    a) If the next block (returned by this method) would overrun\n",
    "        #       for any witness, it will overrun for all witnesses, so the\n",
    "        #       beam option can be added to the finished list and we exit the\n",
    "        #       outer loop (the one that processes the beam option).\n",
    "        #    b) If the next block is a viable option, add it to the options and\n",
    "        #       check the next witness within this same inner loop instance\n",
    "        #       because in case of transposition different blocks will suggest\n",
    "        #       different next blocks, all of which could be viable options.\n",
    "        #       This ends the processing for that beam option.\n",
    "        #    c) If we don't find any viable next option and don't overrun for\n",
    "        #       any witness, increment the counter and replay step #2 (inner\n",
    "        #       loop).\n",
    "        # Exit condition: Eventually we either find a viable option or overrun.\n",
    "        #\n",
    "        # TODO: How should we implement this to terminate the correct loop in\n",
    "        # the right place? For? While? Generator? For and while start with the\n",
    "        # outer loop and work inward; with a generator we start with the inner\n",
    "        # and work outward.\n",
    "        # ###\n",
    "        _witness_count = len(_witnesses)\n",
    "        _new_finished_option_check = False\n",
    "        _new_viable_option_check = 0\n",
    "        _counter = 0\n",
    "        while True:\n",
    "            for i in range(_witness_count): # advance for each witness in turn\n",
    "                if not _beam_option.path: # path is empty only for initial state at tier 0\n",
    "                    _last_offset = -1 # NB: same for all witnesses, and not 0, which will break for witness 0\n",
    "                else:\n",
    "                    _last_offset = _largest_blocks[_beam_option.path[0]][1][i]\n",
    "                try:\n",
    "                    _next_offset = bisect_right(_block_offsets_by_witness[i], _last_offset)\n",
    "                    _next_value = _block_offsets_by_witness[i][_next_offset + _counter]\n",
    "                    _next_block = _witness_offsets_to_blocks[_next_value] # find that next block to get its length\n",
    "                    # would any witness pointer move backwards?\n",
    "                    # perform matrix subtraction; if signs differ, there are items that move in opposite directions\n",
    "                    # first option cannot be transposed, so accept it automatically\n",
    "                    if (not _beam_option.path) or (len(set([np.sign(x - y) for x, y in zip(_largest_blocks[_next_block][1], _largest_blocks[_beam_option.path[0]][1])])) == 1):\n",
    "                        _new_score = _score_by_block[_next_block] # accounts for all witnesses\n",
    "                        # concatenate tuples with a +;  most recent first (for priority heap)\n",
    "                        _new_options.append(BeamOption(score = _new_score, path=((_next_block,) + _beam_option.path)))\n",
    "                        _new_viable_option_check += 1\n",
    "                    else:\n",
    "                        continue\n",
    "                        # print('Transposition detected for beam option:', beam_option)\n",
    "                except IndexError: # we've gone as far as we can with this path\n",
    "                    _new_finished_option_check = True\n",
    "                    _finished_options.append(_beam_option)\n",
    "                    break # if one witness overruns, they all will, so this beam option is done\n",
    "            if _new_viable_option_check >= _witness_count or _new_finished_option_check:\n",
    "                break\n",
    "            _counter += 1\n",
    "    _new_options = list(set(_new_options)) # deduplicate\n",
    "    heapify(_new_options) # sort from low score to high (low score is best)\n",
    "    # print(_beam_options)\n",
    "    if not _new_options and not _finished_options:\n",
    "        raise Exception(\"This shouldn't happen: no new options and no finished options\")\n",
    "    else:\n",
    "        return _new_options[:_beta], _finished_options"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "options, _ = perform_beam_search_step(witnesses, largest_blocks, block_offsets_by_witness, witness_offsets_to_blocks, score_by_block)\n",
    "finished = [] # options that cannot go further\n",
    "counter = 0\n",
    "while options: # no more options means that we're done\n",
    "    # TODO: The beam size at the moment is a magic number; can we rationalize it?\n",
    "    options, end_of_life = perform_beam_search_step(witnesses, largest_blocks, block_offsets_by_witness, witness_offsets_to_blocks, score_by_block, _beam_options=options, _beta=20)\n",
    "    finished.extend(end_of_life) # add any options that cannot go further\n",
    "    print(counter, len(options), len(finished))\n",
    "    counter += 1\n",
    "finished = list(set(finished)) # TODO: Remove this because we'll sort later?\n",
    "# TODO: Verify that better scores are better alignments (how??)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# finished holds beam options that cannot go further, with duplicates removed\n",
    "# BeamOption.score counts tokens placed or skipped, which is correct for traversing, but\n",
    "#   for evaluation we count only most tokens placed and sub-sort by fewest blocks\n",
    "# Blocks know their length, so we sum the lengths of the finalists and keep only the highest\n",
    "# NB: There could be more than one\n",
    "finished.sort(reverse = True, key = lambda f: (sum([largest_blocks[b][0] for b in f.path]), -1 * len(f.path)))\n",
    "if debug:\n",
    "    for pos, f in enumerate(finished):\n",
    "        print(pos, sum([largest_blocks[b][0] for b in f.path]), len(f.path))\n",
    "        print(f)\n",
    "# TODO: Adaptive beam width? We can't evaluate the consequences of a suboptimal\n",
    "# first-pass alignment until we follow through to a full alignment. It could be\n",
    "# that saving time by sacrificing a small improvement in the first pass won't\n",
    "# affect the outcome because we'll inevitably and at no addition cost fix it\n",
    "# later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FIXME 2022-09-20:\n",
    "# We find unaligned tokens by looking between blocks, as well as leading unaligned tokens.\n",
    "# TODO:\n",
    "#  1. We don't find unaligned tokens after the last block.\n",
    "#  2. We don't test what happens if there are no unaligned tokens before the first block.\n",
    "table_top = \"\"\"\n",
    "    <html>\n",
    "        <head>\n",
    "            <style type=\"text/css\">\n",
    "                table, tr, th, td {border: 1px solid black; border-collapse: collapse;}\n",
    "                th, td {padding: 3px;}\n",
    "                td:first-child {text-align: right;}\n",
    "            </style></head><body><table><tr style=\"background-color: pink;\"><th>Row</th>\n",
    "    \"\"\" + '\\n'.join(['<th style=\"border: 1px black solid; border-collapse: collapse; text-align: center;\">w' + str(i) + '</th>' for i in range(len(witnesses))]) + '</tr>'\n",
    "table_bottom = '</table></body></html>'\n",
    "\n",
    "block0_start_positions = largest_blocks[finished[0].path[-1]][1]\n",
    "# if debug:\n",
    "#     print(block0_start_positions)\n",
    "#     print(first_absolute_token_by_witness)\n",
    "if block0_start_positions != first_absolute_token_by_witness:\n",
    "    leading_unaligned_tokens = ['<td>' + \" \".join(token_array[i: j]) + '</td>' for i, j in zip(first_absolute_token_by_witness, block0_start_positions)]\n",
    "    leading_unaligned_row = '<tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td>' + \"\".join(leading_unaligned_tokens) + '</tr>'\n",
    "\n",
    "rows = []\n",
    "# Rows with aligned tokens are the same in all witness by definition\n",
    "# The path contains largest_blocks keys, which represent the last token of\n",
    "#   a block in witness 0\n",
    "# The value of a block is a tuple, the first member of which is the length\n",
    "# We can retrieve the aligned tokens by slicing them from the token_array\n",
    "for index, end_token_offset in enumerate(finished[0].path[::-1]): # path is ordered from last to first\n",
    "    # ###\n",
    "    # Information for aligned block\n",
    "    # This is the same for all witnesses, taken from witness 0\n",
    "    # ###\n",
    "    block_length = largest_blocks[end_token_offset][0]\n",
    "    start_token_offset = end_token_offset - block_length\n",
    "    tokens = token_array[start_token_offset: end_token_offset]\n",
    "    # ###\n",
    "    # Information for preceding non-aligned block\n",
    "    # This is different for each witness\n",
    "    #\n",
    "    # Loop over witnesses using range(len(witnesses))\n",
    "    # Get start token offset for aligned block for current witness\n",
    "    # Get end token offset for preceding aligned block for current witness\n",
    "    # Get tokens by slicing token array\n",
    "    # ###\n",
    "    if index > 0:\n",
    "        current_block = largest_blocks[end_token_offset]\n",
    "        preceding_block = largest_blocks[finished[0].path[::-1][index - 1]]\n",
    "        unaligned_row = []\n",
    "        unaligned_row.append('<tr style=\"background-color: lightgray; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink;\">unaligned</td>')\n",
    "        for i in range(len(witnesses)):\n",
    "            unaligned_start_token_offset = preceding_block[1][i] + preceding_block[0]\n",
    "            unaligned_end_token_offset = current_block[1][i] - 1\n",
    "            unaligned_tokens = token_array[unaligned_start_token_offset: unaligned_end_token_offset + 1]\n",
    "            unaligned_row.append('<td style=\"border: 1px black solid; border-collapse: collapse;\">' + \" \".join(unaligned_tokens) + '</td>')\n",
    "        unaligned_row.append('</tr>')\n",
    "        rows.append(\"\".join(unaligned_row))\n",
    "    # ###\n",
    "    # Create aligned block\n",
    "    # ###\n",
    "    rows.append('<tr style=\"background-color: beige; border: 1px black solid; border-collapse: collapse;\"><td style=\"background-color: pink; border: 1px black solid; border-collapse: collapse;\">' + str(index) + ' (' + str(end_token_offset) + ')</td><td  style=\"border: 1px black solid; border-collapse: collapse;\" colspan=\"' + str(len(witnesses)) + '\">' + \" \".join(tokens) + '</td></tr>')\n",
    "table = table_top + leading_unaligned_row + \"\".join(rows) + table_bottom\n",
    "with open('table-output.html', 'w') as f:\n",
    "    f.write(table)\n",
    "HTML(table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Create hybrid graph\n",
    "# # Aligned blocks are a single node with (except if they are initial or last)\n",
    "# #   separate in- and out-edges for each witness.\n",
    "# # Unaligned blocks are separate nodes for each witness that contained all\n",
    "# #   unaligned cells at that location.\n",
    "# # The aligned blocks are treated as if they were in a variant graph.\n",
    "# # Unaligned blocks can then be processed to look for new alignments (made\n",
    "# #   possible because the first pass removed repetition. Initially we can\n",
    "# #   complete the graph after the initial pass and then traverse it to\n",
    "# #   process the unaligned portions. Eventually we should process them in\n",
    "# #   place, recursively, as we construct the graph.\n",
    "# # Aligned blocks have node numbers a0, a1, a2, etc.\n",
    "# # Unaligned token sequences have node numbers u0, u1, u2, etc.\n",
    "# # START node is numbered -1\n",
    "# # END node is numbered\n",
    "#\n",
    "# # Initialize graph\n",
    "# a = graphviz.Digraph(format=\"svg\", graph_attr={'rankdir': 'LR'})\n",
    "# unaligned_node_no = -1 # Node id values are integers except for START and END\n",
    "# aligned_node_no = 0\n",
    "# preceding_aligned_node_no = -1\n",
    "# a.node(\"\".join(('a', str(aligned_node_no))), \"\".join(('a', str(aligned_node_no))) + ': ' + 'START')\n",
    "#\n",
    "# # Blocks are ordered backwards, so start from end\n",
    "# block0_start_positions = largest_blocks[finished[0].path[-1]][1]\n",
    "#\n",
    "# # Add all aligned blocks as nodes\n",
    "# for index, end_token_offset in enumerate(finished[0].path[::-1]):\n",
    "#     # For aligned block increment node, aligned_node, and preceding_aligned_node\n",
    "#     aligned_node_no += 1\n",
    "#     aligned_id = \"\".join(('a', str(aligned_node_no)))\n",
    "#     preceding_aligned_node_no += 1\n",
    "#     preceding_aligned_id = \"\".join(('a', str(preceding_aligned_node_no)))\n",
    "#     # Same tokens for all witnesses, so get them from witness 0\n",
    "#     block_length = largest_blocks[end_token_offset][0]\n",
    "#     start_token_offset = end_token_offset - block_length\n",
    "#     tokens = token_array[start_token_offset: end_token_offset]\n",
    "#     a.node(aligned_id, aligned_id + \": \" + \" \".join(tokens))\n",
    "#\n",
    "#     # Check for initial unaligned tokens\n",
    "#     # TODO: We assume leading tokens, and probably don't handle their absence correctly\n",
    "#     # TODO: Move this out of the loop; it can happen only once\n",
    "#     if aligned_node_no == 1: # only for first aligned block\n",
    "#         for i, j in zip(first_absolute_token_by_witness, block0_start_positions):\n",
    "#             unaligned_node_no += 1\n",
    "#             unaligned_id = \"\".join(('u', str(unaligned_node_no)))\n",
    "#             unaligned_tokens = \" \".join(token_array[i: j+1])\n",
    "#             a.node(unaligned_id, unaligned_id + ': ' + unaligned_tokens)\n",
    "#             a.edge(preceding_aligned_id, unaligned_id)\n",
    "#             a.edge(unaligned_id, aligned_id)\n",
    "#\n",
    "#     # Add unaligned tokens between last aligned block (preceding_node) and new one (node_id)\n",
    "#     if index > 0:\n",
    "#         current_block = largest_blocks[end_token_offset]\n",
    "#         preceding_block = largest_blocks[finished[0].path[::-1][index - 1]]\n",
    "#         for i in range(len(witnesses)):\n",
    "#             unaligned_node_no += 1\n",
    "#             unaligned_start_token_offset = preceding_block[1][i] + preceding_block[0]\n",
    "#             unaligned_end_token_offset = current_block[1][i] - 1\n",
    "#             unaligned_tokens = token_array[unaligned_start_token_offset: unaligned_end_token_offset + 1]\n",
    "#             unaligned_id = \"\".join(('u', str(unaligned_node_no)))\n",
    "#             a.node(unaligned_id, unaligned_id + \": \" + \" \".join(unaligned_tokens))\n",
    "#             a.edge(preceding_aligned_id, unaligned_id)\n",
    "#             a.edge(unaligned_id, aligned_id)\n",
    "#\n",
    "# # Add END node\n",
    "# # TODO: We assume no trailing tokens and don't look for or handle them\n",
    "# # TODO: Edge connections for END node will have to be revised once we\n",
    "# #   also deal with trailing unaligned tokens\n",
    "# aligned_node_no += 1\n",
    "# aligned_id = \"\".join(('a', str(aligned_node_no)))\n",
    "# preceding_aligned_node_no += 1\n",
    "# preceding_aligned_id = \"\".join(('a', str(preceding_aligned_node_no)))\n",
    "# a.node(aligned_id, aligned_id + \": \" + 'END')\n",
    "# a.edge(preceding_aligned_id, aligned_id)\n",
    "#\n",
    "# # Render graph\n",
    "# svg = a.render()\n",
    "# # display(SVG(svg))\n",
    "#\n",
    "# # Traverse aligned and unaligned intervals as we do with alignment table, above,\n",
    "# #   creating nodes and edges as we proceed.\n",
    "# # TODO: Currently we assume unaligned tokens at the beginning and no unaligned\n",
    "# #   tokens at the end. This is correct for our sample but not reliable for\n",
    "# #   arbitrary input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create tree to represent alignment\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Nodes of all types have integer id properties\"\"\"\n",
    "    id: int\n",
    "\n",
    "@dataclass\n",
    "class Branching_node(Node):\n",
    "    \"\"\"Branching nodes have children, either a list of nodes\n",
    "    or a list of two-item tuples, one per witness, for slicing\n",
    "    into the original token array\n",
    "    \"\"\"\n",
    "    children: list\n",
    "    processed: bool\n",
    "    absolute_offsets: list\n",
    "\n",
    "@dataclass\n",
    "class Leaf_node(Node):\n",
    "    \"\"\"Leaf nodes may be aligned or non-aligned and have string values\"\"\"\n",
    "    string: str\n",
    "    aligned: bool\n",
    "\n",
    "def traverse_tree(r):\n",
    "    \"\"\"Depth-first traversal, return list of all nodes\n",
    "    TODO: Replace list with generator?\n",
    "\n",
    "    Push root (input) onto stack (just once)\n",
    "    Loop: Pop top of stack, process, push all children onto stack\n",
    "        NB: If children are A, B, C, A should be leftmost after being added\n",
    "    Exit when stack is empty\n",
    "    \"\"\"\n",
    "    _l = [] # list of nodes to return\n",
    "    _s = deque() # extendleft() and popleft() to manipulate\n",
    "    _s.appendleft(r) # root is special case\n",
    "    while _s:\n",
    "        current_node = _s.popleft()\n",
    "        _l.append(current_node)\n",
    "        if isinstance(current_node, Branching_node):\n",
    "            # sequence of children should be added with leftmost at left edge\n",
    "            # extendleft() reverses, so we reverse ourselves to retain original order\n",
    "            _s.extendleft(current_node.children[::-1])\n",
    "    return _l\n",
    "\n",
    "# create tree root and make nodes accessible by id property\n",
    "node_by_id = {}\n",
    "root_children = []\n",
    "root = Branching_node(id=0, processed=True, children=[], absolute_offsets=[])\n",
    "node_by_id[0] = root\n",
    "id = 0\n",
    "\n",
    "# # First tier\n",
    "# # Add branching node before first aligned node, if needed\n",
    "# block0_start_positions = largest_blocks[finished[0].path[-1]][1]\n",
    "# if block0_start_positions != first_absolute_token_by_witness:\n",
    "#     id += 1\n",
    "#     leading_unaligned_tokens = [(i, j) for i, j in zip(first_absolute_token_by_witness, block0_start_positions)]\n",
    "#     root.children.append(Branching_node(id=id, processed=False, children=leading_unaligned_tokens))\n",
    "#\n",
    "# # Add all aligned (leaf) nodes to root node,\n",
    "# # each preceded by an unaligned branching node\n",
    "# for index, end_token_offset in enumerate(finished[0].path[::-1]):\n",
    "#     # Add branching node for unaligned tokens\n",
    "#     if index > 0:\n",
    "#         id += 1\n",
    "#         unaligned_pointers = []\n",
    "#         current_block = largest_blocks[end_token_offset]\n",
    "#         preceding_block = largest_blocks[finished[0].path[::-1][index - 1]]\n",
    "#         for i in range(len(witnesses)):\n",
    "#             unaligned_start_token_offset = preceding_block[1][i] + preceding_block[0]\n",
    "#             unaligned_end_token_offset = current_block[1][i]\n",
    "#             unaligned_pointers.append((unaligned_start_token_offset, unaligned_end_token_offset))\n",
    "#         root.children.append(Branching_node(id=id, processed=False, children=unaligned_pointers))\n",
    "#     # Add leaf node for aligned tokens\n",
    "#     id += 1\n",
    "#     block_length = largest_blocks[end_token_offset][0]\n",
    "#     start_token_offset = end_token_offset - block_length\n",
    "#     tokens = token_array[start_token_offset: end_token_offset]\n",
    "#     root.children.append(Leaf_node(id=id, aligned=True, string=\" \".join(tokens)))\n",
    "\n",
    "def expand_branching_node(_parent, _largest_blocks, _finished, _first_absolute_token_by_witness, _witnesses, _original_offsets):\n",
    "    print(f\"{_finished[0]=}\")\n",
    "    # id is global unique node identifier (FIXME: replace with closure [or monad?])\n",
    "    # _parent is node to expand, i.e., parent of nodes we create here\n",
    "    # 1. Add branching node before first aligned node, if needed (optional only for tier 0)\n",
    "    # 2. Add all non-branching and branching nodes\n",
    "    # 3. Add branching node after last aligned node, if needed (optional only for tier 0)\n",
    "    # Void; modifies tree in place\n",
    "    global id\n",
    "    _block0_start_positions = _largest_blocks[_finished[0].path[-1]][1]\n",
    "    _block0_end_positions = [_largest_blocks[_finished[0].path[-1]][1][i] + _largest_blocks[_finished[0].path[-1]][0] for i in range(len(_witnesses))]\n",
    "    _global_positions = []\n",
    "    for i, j in _original_offsets:\n",
    "        _global_positions.extend(range(i, j))\n",
    "        _global_positions.append('separator') # TODO: not needed after last tuple\n",
    "    print(f\"{_largest_blocks[_finished[0].path[-1]]=}\")\n",
    "    print(f\"{_block0_start_positions=}\")\n",
    "    print(f\"{_block0_end_positions=}\")\n",
    "    print(f\"{_original_offsets=}\")\n",
    "    print(f\"{_global_positions=}\")\n",
    "    _global_tokens = []\n",
    "    for t in _global_positions:\n",
    "        if isinstance(t, int):\n",
    "            _global_tokens.append(token_array[t])\n",
    "        else:\n",
    "            _global_tokens.append(t)\n",
    "    print(f\"{_global_tokens=}\")\n",
    "\n",
    "    # Add branching node before first aligned node, if needed (optional only for tier 0)\n",
    "    if _block0_start_positions != _first_absolute_token_by_witness:\n",
    "        id += 1\n",
    "        _leading_unaligned_tokens = [(i, j) for i, j in zip(_first_absolute_token_by_witness, _block0_start_positions)]\n",
    "        _global_leading_unaligned_tokens = []\n",
    "        for t, u in _leading_unaligned_tokens:\n",
    "            try:\n",
    "                _global_leading_unaligned_tokens.append((_global_positions[t], _global_positions[u]))\n",
    "            except:\n",
    "                print(\"Eek!\", t, u, _leading_unaligned_tokens)\n",
    "        print(f\"{_global_leading_unaligned_tokens=}\")\n",
    "        _new_node = Branching_node(id=id, processed=False, children=_global_leading_unaligned_tokens, absolute_offsets=_original_offsets[0:len(_witnesses)])\n",
    "        _parent.children.append(_new_node)\n",
    "        node_by_id[id] = _new_node\n",
    "\n",
    "    # Add all aligned (leaf) nodes to root node,\n",
    "    # each preceded by an unaligned branching node\n",
    "    for _index, _end_token_offset in enumerate(_finished[0].path[::-1]):\n",
    "        # Add branching node for unaligned tokens\n",
    "        if _index > 0:\n",
    "            id += 1\n",
    "            _unaligned_pointers = []\n",
    "            _current_block = _largest_blocks[_end_token_offset]\n",
    "            _preceding_block = _largest_blocks[_finished[0].path[::-1][_index - 1]]\n",
    "            for i in range(len(_witnesses)):\n",
    "                # FIXME: Replace value of children with pointers into global (not local) token array\n",
    "                _unaligned_start_token_offset = _preceding_block[1][i] + _preceding_block[0]\n",
    "                _unaligned_end_token_offset = _current_block[1][i]\n",
    "                _unaligned_pointers.append((_unaligned_start_token_offset, _unaligned_end_token_offset))\n",
    "            _new_node = Branching_node(id=id, processed=False, children=_unaligned_pointers, absolute_offsets=_original_offsets[0:len(_witnesses)])\n",
    "            node_by_id[id] = _new_node\n",
    "            _parent.children.append(_new_node)\n",
    "        # Add leaf node for aligned tokens\n",
    "        id += 1\n",
    "        _block_length = _largest_blocks[_end_token_offset][0]\n",
    "        _start_token_offset = _end_token_offset - _block_length\n",
    "        _tokens = token_array[_start_token_offset: _end_token_offset]\n",
    "        _new_node = Leaf_node(id=id, aligned=True, string=\" \".join(_tokens))\n",
    "        node_by_id[id] = _new_node\n",
    "        _parent.children.append(_new_node)\n",
    "\n",
    "    # Add branching node after last aligned node:\n",
    "    #   START HERE 2022-11-01:\n",
    "    #       To process trailing tokens we need to find start and\n",
    "    #           end offsets for each witness\n",
    "    #       We currently find end offset for branching node we're expanding\n",
    "    #       To find start offset for trailing tokens we try (!) to get\n",
    "    #           end offset of preceding branching node (there must be one)\n",
    "    #           plus length of intervening leaf node plus 1\n",
    "    #           The start offset must be less than the end node\n",
    "    #   TODO: Optional for tier 0 because it can end in an alignment\n",
    "    #   Optional for other tiers because if there is no alignment,\n",
    "    #       we’ve already added this as the unaligned beginning\n",
    "    #       BUT we should never see these because we only invoke\n",
    "    #       the expand_branching_node() function if there are blocks\n",
    "    # Start position of trailing unaligned tokens is start position of\n",
    "    #   last aligned ones (which we get from:\n",
    "    #       end position of last unaligned + 1\n",
    "    #           + length of last aligned\n",
    "    _last_leaf_length = len(_parent.children[-1].string.split())\n",
    "    _trailing_start_positions = [_parent.children[-2].children[i][1] + 1 + _last_leaf_length for i in range(len(_witnesses))]\n",
    "    print(f\"{_trailing_start_positions=}\")\n",
    "    # id += 1\n",
    "    # _trailing_token_offsets = [(_trailing_start_positions[i], _block0_end_positions[i]) for i in range(len(_witnesses))]\n",
    "    # _new_node = Branching_node(id=id, processed=False, children=_trailing_token_offsets, absolute_offsets=_original_offsets[0:len(_witnesses)])\n",
    "    # _parent.children.append(_new_node)\n",
    "\n",
    "expand_branching_node(root, largest_blocks, finished, first_absolute_token_by_witness, witnesses, root.children)\n",
    "# 2022-10-18 Resume here\n",
    "# Traverse tree tier by tier\n",
    "# Expand branching nodes where the value of \"processed\" is False\n",
    "#   and toggle value of \"processed\" to True\n",
    "# TODO: Some current parameters (e.g., count of witnesses) should be global constants."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = traverse_tree(root)\n",
    "for node in result:\n",
    "    # only branching nodes have a processed property\n",
    "    if isinstance(node, Branching_node) and not node.processed:\n",
    "        # Micro-witnesses might have no full-depth blocks. If so, expand into\n",
    "        # leaf nodes and don't initiate beam search\n",
    "        # FIXME: This could happen, at least in theory, on tier 0 if there are\n",
    "        #   no full-depth blocks. Here we check only on lower tiers, but we need\n",
    "        #   to fix it for tier 0, too.\n",
    "        # ###\n",
    "        # START HERE 2022-10-29\n",
    "        # ###\n",
    "        # FIXME: tier_witnesses is the last time we see offsets into the global token array,\n",
    "        #   but we need them for unaligned portions of lower tiers. Pass the global offsets\n",
    "        #   into the function that does the expansion. Simplify (or better document) the\n",
    "        #   function first?\n",
    "        tier_witnesses = [token_array[i:j] for i, j in node.children]\n",
    "        tier_token_array, tier_token_membership_array, tier_token_witness_offset_array = create_token_array(tier_witnesses)\n",
    "        # print(f\"{tier_token_witness_offset_array=}\")\n",
    "        print(f\"{tier_token_array=}\")\n",
    "        tier_suffix_array = SuffixArray(tier_token_array, unit=UNIT_BYTE)\n",
    "        tier_lcp_array = tier_suffix_array._LCP_values\n",
    "        # print(tier_suffix_array)\n",
    "        tier_blocks = create_blocks(tier_suffix_array, tier_token_membership_array, tier_witnesses, tier_lcp_array)\n",
    "        tier_frequent_sequences = create_blocks(tier_suffix_array,tier_token_membership_array, tier_witnesses, tier_lcp_array)\n",
    "        tier_largest_blocks = find_longest_sequences(tier_frequent_sequences, tier_suffix_array)\n",
    "        # print(len(tier_witnesses)) # should always be 6\n",
    "        # print(tier_token_membership_array)\n",
    "        # print(tier_largest_blocks)\n",
    "        if tier_largest_blocks:\n",
    "            tier_block_offsets_by_witness, tier_witness_offsets_to_blocks, tier_first_token_offset_in_block_by_witness, tier_first_absolute_token_by_witness, tier_score_by_block \\\n",
    "                = prepare_for_beam_search(tier_witnesses, tier_token_membership_array, tier_largest_blocks)\n",
    "            tier_initial = [BeamOption(score=0, path=())] # one-item list\n",
    "            tier_options, _ = perform_beam_search_step(tier_witnesses, tier_largest_blocks, tier_block_offsets_by_witness, tier_witness_offsets_to_blocks, tier_score_by_block)\n",
    "            tier_finished = [] # options that cannot go further\n",
    "            tier_counter = 0\n",
    "            while tier_options: # no more options means that we're done\n",
    "                # print(f\"{tier_options=}\")\n",
    "                # TODO: The beam size at the moment is a magic number; can we rationalize it?\n",
    "                tier_options, tier_end_of_life \\\n",
    "                    = perform_beam_search_step(tier_witnesses, tier_largest_blocks, tier_block_offsets_by_witness, tier_witness_offsets_to_blocks, tier_score_by_block, _beam_options=tier_options, _beta=20)\n",
    "                tier_finished.extend(tier_end_of_life) # add any options that cannot go further\n",
    "                print(tier_counter, len(tier_options), len(tier_finished))\n",
    "                tier_counter += 1\n",
    "            tier_finished = list(set(tier_finished)) # TODO: Remove this because we'll sort later?\n",
    "            # print(tier_finished[0])\n",
    "            # Set processed property and expand node\n",
    "            expand_branching_node(node, tier_largest_blocks, tier_finished, tier_first_absolute_token_by_witness, tier_witnesses, node.children)\n",
    "        node.processed = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "# Create digraph and add root node\n",
    "tree = graphviz.Digraph(format=\"svg\")\n",
    "tree.node(str(root.id), label=\"Root\")\n",
    "# Expand tree recursively\n",
    "def populate_tree(_digraph, _parent): # void\n",
    "    for n in _parent.children:\n",
    "        # print(n.id)\n",
    "        if isinstance(n, Leaf_node):\n",
    "            label = n.string\n",
    "        else:\n",
    "            label = repr(n.children)\n",
    "        _digraph.node(str(n.id), label=label)\n",
    "        _digraph.edge(str(_parent.id), str(n.id))\n",
    "populate_tree(tree, root)\n",
    "for child in root.children:\n",
    "    if isinstance(child, Branching_node) and child.processed == False:\n",
    "        populate_tree(tree, child)\n",
    "svg_tree = tree.render()\n",
    "display(SVG(svg_tree))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for w, r in enumerate(node_by_id[991].absolute_offsets):\n",
    "    print(w, token_array[r[0]:r[1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(node_by_id[991])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
